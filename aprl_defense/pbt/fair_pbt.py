from copy import deepcopy

import wandb

from aprl_defense.common.train import run_trainer
from aprl_defense.pbt.fixed_iter_pbt import _get_agent
from aprl_defense.training_manager import TrainingManager, create_trainer


class FairPBTManager(TrainingManager):
    def __init__(self, log_dir: str, alg: str, max_timesteps: int, checkpoint_freq: int, main_id: int, scheduler_func, iter_steps=25000,
                 num_ops=10, cycle_agents=False):
        super().__init__(log_dir, alg, max_timesteps, checkpoint_freq)

        self.main_id = main_id
        self.scheduler_func = scheduler_func
        self.num_ops = num_ops
        self.iter_steps = iter_steps
        self.cycle_agents = cycle_agents

    def _train_loop(self, checkpoint_freq, max_steps, train_both_trainer, config):
        print(f'Running PBT')

        # Initialize PBT training
        primary_name = f'policy_{self.main_id}'
        secondary_id = 1 - self.main_id
        secondary_name = f'policy_{secondary_id}'
        # Table that contains infos for each agent, used to determine next opponent.
        # Currently temporary simply represented by a dict
        agent_infos = {'num_agents': self.num_ops,
                       'opponent_timesteps': [0] * self.num_ops,
                       'deactivated': [False] * self.num_ops}

        stored_agents = [None] * self.num_ops

        # Create second trainer that is only for training the secondary agent
        sec_pol_to_train = [secondary_name]

        # self.train_sec_trainer = create_trainer(policies_to_train=sec_pol_to_train,
        #                                         use_local_critic=self._get_use_local_critic(),
        #                                         config=self.config,
        #                                         scenario_name=self.scenario_name,
        #                                         env=self.env,
        #                                         trainer_cls=self.trainer_cls)

        timesteps_main_total = 0
        iterations = 0
        while timesteps_main_total < max_steps:
            # Determine id of secondary agent for this training iterations
            sec_id = self.scheduler_func(agent_infos)

            # Get stored weights, initialize new ones if necessary
            sec_weights = _get_agent(sec_id, stored_agents, config, secondary_name, self.trainer_cls)

            # Train secondary agent to get it to same number of time steps as primary
            opponent_timesteps = agent_infos['opponent_timesteps'][sec_id]
            if opponent_timesteps < timesteps_main_total:
                # Create trainer that is only for training the secondary agent:
                train_sec_trainer = create_trainer(policies_to_train=sec_pol_to_train,
                                                   use_local_critic=self._get_use_local_critic(),
                                                   config=self.config,
                                                   scenario_name=self.scenario_name,
                                                   env=self.env,
                                                   trainer_cls=self.trainer_cls)

                main_weights = train_both_trainer.get_weights(primary_name)
                train_sec_trainer.set_weights(main_weights)
                train_sec_trainer.set_weights({secondary_name: sec_weights})
                missing_timesteps = timesteps_main_total - opponent_timesteps

                log_setup = [(secondary_name, f'opponent_{sec_id}')]

                additional_timesteps = run_trainer(train_sec_trainer,
                                                   -1,
                                                   self.log_dir,
                                                   missing_timesteps,
                                                   config=config,
                                                   save=False,
                                                   log_setup=log_setup,
                                                   add_log_timesteps=opponent_timesteps)

                assert additional_timesteps >= missing_timesteps

                sec_weights = train_sec_trainer.get_weights()[secondary_name]
                # train_both_trainer.set_weights(self.train_sec_trainer.get_weights(primary_name))
                # Update how many steps this specific opponent was trained
                agent_infos['opponent_timesteps'][sec_id] += additional_timesteps  # self.iter_steps

            # Use these weights in the trainer (for secondary agent)
            train_both_trainer.set_weights({secondary_name: sec_weights})

            log_setup = [(primary_name, 'main'), (secondary_name, f'opponent_{sec_id}')]

            # Log which opponent is being played against
            wandb.log({f'opponent_id': sec_id,
                       'timestep': timesteps_main_total + 1})  # The + 1 ensures this timestep does not have duplicate values for op_id

            op_overhang = agent_infos['opponent_timesteps'][sec_id] - timesteps_main_total

            timesteps_main_total, mean_rew = run_trainer(train_both_trainer,
                                                         -1,
                                                         self.log_dir,
                                                         self.iter_steps + timesteps_main_total,
                                                         config,
                                                         log_setup=log_setup,
                                                         return_mean_rew=primary_name)

            # Log which opponent was played against
            wandb.log({f'opponent_id': sec_id,
                       'timestep': timesteps_main_total})

            # Store updated weights
            new_weights = deepcopy(train_both_trainer.get_weights()[secondary_name])
            stored_agents[sec_id] = new_weights
            # Update timesteps
            agent_infos['opponent_timesteps'][sec_id] = timesteps_main_total + op_overhang

            # Check whether main agent surpassed threshold
            if self.cycle_agents and mean_rew > -3.5:
                # Retire the current opponent
                agent_infos['deactivated'][sec_id] = True

                # Add new agent
                self.num_ops += 1
                agent_infos['num_agents'] += 1
                agent_infos['deactivated'].append(False)
                agent_infos['opponent_timesteps'].append(0)
                stored_agents.append(None)

            iterations += 1

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return None  # Trains all
