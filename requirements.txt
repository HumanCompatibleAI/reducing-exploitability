gym==0.19.0
torch==1.10.0
# ray[rllib]==1.7
# The specific ray commit that I modified in ray-testbed
# ray[rllib] @ https://s3-us-west-2.amazonaws.com/ray-wheels/master/f5ac915ed5470ea5a68cd32b262ee857cbad9d17/ray-2.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl

# We use a version of ray that includes modifications. By installing the wheel which corresponds to the master branch at the time of making
# this change, we don't need to build the binary parts of ray from source ourselves. Instead, ray includes a script that combines the binary
# parts from the wheel with our modified python code.
ray[rllib,default] @ https://s3-us-west-2.amazonaws.com/ray-wheels/master/6441335f5e8447ad07c0dbb7916e11aa43591876/ray-2.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl

# Ray 1.10
# ray[rllib] @ https://s3-us-west-2.amazonaws.com/ray-wheels/master/8b4cb45088a170e37d7d92c2e8a3dacddf67e1bc/ray-2.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl
wandb==0.12.4
tqdm==4.62.1
# The exact tf version was not important in my experiments. However, saved policies can only be loaded with the same tf version, so it is
# important to keep it fixed
tensorflow==2.7.0
pyglet==1.5.19
open_spiel==1.0.1
pettingzoo[classic]==1.12.0
# mujoco-py<2.2,>=2.1
multiagent @ git+https://github.com/PavelCz/multiagent-particle-envs.git@be8e975910e0ca6d54232dd15b96d6ea04203870
# gym_compete @ git+https://github.com/PavelCz/multiagent-competition.git@v0.2.1
stable-baselines3==1.3.0
fire==0.4.0
python-dotenv==0.20.0
gin-config==0.5.0
