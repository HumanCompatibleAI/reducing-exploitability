# Defenses Against Adversarial Policies

## Setup

Should work with python 3.7, 3.8

### Installation example

Install using Docker or using the following process.

```
conda create -n defense python=3.8
```

Install necessary packages:

```
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

For generating videos:

```
conda install ffmpeg
```

`ffmpeg` an optionally also be installed with your systems package manager

## Running Training

To change the output path change `TrialSettings.out_path` via gin-config.
This can be overwritten with the environment variable `POLICY_DEFENSE_OUT`.

### Configuration

Most frequently used settings can be changed via gin.  
The settings intended to be configured with gin are:
- `TrialSettings` (`aprl_defense.trial.settings.TrialSettings`)
- `RLSettings` (`aprl_defense.trial.settings.RLSettings`)
- Additionally, depending on whether one of these modes is used
    - `selfplay` (`aprl_defense.training_managers.simple_training_manager.SelfplayTrainingManager`)
    - `single-agent` - no additonal arguments
    - `attack` (`aprl_defense.training_managers.simple_training_manager.AttackManager`)
    - `pbt` (`aprl_defense.training_managers.pbt_manager.PBTManager`)
  
For further documentation on the configurable parameters check the Documentation of the respective classes.
 
`base.gin` offer useful defaults that should be fine for most experiments.

To change hyperparameters we recommend creating RLlib configs that can be passed in via `--override` / `--override_f`

The following examples should clarify how to specify training for different modes (run from `src` folder).
Further configuration options are possible, but the ones shown are necessary.

### Single Job Runs

#### Selfplay Training

```bash
python -m aprl_defense.train \
  -f "gin/pbrl-defense/base.gin" \
  -p "TrialSettings.mode='selfplay'" \
  -p "TrialSettings.wandb_group = <name for group of experiments>" \
  -p "RLSettings.env = <env>" \
  -p "RLSettings.max_timesteps = <int>"
```

#### Adversary Training

```bash
python -m aprl_defense.train \
  -f "gin/pbrl-defense/base.gin" \
  -p "TrialSettings.mode='attack'" \
  -p "TrialSettings.wandb_group = <name for group of experiments>" \
  -p "RLSettings.env = <env>" \
  -p "RLSettings.max_timesteps = <int>" \
  -p "attack.victim_artifact = <wandb_id:version>" \
  -p "attack.adversary_d = <0 or 1>" \
  -p "victim_policy_name = <str>"
```

#### Population-Based Training

```bash
python -m aprl_defense.train \
  -f "gin/pbrl-defense/base.gin" \
  -p "TrialSettings.mode='pbt'" \
  -p "TrialSettings.wandb_group = <name for group of experiments>" \
  -p "RLSettings.env = <env>" \
  -p "RLSettings.max_timesteps = <int>" \
  -p "pbt.main_id = <0 or 1>" \
  -p "pbt.num_ops = <int>"
```

### Multi-Job Runs

#### PBT With Subsequent Attack

Runs several PBT jobs, subsequently attecks each seed.

```bash
python -m aprl_defense.train \
  -f "gin/pbrl-defense/base.gin" \
  -p "TrialSettings.mode='pbt+attack'" \
  -p "TrialSettings.wandb_group = <name for group of experiments>" \
  -p "RLSettings.env = 'mpe_simple_push'" \
  -p "pbt_train_attack.num_ops_list=[2, 4, 8, 16]" \
  -p "pbt_train_attack.num_training=5" \
  -p "pbt_train_attack.num_attacks=5" \
  -p "pbt_train_attack.num_processes=4"
```

## Running Evaluation

For the moment these should be considered fairly untested.

`aprl_defense.evaluate` is mostly a copy of the RLlib `evaluate.py` script. This copy is necessary to use custom environments.

`aprl_defense.eval` contains eval scripts to run multple eval jobs and to evaluate given checkpoint in wandb.

## Additional Scripts

`aprl_defense/experiments` contains several scripts for specific types of experiments. Some of these might be outdated and will be updated 
as needed.

## Some Explanations

In all but the most basic setups creating an RLlib config for multiagent training requires programmatically creating a config in python and
these configs could not be created simply by passing in a config file.
For convenience the most commonly changed hyperparameters and set-up configurations can be changed with gin, additional modifications can be
performed by overriding the RLlib config.