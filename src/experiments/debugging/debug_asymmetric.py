import argparse

import gym
import ray
from ray import tune
from ray.rllib.agents.a3c import A3CTrainer, A2CTrainer
from ray.rllib.agents.ddpg import DDPGTrainer
from ray.rllib.agents.impala import ImpalaTrainer
from ray.rllib.agents.ppo import PPOTrainer, APPOTrainer, PPOTFPolicy
from ray.rllib.agents.sac import SACTrainer
from experiments.debugging.asymmetric_basic_env import AsymmetricMultiAgent



trainer_map = {  # Trainers are not used atm but might be helpful later with trainable training instead of ray.tune
    #    'maddpg': MADDPGTrainer,
    'PPO': PPOTrainer,
    'SAC': SACTrainer,
    'A3C': A3CTrainer,
    'A2C': A2CTrainer,
    'DDPG': DDPGTrainer,
    'APPO': APPOTrainer,
    'IMPALA': ImpalaTrainer

}

parser = argparse.ArgumentParser()
# Use torch for both policies.
parser.add_argument(
    "--framework",
    choices=["tf", "tf2", "tfe", "torch"],
    default="tf",
    help="The DL framework specifier.",
)
parser.add_argument(
    "--alg",
    choices=trainer_map.keys(),
    default="PPO",
    help="RL alg to use.",
)
parser.add_argument(
    "--as-test",
    action="store_true",
    help="Whether this script should be run as a test: --stop-reward must "
         "be achieved within --stop-timesteps AND --stop-iters.",
)
parser.add_argument(
    "--stop-iters", type=int, default=20, help="Number of iterations to train."
)
parser.add_argument(
    "--stop-timesteps", type=int, default=100000, help="Number of timesteps to train."
)
parser.add_argument(
    "--stop-reward", type=float, default=50.0, help="Reward at which we stop training."
)
parser.add_argument(
    "--local-mode", action='store_true', help="Ray local mode."
)

if __name__ == "__main__":
    args = parser.parse_args()

    ray.init(local_mode=args.local_mode)

    # env_name = 'multicomp/SumoHumans-v0'
    #
    #
    # def env_creator(args):
    #     # noinspection PyUnresolvedReferences
    #     import gym_compete
    #     env = gym.make(env_name)
    #
    #     # shaping_params = default
    #     # if scheduler is not None:  # Apply reward shaping with scheduler only if scheduler is provided
    #     #     env = apply_reward_wrapper(env, shaping_params, scheduler)
    #     # Apply wrapper for RLlib compatibility
    #     env = MujocoMultiagentWrapper(env)
    #     return env


    # env = env_creator({})
    # register_env("current-env", env_creator)
    env = AsymmetricMultiAgent()
    agent_ids = env.agent_ids

    # You can also have multiple policies per trainer, but here we just
    # show one each for PPO and DQN.
    policies = {
        f"policy_{id}": (
            PPOTFPolicy,
            env.observation_space[id],
            env.action_space,
            {}
        ) for id in agent_ids
    }


    def policy_mapping_fn(agent_id, episode, worker, **kwargs):
        return f"policy_{agent_id}"


    config = {
        'env': AsymmetricMultiAgent,
        "multiagent": {
            "policies": policies,
            "policy_mapping_fn": policy_mapping_fn,
            "policies_to_train": None,
        },
        "framework": args.framework,
        # "observation_filter": "ConcurrentMeanStdFilter"
    }

    results = tune.run(args.alg, config=config)
