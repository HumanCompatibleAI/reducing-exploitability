import argparse

import gym
import numpy as np
import ray
from ray import tune
from ray.rllib.agents.a3c import A3CTrainer, A2CTrainer
from ray.rllib.agents.a3c.a3c_tf_policy import A3CTFPolicy
from ray.rllib.agents.ddpg import DDPGTrainer
from ray.rllib.agents.impala import ImpalaTrainer
from ray.rllib.agents.ppo import PPOTrainer, APPOTrainer, PPOTFPolicy
from ray.rllib.agents.sac import SACTrainer, SACTFPolicy
from ray.train.trainer import Trainable
from ray.tune import register_env
from ray.tune.logger import pretty_print

from aprl_defense.common.train import init_env
from aprl_defense.common.utils import spaces_from_env


def _dict_from_tuple(data):
    return {i: e for i, e in enumerate(data)}


class MujocoSingleAgentWrapper(gym.Wrapper):
    def __init__(self, env, duplicate_actions):
        super().__init__(env)
        self.env = env

        self.observation_space = env.observation_space[0]
        self.action_space = env.action_space[0]
        self.metadata = self.env.metadata
        self.duplicate_actions = duplicate_actions

    # def __getattr__(self, name):
    #     if name.startswith("_"):
    #         raise AttributeError(
    #             "attempted to get missing private attribute '{}'".format(name)
    #         )
    #     return getattr(self.env, name)

    def step(self, actions):
        # Transform back from dict to tuple
        if self.duplicate_actions:
            actions_list = list(actions for i in range(2))
        else:
            actions_list = [actions, np.zeros_like(actions)]

        obs, rews, done, info = self.env.step(actions_list)

        return obs[0], rews[0], done, info

    def reset(self, **kwargs):
        obs = self.env.reset(**kwargs)

        return obs[0]

alg_map = {  # Trainers are not used atm but might be helpful later with trainable training instead of ray.tune
    #    'maddpg': MADDPGTrainer,
    'PPO': (PPOTrainer, PPOTFPolicy),
    'SAC': (SACTrainer, SACTFPolicy),
    'A3C': (A3CTrainer, A3CTFPolicy),
    'A2C': (A2CTrainer, None),
    'DDPG': (DDPGTrainer, None),
    'APPO': (APPOTrainer, None),
    'IMPALA': (ImpalaTrainer, None)

}

parser = argparse.ArgumentParser()
# Use torch for both policies.
parser.add_argument(
    "--framework",
    choices=["tf", "tf2", "tfe", "torch"],
    default="tf",
    help="The DL framework specifier.",
)
parser.add_argument(
    "--alg",
    type=str,
    choices=alg_map.keys(),
    default="PPO",
    help="RL alg to use.",
)
# parser.add_argument(
#     "--env",
#     type=str,
#     default="mpe",
#     help="Environment.",
# )
parser.add_argument(
    "--scenario",
    type=str,
    default="simple_push",
    help="Scenario within the environment.",
)
parser.add_argument(
    "--as-test",
    action="store_true",
    help="Whether this script should be run as a test: --stop-reward must "
         "be achieved within --stop-timesteps AND --stop-iters.",
)
parser.add_argument(
    "--stop-iters", type=int, default=20, help="Number of iterations to train."
)
parser.add_argument(
    "--stop-timesteps", type=int, default=100000, help="Number of timesteps to train."
)
parser.add_argument(
    "--stop-reward", type=float, default=50.0, help="Reward at which we stop training."
)
parser.add_argument(
    "--local-mode", action='store_true', help="Ray local mode."
)
parser.add_argument(
    "--use-trainable", action='store_true', help="Whether to use trainable class instead of ray tune."
)
parser.add_argument(
    "--duplicate-actions", action='store_true', help=""
)

if __name__ == "__main__":
    args = parser.parse_args()

    ray.init(local_mode=args.local_mode)

    # env_name = args.env
    scenario = args.scenario
    duplicate_actions = args.duplicate_actions

    # env = init_env(env_name=env_name, scenario_name=scenario)

    def env_creator(args):
        import gym_compete  # Necessary so gym_compete envs are registered
        env = gym.make(f'multicomp/{scenario}')
        env = MujocoSingleAgentWrapper(env, duplicate_actions)
        return env

    env = env_creator({})
    register_env("current-env", env_creator)

    # action_spaces, observation_spaces, agent_ids = spaces_from_env(env)

    # You can also have multiple policies per trainer, but here we just
    # show one each for PPO and DQN.
    # policies = {
    #     f"policy_{id}": (
    #         # PPOTFPolicy,
    #         alg_map[args.alg][1],
    #         observation_spaces[id],
    #         action_spaces[id],
    #         {'agent_id': id},
    #     ) for id in agent_ids
    # }


    def policy_mapping_fn(agent_id, episode, worker, **kwargs):
        return f"policy_{agent_id}"


    config = {
        'env_config': {
            'scenario_name': scenario
        },
        # "grad_clip": 10,
        # "multiagent": {
        #     "policies": policies,
        #     "policy_mapping_fn": policy_mapping_fn,
        #     "policies_to_train": None,
        # },
        "framework": args.framework,
        # "observation_filter": "ConcurrentMeanStdFilter"
    }

    if not args.use_trainable:
        config.update({'env': 'current-env'})
        results = tune.run(args.alg, config=config, stop={'timesteps_total': 100_000_000})
    else:
        trainer_cls: type(Trainable) = alg_map[args.alg][0]
        trainer = trainer_cls(config=config, env='current-env')

        for i in range(25000):
            # Perform one iteration of training the policy with PPO
            result = trainer.train()
            print(pretty_print(result))
