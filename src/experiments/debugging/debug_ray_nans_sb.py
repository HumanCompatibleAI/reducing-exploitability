import argparse

import gym
import gym_compete
import ray
from stable_baselines3 import PPO

from experiments.debugging.debug_ray_nans_single_agent import MujocoSingleAgentWrapper

# alg_map = {  # Trainers are not used atm but might be helpful later with trainable training instead of ray.tune
#     #    'maddpg': MADDPGTrainer,
#     'PPO': (PPOTrainer, PPOTFPolicy),
#     'SAC': (SACTrainer, SACTFPolicy),
#     'A3C': (A3CTrainer, A3CTFPolicy),
#     'A2C': (A2CTrainer, None),
#     'DDPG': (DDPGTrainer, None),
#     'APPO': (APPOTrainer, None),
#     'IMPALA': (ImpalaTrainer, None)
#
# }

parser = argparse.ArgumentParser()
# Use torch for both policies.
parser.add_argument(
    "--scenario",
    type=str,
    default="simple_push",
    help="Scenario within the environment.",
)
parser.add_argument(
    "--stop-timesteps", type=int, default=100_000_000, help="Number of timesteps to train."
)
parser.add_argument(
    "--duplicate-actions", action='store_true', help=""
)
parser.add_argument(
    "--agent-id", type=int, default=0, help="Which agent is the single agent."
)

if __name__ == "__main__":
    args = parser.parse_args()

    # env_name = args.env
    scenario = args.scenario
    duplicate_actions = args.duplicate_actions
    agent_id = args.agent_id


    def env_creator(args):
        env = gym.make(f'multicomp/{scenario}')
        env = MujocoSingleAgentWrapper(env, duplicate_actions, agent_id)
        return env


    env = env_creator({})

    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=args.stop_timesteps)

    obs = env.reset()
    for i in range(1000):
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, done, info = env.step(action)
        env.render()
        if done:
            obs = env.reset()

    env.close()
