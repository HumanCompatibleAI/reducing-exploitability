import argparse

import gym

import ray
from ray import tune
from ray.rllib.agents.a3c import A3CTrainer, A2CTrainer
from ray.rllib.agents.ddpg import DDPGTrainer
from ray.rllib.agents.impala import ImpalaTrainer
from ray.rllib.agents.ppo import PPOTrainer, APPOTrainer
from ray.rllib.agents.sac import SACTrainer
from ray.tune import register_env

from aprl_defense.common.utils import spaces_from_env
from aprl_defense.common.wrappers import MujocoToRllibWrapper

trainer_map = {  # Trainers are not used atm but might be helpful later with trainable training instead of ray.tune
    #    'maddpg': MADDPGTrainer,
    'PPO': PPOTrainer,
    'SAC': SACTrainer,
    'A3C': A3CTrainer,
    'A2C': A2CTrainer,
    'DDPG': DDPGTrainer,
    'APPO': APPOTrainer,
    'IMPALA': ImpalaTrainer

}

parser = argparse.ArgumentParser()
# Use torch for both policies.
parser.add_argument(
    "--framework",
    choices=["tf", "tf2", "tfe", "torch"],
    default="tf",
    help="The DL framework specifier.",
)
parser.add_argument(
    "--alg",
    choices=trainer_map.keys(),
    default="PPO",
    help="RL alg to use.",
)
parser.add_argument(
    "--as-test",
    action="store_true",
    help="Whether this script should be run as a test: --stop-reward must "
         "be achieved within --stop-timesteps AND --stop-iters.",
)
parser.add_argument(
    "--stop-iters", type=int, default=20, help="Number of iterations to train."
)
parser.add_argument(
    "--stop-timesteps", type=int, default=100000, help="Number of timesteps to train."
)
parser.add_argument(
    "--stop-reward", type=float, default=50.0, help="Reward at which we stop training."
)
parser.add_argument(
    "--local-mode", action='store_true', help="Ray local mode."
)

if __name__ == "__main__":
    args = parser.parse_args()

    ray.init(local_mode=args.local_mode)

    env_name = 'multicomp/SumoHumans-v0'


    def env_creator(args):
        # noinspection PyUnresolvedReferences
        import gym_compete
        env = gym.make(env_name)

        # shaping_params = default
        # if scheduler is not None:  # Apply reward shaping with scheduler only if scheduler is provided
        #     env = apply_reward_wrapper(env, shaping_params, scheduler)
        # Apply wrapper for RLlib compatibility
        env = MujocoToRllibWrapper(env)
        return env


    env = env_creator({})
    register_env("current-env", env_creator)

    action_spaces, observation_spaces, agent_ids = spaces_from_env(env)

    num_policies = 2  # For now use 2 policies for 2 agents

    # You can also have multiple policies per trainer, but here we just
    # show one each for PPO and DQN.
    policies = {
        f"policy_{i}": (
            None,
            observation_spaces[i],
            action_spaces[i],
            {},
        ) for i in range(num_policies)
    }


    def policy_mapping_fn(agent_id, episode, worker, **kwargs):
        return f"policy_{agent_id}"


    config = {
        'env': 'current-env',
        "multiagent": {
            "policies": policies,
            "policy_mapping_fn": policy_mapping_fn,
            "policies_to_train": None,
        },
        "framework": args.framework,
    }

    results = tune.run(args.alg, config=config)
