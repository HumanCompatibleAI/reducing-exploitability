import argparse

import gym
import ray
from ray import tune
from ray.rllib.agents.a3c import A3CTrainer, A2CTrainer
from ray.rllib.agents.a3c.a3c_tf_policy import A3CTFPolicy
from ray.rllib.agents.ddpg import DDPGTrainer
from ray.rllib.agents.impala import ImpalaTrainer
from ray.rllib.agents.ppo import PPOTrainer, APPOTrainer, PPOTFPolicy
from ray.rllib.agents.sac import SACTrainer, SACTFPolicy
from ray.train.trainer import Trainable
from ray.tune.logger import pretty_print

from aprl_defense.common.train import init_env
from aprl_defense.common.utils import spaces_from_env


def _dict_from_tuple(data):
    return {i: e for i, e in enumerate(data)}


class MujocoMultiagentWrapper(ray.rllib.MultiAgentEnv):
    def __init__(self, env):
        super().__init__()
        self.env = env

        observation_space = _dict_from_tuple(env.observation_space)
        self.observation_space = gym.spaces.Dict(spaces=observation_space)
        action_space = _dict_from_tuple(env.action_space)
        self.action_space = gym.spaces.Dict(spaces=action_space)
        self.metadata = self.env.metadata

    def __getattr__(self, name):
        if name.startswith("_"):
            raise AttributeError(
                "attempted to get missing private attribute '{}'".format(name)
            )
        return getattr(self.env, name)

    def step(self, actions_dict):
        # Transform back from dict to tuple
        actions_tuple = (actions_dict[i] for i in range(len(actions_dict)))

        obs, rews, done, info = self.env.step(actions_tuple)

        done_dict = {}
        for key in info.keys():
            done_dict[key] = info[key]['agent_done']
        # We set to done for everyone when at least one agent is done. Otherwise we would run into problems because RLlib doesn't sample actions for agents that
        # are already done, but mujoco expects actions from all agents
        done_dict['__all__'] = True in done_dict.values()
        return _dict_from_tuple(obs), _dict_from_tuple(rews), done_dict, info

    def reset(self, **kwargs):
        obs = self.env.reset(**kwargs)

        return _dict_from_tuple(obs)


alg_map = {  # Trainers are not used atm but might be helpful later with trainable training instead of ray.tune
    #    'maddpg': MADDPGTrainer,
    'PPO': (PPOTrainer, PPOTFPolicy),
    'SAC': (SACTrainer, SACTFPolicy),
    'A3C': (A3CTrainer, A3CTFPolicy),
    'A2C': (A2CTrainer, None),
    'DDPG': (DDPGTrainer, None),
    'APPO': (APPOTrainer, None),
    'IMPALA': (ImpalaTrainer, None)

}

parser = argparse.ArgumentParser()
# Use torch for both policies.
parser.add_argument(
    "--framework",
    choices=["tf", "tf2", "tfe", "torch"],
    default="tf",
    help="The DL framework specifier.",
)
parser.add_argument(
    "--alg",
    type=str,
    choices=alg_map.keys(),
    default="PPO",
    help="RL alg to use.",
)
parser.add_argument(
    "--env",
    type=str,
    default="mpe",
    help="Environment.",
)
parser.add_argument(
    "--scenario",
    type=str,
    default="simple_push",
    help="Scenario within the environment.",
)
parser.add_argument(
    "--as-test",
    action="store_true",
    help="Whether this script should be run as a test: --stop-reward must "
         "be achieved within --stop-timesteps AND --stop-iters.",
)
parser.add_argument(
    "--stop-iters", type=int, default=20, help="Number of iterations to train."
)
parser.add_argument(
    "--stop-timesteps", type=int, default=100000, help="Number of timesteps to train."
)
parser.add_argument(
    "--stop-reward", type=float, default=50.0, help="Reward at which we stop training."
)
parser.add_argument(
    "--local-mode", action='store_true', help="Ray local mode."
)
parser.add_argument(
    "--use-trainable", action='store_true', help="Whether to use trainable class instead of ray tune."
)

if __name__ == "__main__":
    args = parser.parse_args()

    ray.init(local_mode=args.local_mode)

    env_name = args.env
    scenario = args.scenario

    # env_name = 'multicomp/SumoHumans-v0'
    #
    #
    # def env_creator(args):
    #     # noinspection PyUnresolvedReferences
    #     import gym_compete
    #     env = gym.make(env_name)
    #
    #     # shaping_params = default
    #     # if scheduler is not None:  # Apply reward shaping with scheduler only if scheduler is provided
    #     #     env = apply_reward_wrapper(env, shaping_params, scheduler)
    #     # Apply wrapper for RLlib compatibility
    #     env = MujocoMultiagentWrapper(env)
    #     return env


    # env = env_creator({})
    # register_env("current-env", env_creator)
    env = init_env(env_name=env_name, scenario_name=scenario)

    action_spaces, observation_spaces, agent_ids = spaces_from_env(env)

    # You can also have multiple policies per trainer, but here we just
    # show one each for PPO and DQN.
    policies = {
        f"policy_{id}": (
            alg_map[args.alg][1],
            observation_spaces[id],
            action_spaces[id],
            {'agent_id': id},
        ) for id in agent_ids
    }


    def policy_mapping_fn(agent_id, episode, worker, **kwargs):
        return f"policy_{agent_id}"


    config = {
        'env_config': {
            'scenario_name': scenario
        },
        # "grad_clip": 10,
        "multiagent": {
            "policies": policies,
            "policy_mapping_fn": policy_mapping_fn,
            "policies_to_train": None,
        },
        "framework": args.framework,
        # "observation_filter": "ConcurrentMeanStdFilter"
    }

    if not args.use_trainable:
        config.update({'env': 'current-env'})
        results = tune.run(args.alg, config=config, stop={'timesteps_total': 100_000_000})
    else:
        trainer_cls: type(Trainable) = alg_map[args.alg][0]
        trainer = trainer_cls(config=config, env='current-env')

        for i in range(25000):
            # Perform one iteration of training the policy with PPO
            result = trainer.train()
            print(pretty_print(result))
