import os
import fire
from typing import Optional


def run_multi_finetune(
    env_config: str,
    num_workers: int,
    sweep_name: str,
    continue_artifact: str,
    fixed_artifact: str,
    description: str = "",
    num_seeds: int = 1,
    parallel: bool = True,
    continue_id: int = 0,
    override_config: bool = False,
    victim_policy_name: Optional[str] = None,
    train_batch_size: Optional[int] = None,
    max_timesteps: Optional[int] = None,
):
    make_parallel = "&" if parallel else ""
    for i in range(num_seeds):
        command = (
            f"python -m aprl_defense.train "
            f'-f "gin/{env_config}" '
            f"-p \"TrialSettings.mode = 'attack'\" "
            f'-p "TrialSettings.num_workers = {num_workers}" '
            f'-p "TrialSettings.override_config = {override_config}" '
            f"-p \"TrialSettings.wandb_group = '{sweep_name}'\" "
            f"-p \"TrialSettings.description = '{description}'\" "
            f"-p \"TrialSettings.continue_artifact = '{continue_artifact}'\" "
            f"-p \"attack.victim_artifact = '{fixed_artifact}'\" "
            f'-p "attack.adversary_id = {continue_id}" '
        )
        # It is necessary to manually set which policy to train, since using the
        # 'continue_artifact' in attack_manager defaults to using the loaded config. In
        # the loaded config for finetuning both agents are being trained (because
        # selfplay), however for this finetuning we only want the continue policy to be
        # trained, not the adversarial fixed opponent we finetune against.
        if continue_id == 0:
            command += f"-p \"TrialSettings.override_f = ['config/finetune-policy-0-override.json']\" "
        elif continue_id == 1:
            command += f"-p \"TrialSettings.override_f = ['config/finetune-policy-1-override.json']\" "
        else:
            raise ValueError(f"continue_id must be 0 or 1, got {continue_id}")
        if victim_policy_name is not None:
            command += f"-p \"attack.victim_policy_name = '{victim_policy_name}'\" "
        if train_batch_size is not None:
            command += f'-p "RLSettings.train_batch_size = {train_batch_size}" '
        if max_timesteps is not None:
            command += f'-p "RLSettings.max_timesteps = {max_timesteps}" '

        # Execute all seeds in parallel
        # if i != num_seeds - 1:  # Don't add & to last command
        command += f"{make_parallel} "

        print(f"Starting {i} with command:")
        print(command)
        os.system(command)


if __name__ == "__main__":
    fire.Fire(run_multi_finetune)
