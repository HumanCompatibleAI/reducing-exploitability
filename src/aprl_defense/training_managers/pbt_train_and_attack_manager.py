import datetime
from typing import Optional, List, Tuple

import gin
import ray

from aprl_defense.training_managers.base_training_manager import BaseTrainingManager
from aprl_defense.training_managers.pbt_manager import PBTManager
from aprl_defense.training_managers.simple_training_manager import AttackManager
from aprl_defense.trial.settings import TrialSettings, RLSettings


def _start_next_process(i, process_queue, running_processes):
    new_p = process_queue.pop(0)
    new_p["process"].start()
    running_processes[i] = new_p


@gin.configurable(name_or_fn="pbt_train_attack")
class PBTTrainAndAttackManager(BaseTrainingManager):
    def __init__(
        self,
        trial_settings: TrialSettings,
        rl_settings: RLSettings,
        override: Optional[str],
        override_f: Optional[str],
        num_ops_list: List[int],
        num_training: int,
        num_attacks: int,
        num_processes: int,
        both_agents: bool = False,
        op_experience_factor: float = 1.0,
    ):
        # For this type of multi-job run I would like to have all runs even with the same group name in separate groups, this is why I have a timestamp in
        # addition to the original name as the group name
        now = datetime.datetime.now()
        timestamp = now.strftime("%Y-%m-%d-%H-%M-%S")
        trial_settings.wandb_group += "pbt+attack-" + timestamp
        trial_settings.init_ray = False  # Because this is used by the sub-processes, we don't want them to init ray again
        self.trial_settings = trial_settings
        self.rl_settings = rl_settings
        self.override = override
        self.override_f = override_f
        self.num_ops_list = num_ops_list
        self.num_training = num_training
        self.num_attacks = num_attacks
        self.num_processes = num_processes
        self.both_agents = both_agents
        self.op_experience_factor = op_experience_factor

    def train(self):
        if self.both_agents:
            agent_ids = [0, 1]
        else:
            agent_ids = [0]

        @ray.remote(num_returns=2)
        def pbt_training(main_id: int, num_ops: int) -> Tuple[str, str]:
            training_manager = PBTManager(
                self.trial_settings,
                self.rl_settings,
                self.override,
                self.override_f,
                main_id=main_id,
                num_ops=num_ops,
            )
            training_manager.train()
            return training_manager.wandb_id, training_manager.main_policy

        @ray.remote
        def attack(
            adversary_id: int, victim_policy_name: str, victim_artifact: str
        ) -> None:
            training_manager = AttackManager(
                self.trial_settings,
                self.rl_settings,
                self.override,
                self.override_f,
                victim_artifact=victim_artifact + ":latest",
                adversary_id=adversary_id,
                victim_policy_name=victim_policy_name,
            )
            training_manager.train()

        ray.init()

        futures = []
        # Start processes
        for it in range(self.num_training):
            for agent_id in agent_ids:
                for num_ops in self.num_ops_list:
                    # Start PBT training
                    wandb_id_future, main_policy_future = pbt_training.remote(
                        agent_id, num_ops
                    )
                    futures.append(wandb_id_future)
                    futures.append(main_policy_future)
                    # Start attacks
                    for attack_id in range(self.num_attacks):
                        adversary_id = 1 - agent_id
                        future = attack.remote(
                            adversary_id, main_policy_future, wandb_id_future
                        )
                        futures.append(future)

        ray.wait(futures)

        print("All processes finished!")
