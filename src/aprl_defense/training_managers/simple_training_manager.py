from copy import deepcopy
from typing import Optional

import gin
from mujoco_py import MujocoException

from aprl_defense.common.base_logger import logger
from aprl_defense.common.utils import (
    noop_logger_creator,
    load_saved_weights,
    load_saved_checkpoint,
)
from aprl_defense.training_managers.base_training_manager import (
    SingleJobTrainingManager,
)
from aprl_defense.trial.settings import TrialSettings, RLSettings


@gin.configurable(name_or_fn="selfplay")
class SelfplayTrainingManager(SingleJobTrainingManager):
    def __init__(
        self,
        trial_settings: TrialSettings,
        rl_settings: RLSettings,
        override: Optional[str],
        override_f: Optional[str],
        train_only_one_policy: bool = False,
    ):
        super().__init__(trial_settings, rl_settings, override, override_f)
        self.train_only_one_policy = train_only_one_policy

    def start_training_loop(self):
        log_setup = [("policy_0", "policy_0"), ("policy_1", "policy_1")]
        try:
            self._run_trainer_helper(
                self.trainer,
                self.checkpoint_freq,
                self.rl.max_timesteps,
                artifact_manager=self.artifact_manager,
                log_setup=log_setup,
            )
        except MujocoException as e:
            self.artifact_manager.save_new_checkpoint()
            raise e

    def set_up_trainer(self):
        if self.trial.continue_artifact is None:
            if self.train_only_one_policy:
                policies_to_train = ["policy_1"]
            else:
                policies_to_train = None  # None == all policies train
            self.generic_trainer_setup(policies_to_train)
        else:
            # Use saved trainer from checkpoint
            file = self.artifact_manager.get_remote_checkpoint(
                self.trial.continue_artifact
            )
            self.trainer = load_saved_checkpoint(
                self.scenario_name, self.trainer_class, file
            )

    def get_mode(self):
        return "selfplay"


@gin.configurable(name_or_fn="attack")
class AttackManager(SingleJobTrainingManager):
    def __init__(
        self,
        trial_settings: TrialSettings,
        rl_settings: RLSettings,
        override: Optional[str],
        override_f: Optional[str],
        victim_artifact: Optional[str] = None,
        adversary_id: int = 1,
        victim_policy_name: Optional[str] = None,
    ):
        super().__init__(trial_settings, rl_settings, override, override_f)

        self.victim_artifact = victim_artifact
        self.adversary_id = adversary_id

        self.victim_id = 1 - adversary_id

        if victim_policy_name is None:
            self.victim_policy_name = f"policy_{self.victim_id}"
            logger.info(
                f"Using {self.victim_policy_name} as automatic fallback because no victim policy name was provided"
            )
        else:
            self.victim_policy_name = victim_policy_name

    def start_training_loop(self):
        trainer = self.trainer
        adv_name = f"policy_{self.adversary_id}"
        new_victim_name = f"policy_{self.victim_id}"

        file = self.artifact_manager.get_remote_checkpoint(self.victim_artifact)

        victim_weights = load_saved_weights(
            file, self.scenario_name, self.victim_policy_name, self.trainer_class
        )

        if self.victim_policy_name not in victim_weights:
            raise ValueError(
                f"No policy named {self.victim_policy_name} among victim weights. Please provide the correct name of the victim policy."
            )

        # Overwrite the weights for the agent that will not be trained (policy_1)
        trainer.set_weights({new_victim_name: victim_weights[self.victim_policy_name]})

        trainer.workers.sync_weights()  # IMPORTANT!!1!!1

        log_setup = [(adv_name, "adversary")]
        self._run_trainer_helper(
            trainer,
            self.checkpoint_freq,
            self.rl.max_timesteps,
            artifact_manager=self.artifact_manager,
            log_setup=log_setup,
        )

    def set_up_trainer(self):
        policies_to_train = [f"policy_{self.adversary_id}"]
        self.generic_trainer_setup(policies_to_train)

    def get_mode(self):
        return "attack"


class SingleAgentTrainingManager(SingleJobTrainingManager):
    def __init__(
        self,
        trial_settings: TrialSettings,
        rl_settings: RLSettings,
        override: str,
        override_f: str,
    ):
        super().__init__(trial_settings, rl_settings, override, override_f)

    def start_training_loop(self):
        log_setup = [("policy_0", "policy_0")]
        self._run_trainer_helper(
            self.trainer,
            self.checkpoint_freq,
            self.rl.max_timesteps,
            artifact_manager=self.artifact_manager,
            log_setup=log_setup,
        )

    def set_up_trainer(self):
        if self.trial.continue_artifact is None:
            # This general setup only allows one type of algorithm
            if len(self.trainer_class) > 1:
                raise ValueError("Only one algorithm allowed.")
            self.config["env_config"]["scenario_name"] = self.scenario_name
            # Create a new maddpg trainer which will be used to train the adversarial policy
            self.trainer = self.trainer_class(
                env="current-env",
                config=deepcopy(self.config),
                logger_creator=noop_logger_creator,
            )
        else:
            # Use saved trainer from checkpoint
            file = self.artifact_manager.get_remote_checkpoint(
                self.trial.continue_artifact
            )
            self.trainer = load_saved_checkpoint(
                self.scenario_name, self.trainer_class, file
            )

    def get_mode(self):
        return "single-agent"
