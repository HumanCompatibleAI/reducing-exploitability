from copy import deepcopy
from typing import Optional

import gin

import aprl_defense
from aprl_defense.common.base_logger import logger
from aprl_defense.common.utils import (
    noop_logger_creator,
    load_saved_weights,
    load_saved_checkpoint,
)
from aprl_defense.training_managers.base_training_manager import (
    SingleJobTrainingManager,
)
from aprl_defense.trial.settings import TrialSettings, RLSettings


@gin.configurable(name_or_fn="two_policy_selfplay")
class TwoPolicySelfplayTrainingManager(SingleJobTrainingManager):
    """Training manager for selfplay with separate policies for each agent."""

    def __init__(
        self,
        trial_settings: TrialSettings,
        rl_settings: RLSettings,
        override: Optional[str],
        override_f: Optional[str],
        train_only_one_policy: bool = False,
    ):
        """
        :param trial_settings:
        :param rl_settings:
        :param override:
        :param override_f:
        :param train_only_one_policy: Settings this to true will disable training for the second agent. For debugging / sanity checks purposes.
        """
        super().__init__(trial_settings, rl_settings, override, override_f)
        self.train_only_one_policy = train_only_one_policy

    def start_training_loop(self):
        """Default training loop with policy for each agent."""
        log_setup = [("policy_0", "policy_0"), ("policy_1", "policy_1")]
        try:
            self._run_trainer_helper(
                self.trainer,
                self.checkpoint_freq,
                self.rl.max_timesteps,
                artifact_manager=self.artifact_manager,
                log_setup=log_setup,
            )
        except Exception as e:
            logger.error(
                f"Encountered Exception will try to save checkpoint before "
                "re-raising"
            )
            self.artifact_manager.save_new_checkpoint()
            raise e

    def set_up_trainer(self):
        """Set up the trainer with possible continuation from checkpoint."""
        if self.trial.continue_artifact is None:
            if self.train_only_one_policy:
                policies_to_train = ["policy_1"]
            else:
                policies_to_train = None  # None == all policies train
            self.generic_trainer_setup(policies_to_train)
        else:
            # Use saved trainer from checkpoint
            file = self.artifact_manager.get_remote_checkpoint(
                self.trial.continue_artifact
            )
            self.trainer = load_saved_checkpoint(
                self.scenario_name, self.trainer_class, file
            )

    def get_mode(self):
        return "two_policy_selfplay"


@gin.configurable(name_or_fn="selfplay")
class SinglePolicySelfplayTrainingManager(SingleJobTrainingManager):
    """Training manager for selfplay with separate policies for each agent."""

    def __init__(
        self,
        trial_settings: TrialSettings,
        rl_settings: RLSettings,
        override: Optional[str],
        override_f: Optional[str],
    ):
        """
        :param trial_settings:
        :param rl_settings:
        :param override:
        :param override_f:
        :param train_only_one_policy: Settings this to true will disable training for the second agent. For debugging / sanity checks purposes.
        """
        super().__init__(trial_settings, rl_settings, override, override_f)

    def start_training_loop(self):
        """Default training loop with policy for each agent."""
        log_setup = [("policy_0", "policy_0")]
        try:
            self._run_trainer_helper(
                self.trainer,
                self.checkpoint_freq,
                self.rl.max_timesteps,
                artifact_manager=self.artifact_manager,
                log_setup=log_setup,
            )
        except Exception as e:
            logger.error(
                f"Encountered Exception will try to save checkpoint before "
                "re-raising"
            )
            self.artifact_manager.save_new_checkpoint()
            raise e

    def set_up_trainer(self):
        """Set up the trainer with possible continuation from checkpoint."""
        if self.trial.continue_artifact is None:
            policies_to_train = None  # None == all policies train

            multiagent = aprl_defense.common.utils.generate_multiagent_shared_policy()
            self.config["env_config"]["scenario_name"] = self.scenario_name
            self.config["multiagent"] = multiagent
            # Update config with some settings that apply to all configs regardless of
            # algorithm/trainer
            # Create a new trainer based on class.
            # Use noop_logger_creator to surpress ray logs that get saved by default.
            self.trainer = self.trainer_class(
                env="current-env", config=deepcopy(self.config),
                logger_creator=noop_logger_creator
            )
        else:
            # Use saved trainer from checkpoint
            file = self.artifact_manager.get_remote_checkpoint(
                self.trial.continue_artifact
            )
            self.trainer = load_saved_checkpoint(
                self.scenario_name, self.trainer_class, file
            )

    def get_mode(self):
        return "selfplay"


@gin.configurable(name_or_fn="attack")
class AttackManager(SingleJobTrainingManager):
    """Training manager for adversarial policy attack."""

    def __init__(
        self,
        trial_settings: TrialSettings,
        rl_settings: RLSettings,
        override: Optional[str],
        override_f: Optional[str],
        victim_artifact: str,
        adversary_id: int = 1,
        victim_policy_name: Optional[str] = None,
    ):
        """
        Create an attack manager against a specific victim.

        :param trial_settings:
        :param rl_settings:
        :param override:
        :param override_f:
        :param victim_artifact: wandb artifact id with version of saved victim policy.
        :param adversary_id: Agent id to use for training adversary. Victim used is the other agent.
        :param victim_policy_name: In case the name of the policy saved in the checkpoint is not simply 'policy_<victim_id>', provide the name of the victim
            policy here.
        """
        super().__init__(trial_settings, rl_settings, override, override_f)

        self.victim_artifact = victim_artifact
        self.adversary_id = adversary_id

        self.victim_id = 1 - adversary_id

        if victim_policy_name is None:
            self.victim_policy_name = f"policy_{self.victim_id}"
            logger.info(
                f"Using {self.victim_policy_name} as automatic fallback because no victim policy name was provided"
            )
        else:
            self.victim_policy_name = victim_policy_name

    def start_training_loop(self):
        """Load victim and start normal training against it."""
        trainer = self.trainer
        adv_name = f"policy_{self.adversary_id}"
        new_victim_name = f"policy_{self.victim_id}"

        file = self.artifact_manager.get_remote_checkpoint(self.victim_artifact)

        victim_weights = load_saved_weights(
            file, self.scenario_name, self.victim_policy_name, self.trainer_class
        )

        if self.victim_policy_name not in victim_weights:
            raise ValueError(
                f"No policy named {self.victim_policy_name} among victim weights. Please provide the correct name of the victim policy."
            )

        # Overwrite the weights for the agent that will not be trained, which is the victim.
        trainer.set_weights({new_victim_name: victim_weights[self.victim_policy_name]})

        trainer.workers.sync_weights()  # IMPORTANT!!1!!1

        log_setup = [(adv_name, "adversary")]
        self._run_trainer_helper(
            trainer,
            self.checkpoint_freq,
            self.rl.max_timesteps,
            artifact_manager=self.artifact_manager,
            log_setup=log_setup,
        )

    def set_up_trainer(self):
        """Generic trainer setup where only attacker is trained."""
        policies_to_train = [f"policy_{self.adversary_id}"]
        self.generic_trainer_setup(policies_to_train)

    def get_mode(self):
        return "attack"


class SingleAgentTrainingManager(SingleJobTrainingManager):
    """Training manager for single agent training. Should support all normal gym single-agent envs."""

    def __init__(
        self,
        trial_settings: TrialSettings,
        rl_settings: RLSettings,
        override: str,
        override_f: str,
    ):
        """
        :param trial_settings:
        :param rl_settings:
        :param override:
        :param override_f:
        """
        super().__init__(trial_settings, rl_settings, override, override_f)

    def start_training_loop(self):
        """Normal training loop with only one agent."""
        log_setup = [("policy_0", "policy_0")]
        self._run_trainer_helper(
            self.trainer,
            self.checkpoint_freq,
            self.rl.max_timesteps,
            artifact_manager=self.artifact_manager,
            log_setup=log_setup,
        )

    def set_up_trainer(self):
        """Set up a trainer without multi-agent."""
        if self.trial.continue_artifact is None:
            # This general setup only allows one type of algorithm
            if len(self.trainer_class) > 1:
                raise ValueError("Only one algorithm allowed.")
            self.config["env_config"]["scenario_name"] = self.scenario_name
            # Create a new maddpg trainer which will be used to train the adversarial policy
            self.trainer = self.trainer_class(
                env="current-env",
                config=deepcopy(self.config),
                logger_creator=noop_logger_creator,
            )
        else:
            # Use saved trainer from checkpoint
            file = self.artifact_manager.get_remote_checkpoint(
                self.trial.continue_artifact
            )
            self.trainer = load_saved_checkpoint(
                self.scenario_name, self.trainer_class, file
            )

    def get_mode(self):
        return "single-agent"
