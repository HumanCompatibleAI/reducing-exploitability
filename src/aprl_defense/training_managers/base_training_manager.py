import json
import random
from abc import ABC, abstractmethod
from copy import deepcopy
from pathlib import Path
from typing import Optional, Any

import numpy as np
import ray
import wandb
from ray.tune import Trainable
from ray.tune.logger import pretty_print
from tqdm import tqdm

import aprl_defense
import aprl_defense.common.utils
import aprl_defense.configs.common
import aprl_defense.configs.train
from aprl_defense.base_logger import logger
from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.rllib_io import save_params
from aprl_defense.common.train import (
    init_env,
)
from aprl_defense.common.utils import get_base_train_config, noop_logger_creator
from aprl_defense.common.utils import trainer_cls_from_str, CustomMujocoMetricsCallbacks
from aprl_defense.pbt.utils import custom_eval_log
from aprl_defense.trial.settings import RLSettings, TrialSettings
from ext.aprl.training.scheduling import Scheduler


class SingleJobTrainingManager(ABC):
    def __init__(
        self,
        trial_settings: TrialSettings,
        rl_settings: RLSettings,
        artifact_manager: ArtifactManager,
        override: Optional[str],
        override_f: Optional[str],
    ):
        self.trial = trial_settings
        self.rl = rl_settings

        self.artifact_manager = artifact_manager
        self.override = override
        self.override_f = override_f

        # Calculate checkpoint freq
        if (
            self.trial.checkpoint_freq_M is None
            and self.trial.num_checkpoints is not None
        ):
            self.checkpoint_freq = self.rl.max_timesteps / self.trial.num_checkpoints
        elif (
            self.trial.checkpoint_freq_M is not None
            and self.trial.num_checkpoints is None
        ):
            self.checkpoint_freq = int(self.trial.checkpoint_freq_M * 1_000_000)
        elif (
            self.trial.checkpoint_freq_M is None and self.trial.num_checkpoints is None
        ):
            num_checkpoints = 10
            logger.info(
                f"No checkpoint freq given, setting to {num_checkpoints} checkpoints overall"
            )
            self.checkpoint_freq = self.rl.max_timesteps / num_checkpoints
        else:  # checkpoint_freq_M is not None and num_checkpoints is not None:
            raise ValueError(
                "Can't set both checkpoint frequency and number of checkpoints! Choose one."
            )

        # Initialize these later
        self.scheduler = None
        # I couldn't get more strict types to work, this for example was not enough for pytype:
        # Optional[Dict[str, Union[Dict[str, Union[int, None, str]], int, str, list, float]]]
        self.config: Any = None
        self.trainer: Optional[Trainable] = None

        self._init_env()

        self.trainer_class = trainer_cls_from_str(self.rl.alg)

        # Init up seeding
        # Set seed for lib random
        random.seed(a=self.trial.seed)
        # Set seed for numpy
        np.random.seed(self.trial.seed)

    def _init_env(self):
        if self.rl.env.startswith("mpe_"):
            scenario_name = self.rl.env[4:]  # Remove the prefix
            self.env_name = "mpe"
        elif self.rl.env.startswith("gym_"):
            self.env_name = "gym"
            scenario_name = self.rl.env[4:]
        elif self.rl.env.startswith("os_"):
            self.env_name = "open_spiel"
            scenario_name = self.rl.env[3:]  # Remove the prefix
        elif self.rl.env.startswith("pz_"):
            scenario_name = self.rl.env[3:]  # Remove the prefix
            self.env_name = "pettingzoo"
        elif self.rl.env.startswith("mc_"):
            scenario_name = self.rl.env[3:]  # Remove the prefix
            self.env_name = "multicomp"
            # Use scheduler
            SchedulerActor = ray.remote(Scheduler)
            self.scheduler = SchedulerActor.remote()
        else:
            raise ValueError(f"Env {self.rl.env} not supported!")
        self.scenario_name = scenario_name

        self.env = init_env(self.env_name, self.scenario_name, self.scheduler)

    def _log_config(self, config, config_file_name):
        config_log = pretty_print(config)
        config_file_path = Path(self.trial.out_path) / config_file_name
        ray_config_file = open(config_file_path, mode="wt")
        ray_config_file.write(config_log)
        wandb.save(str(config_file_path))

    def train(self):
        # Config setup
        self.set_up_config()
        self._handle_config_override()

        # Save the ray config in wandb
        Path(self.trial.out_path).mkdir(
            exist_ok=True, parents=True
        )  # Create the out folder
        config_file_name = "ray_config.txt"
        self._log_config(self.config, config_file_name)
        # Save config to be used with rllibs's checkpoint loading (rllib expects a pickled config object)
        save_params(self.config, Path(self.trial.out_path))

        # Handle seeding for tf and torch
        if self.config["framework"] in ["tf", "tf2", "tfe"]:
            import tensorflow as tf

            tf.random.set_seed(self.trial.seed)
        elif self.config["framework"] == "torch":
            import torch

            torch.manual_seed(self.trial.seed)

        """Main function to be called to start training. Performs setup for config, trainer and starts training loop."""
        # Trainer setup
        self.set_up_trainer()
        # Set trainer for artifact manager, as ArtifactManager needs reference to the trainable
        self.artifact_manager.trainer = self.trainer
        # The previously logged config only contains the values that are different from the default, use this config to see all config params that ray uses
        self._log_config(self.trainer.config, "ray_config_after_init.txt")

        # Start training
        logger.info(f"Starting run, output saved at {self.trial.out_path}")
        # Start the actual training
        self.start_training_loop()

    def set_up_config(self):
        logger.info("Initializing config")
        self.config = get_base_train_config(self.rl.alg)
        if self.trial.num_workers is not None:
            self.config["num_workers"] = self.trial.num_workers

        self.config["train_batch_size"] = self.rl.train_batch_size
        if self.env_name == "multicomp":
            # Add mujoco logging callback
            # This callback logs the reward shaping
            self.config["callbacks"] = CustomMujocoMetricsCallbacks

        self.config["seed"] = self.trial.seed
        self.config["framework"] = self.trial.framework

    @abstractmethod
    def set_up_trainer(self) -> None:
        pass

    def generic_trainer_setup(self, policies_to_train):
        """Can be used by subclasses to implement trainer setup"""

        self.trainer = create_ma_trainer(
            policies_to_train,
            self.config,
            self.scenario_name,
            self.env,
            self.trainer_class,
        )

    @abstractmethod
    def start_training_loop(self):
        pass

    def _handle_config_override(self):
        if self.override is not None:
            # Convert json string into dict
            override_params = json.loads(self.override)
            # Update internal config with new override vals
            self.config.update(override_params)
        if self.override_f is not None:
            json_file = open(self.trial.override_f)
            # Convert json string into dict
            override_params = json.load(json_file)
            # Update internal config with new override vals
            self.config.update(override_params)

    def _run_trainer_helper(
        self,
        trainer,
        checkpoint_freq,
        max_timesteps,
        artifact_manager: ArtifactManager,
        log_setup=None,
    ):
        next_checkpoint = checkpoint_freq

        pbar = tqdm(total=max_timesteps)

        timesteps_total = 0
        while timesteps_total < max_timesteps:
            results = trainer.train()

            timesteps_total = results["timesteps_total"]

            if self.scheduler is not None:
                frac_remaining = (max_timesteps - timesteps_total) / max_timesteps
                _ = ray.get(
                    self.scheduler.get_val.remote("rew_shape", frac_remaining)
                )  # Update frac remaining for scheduler

            # for policy in trainer.config['multiagent']['policies']:
            if log_setup is not None:
                for policy, name in log_setup:

                    # Check that the metric for this policy was actually collected
                    if policy not in results["policy_reward_mean"]:
                        # raise ValueError(f"No result values collected. Is 'horizon' set?")
                        pass
                    else:
                        value = results["policy_reward_mean"][policy]
                        wandb.log(
                            {f"{name}_reward": value, "timestep": timesteps_total}
                        )
                if len(results["custom_metrics"]) != 0:
                    log_dict = {"timestep": timesteps_total}
                    log_dict.update(results["custom_metrics"])
                    wandb.log(log_dict)

            custom_eval_log(results, timesteps_total, timesteps_total)

            # Save intermediate checkpoint if necessary
            if checkpoint_freq != -1 and timesteps_total > next_checkpoint:
                artifact_manager.save_new_checkpoint()

                next_checkpoint += (
                    checkpoint_freq  # We save the next checkpoint over this threshold
                )

            pbar.update(timesteps_total - pbar.n)

        pbar.close()

        # Save last checkpoint at the end
        artifact_manager.save_new_checkpoint()
        logger.info("checkpoint saved")
        return timesteps_total


def create_ma_trainer(policies_to_train, config, scenario_name, env, trainer_cls):
    """Create new trainer with new weights"""
    multiagent = aprl_defense.common.utils.generate_multiagent_2_policies(
        env, policies_to_train
    )
    config["env_config"]["scenario_name"] = scenario_name
    config["multiagent"] = multiagent
    # Update config with some settings that apply to all configs regardless of algorithm/trainer
    # common_config = aprl_defense.configs.common.train
    # config.update(common_config)
    # Create a new maddpg trainer which will be used to train the adversarial policy
    new_trainer = trainer_cls(
        env="current-env", config=deepcopy(config), logger_creator=noop_logger_creator
    )
    return new_trainer
