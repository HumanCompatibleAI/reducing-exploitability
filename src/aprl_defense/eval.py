import argparse
import contextlib
from pathlib import Path
from typing import List

import ray
from ray.rllib.agents.ppo import PPOTrainer
from ray.rllib.rollout import rollout
from ray.tune.logger import pretty_print
from ray.tune.utils import merge_dicts

import aprl_defense.configs.common
import aprl_defense.configs.eval
from aprl_defense.common.in_memory_rollout_saver import InMemoryRolloutSaver
from aprl_defense.common.io import restore_trainer_from_path, get_checkpoint_file
from aprl_defense.common.train import init_env
from aprl_defense.common.utils import is_one_hot_agent, trainer_from_str, generate_multiagent


def multi_eval(agent_algs: List[str],
               agent_paths: List[str],
               num_steps: int,
               render: bool,
               local_mode: bool,
               use_newest_checkpoint=True) -> dict:
    num_agents = len(agent_algs)
    if num_agents != len(agent_paths):
        raise ValueError(f"Incompatible number of agents: len(agent_algs)={num_agents}, "
                         f"len(agent_paths)={len(agent_paths)}")
    if num_agents < 2:
        raise ValueError(f"Need at least 2 agents, got {num_agents}")

    if use_newest_checkpoint:
        for i, path in enumerate(agent_paths):
            checkpoint_file = get_checkpoint_file(path)

            agent_paths[i] = checkpoint_file

    scenario_name = 'simple_push'

    trainer_classes = [trainer_from_str(agent_alg) for agent_alg in agent_algs]

    # This is a list that gives info to the env policy of which agent is one-hot
    one_hot_agents = [is_one_hot_agent(agent_alg) for agent_alg in agent_algs]

    policy_names = [f'policy_{i}' for i in range(num_agents)]

    ray.init(local_mode=local_mode)

    # Create and register env
    env = init_env(scenario_name, max_steps=25, one_hot_agents=one_hot_agents)

    # Load original weights by way of trainers
    restored_trainers = []  # Contains trainers for all agents except the first one
    for i in range(num_agents):
        trainer = restore_trainer_from_path(Path(agent_paths[i]),
                                            scenario_name,
                                            trainer_classes[i],
                                            config_update=aprl_defense.configs.eval.eval_update)
        restored_trainers.append(trainer)

    # Set up the config for the eval trainer
    eval_config = aprl_defense.configs.common.eval_config  # get_victim_config(agent_paths[0])
    eval_config = _update_config_for_eval(eval_config, scenario_name)
    eval_config['multiagent'] = generate_multiagent(env,
                                                    use_local_critic=[False] * num_agents,
                                                    policies_to_train=None)
    # Set policies according to loaded agents
    for i, trainer in enumerate(restored_trainers):
        eval_config['multiagent']['policies'][policy_names[i]] = (
            type(trainer.get_policy(policy_names[i])),
            env.observation_space_dict[i],
            env.action_space_dict[i],
            {'agent_id': i}
        )

    print(pretty_print(eval_config))

    # eval_config['evaluation_num_episodes'] = 4
    # Create the trainer to perform eval in
    eval_trainer = PPOTrainer(config=eval_config, env='mpe-push')

    # Set loaded weights
    for i, trainer in enumerate(restored_trainers):
        # Set the weights for the other policy
        eval_trainer.set_weights(trainer.get_weights(policy_names[i]))

    # Now the trainer object should have the agent0 weights in policy0 and agent1 weights in policy1

    # results = eval_trainer.evaluate()

    # ray.shutdown()
    #
    # return {
    #     'mean_rewards': results['policy_reward_mean'],
    #     'num_episodes': results['episodes_this_iter']
    # }
    saver = InMemoryRolloutSaver()

    # Suppress command line output because rollout() is a bit too verbose for my taste
    with contextlib.redirect_stdout(None):
        rollout(eval_trainer,
                None,  # Parameter is unused
                num_steps=num_steps,
                saver=saver,
                no_render=not render)

    ray.shutdown()

    return {
        'mean_rewards': saver.mean_rewards,
    }


def _update_config_for_eval(config, scenario_name):
    """Updates the config for evaluation. Code from rllib/rollout.py"""

    # Make sure worker 0 has an Env.
    config["create_env_on_driver"] = True

    # Merge with `evaluation_config` (first try from command line, then from
    # pkl file).
    evaluation_config = config.get("evaluation_config", {})
    evaluation_config['horizon'] = 25
    config = merge_dicts(config, evaluation_config)

    # Make sure we have evaluation workers.
    # if not config.get("evaluation_num_workers"):
    #     config["evaluation_num_workers"] = config.get("num_workers", 0)
    if not config.get("evaluation_num_episodes"):
        config["evaluation_num_episodes"] = 4
    config['horizon'] = 25  # Number of timesteps before the episode is forced to terminate

    config.update(aprl_defense.configs.eval.eval_update)
    # config["render_env"] = not args.no_render
    # config["record_env"] = args.video_dir
    config["env_config"]["scenario_name"] = scenario_name  # IMPORTANT, overwrite

    return config


def parse_args():
    parser = argparse.ArgumentParser("Adversarial Policy Defenses Eval")
    parser.add_argument('--agent0', type=str, required=True)
    parser.add_argument('--agent1', type=str, required=True)
    parser.add_argument('--alg0', type=str, required=True)
    parser.add_argument('--alg1', type=str, required=True)
    parser.add_argument('--steps', type=int, default=10000)
    parser.add_argument('--render', action='store_true')
    parser.add_argument('--local-mode', action='store_true')
    parser.add_argument('--give-exact-path', action='store_true')

    return parser.parse_args()


def _main():
    args = parse_args()

    results = multi_eval(agent_algs=[args.alg0, args.alg1],
                         agent_paths=[args.agent0, args.agent1],
                         num_steps=args.steps,
                         render=args.render,
                         local_mode=args.local_mode,
                         use_newest_checkpoint=not args.give_exact_path)

    print(f"Results: {results}")


if __name__ == '__main__':
    _main()
