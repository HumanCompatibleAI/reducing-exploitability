train_default = {
    # === Log ===
    "log_level": "ERROR",

    # === Environment ===
    "env_config": {
        "scenario_name": None,  # IMPORTANT, overwrite
    },
    "num_envs_per_worker": 4,
    "horizon": 25,

    # === Policy Config ===
    # --- Model ---
    "good_policy": 'maddpg',
    "adv_policy": 'maddpg',
    "actor_hiddens": [64] * 2,
    "actor_hidden_activation": "relu",
    "critic_hiddens": [64] * 2,
    "critic_hidden_activation": "relu",
    "n_step": 1,
    "gamma": 0.95,

    # --- Exploration ---
    "tau": 0.01,

    # --- Replay buffer ---
    "buffer_size": 1000000,

    # --- Optimization ---
    "actor_lr": 1e-2,
    "critic_lr": 1e-2,
    "learning_starts": 1024 * 25,  # train_batch_size * max_episode_len,
    # "sample_batch_size": args.sample_batch_size,
    "train_batch_size": 1024,
    "batch_mode": "truncate_episodes",

    # --- Parallelism ---
    "num_workers": 1,
    "num_gpus": 0,
    "num_gpus_per_worker": 0,

    # === Multi-agent setting ===
    "multiagent": {  # IMPORTANT, overwrite these settings before using config
        "policies": None,
        "policy_mapping_fn": None
    },
}


