common = {  # Configs that apply to all runs
    # --- Parallelism ---
    "num_gpus": 0,
    "num_gpus_per_worker": 0,
    "horizon": 25,  # !IMPORTANT! Without this plotting results won't work
    # Normalize actions is not available for multi-agent envs, at least according to this: https://github.com/ray-project/ray/issues/8518k
    # For me this problem only occured with SAC, DDPG
    "normalize_actions": False,
    # "num_envs_per_worker": 4,
    # "sgd_minibatch_size": 256,
    # "rollout_fragment_length": 200,
    'env_config': {'scenario_name': None},
}

alg_specific = {
    'ppo': {
        "num_sgd_iter": 1,
        "sgd_minibatch_size": 25,  # TODO: Maybe this setting should only be used for PBT?
    },
    'maddpg': {
        # === Policy Config ===
        # --- Model ---
        "good_policy": 'maddpg',
        "adv_policy": 'maddpg',
        "actor_hiddens": [64] * 2,
        "actor_hidden_activation": "relu",
        "critic_hiddens": [64] * 2,
        "critic_hidden_activation": "relu",
        "n_step": 1,
        "gamma": 0.95,

        # --- Exploration ---
        "tau": 0.01,

        # --- Replay buffer ---
        "buffer_size": 1000000,

        # --- Optimization ---
        "actor_lr": 1e-2,
        "critic_lr": 1e-2,
        "learning_starts": 1024 * 25,  # train_batch_size * max_episode_len,
        # "sample_batch_size": args.sample_batch_size,
        "train_batch_size": 1024,
        "batch_mode": "truncate_episodes",

        # === Multi-agent setting ===
        "multiagent": {  # IMPORTANT, overwrite these settings before using config
            "policies": None,
            "policy_mapping_fn": None
        },
    }
}


pbt = {
    # The following settings ensure each iteration is only a single episode. This allows switching opponent after 1 episode
    "train_batch_size": 25,
    "num_workers": 1,
    "rollout_fragment_length": 25,
    "num_envs_per_worker": 1,
}
