from pathlib import Path

import gym
import pyspiel
from pettingzoo.classic import rps_v2
from ray.rllib.env import PettingZooEnv
from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv
from ray.tune import register_env

from aprl_defense.common.base_logger import logger
from aprl_defense.common.io import get_saved_config
from aprl_defense.common.utils import create_trainer
from aprl_defense.common.wrappers import MujocoToRllibWrapper, MujocoEnvFromStateWrapper
from aprl_defense.configs.reward import default
from aprl_defense.envs.open_spiel_zs_env import OpenSpielZSEnv
from ext.aprl.training.shaping_wrappers import apply_reward_wrapper
from ext.envs.multiagent_particle_env import (
    RLlibMultiAgentParticleEnv as MultiAgentParticleEnv,
)


def init_env(
    env_name: str, scenario_name: str, scheduler=None, max_steps=None, mujoco_state=None
):
    if env_name == "mpe":
        # Initialize environment
        def create_mpe_env(mpe_args):
            env = MultiAgentParticleEnv(max_steps=max_steps, **mpe_args)
            # env = Float64To32Wrapper(env)  # Apply float wrapper

            # The following only works with gym or pettingzoo envs
            # def action_wrapper(action, space):  # Convert discrete to one-hot actions, for mpe env
            #     one_hot = np.zeros((1, 5))
            #     discrete_action = action[0]
            #     one_hot[discrete_action] = 1.0
            #     return one_hot
            #
            # if alg != 'maddpg':  # MADDPG is the only alg that already returns one-hot activations
            #     env = action_lambda_v1(env,
            #                            action_wrapper,
            #                            lambda space: space)

            return env

        register_env("current-env", create_mpe_env)
        env = create_mpe_env({"scenario_name": scenario_name})
    elif env_name == "gym":

        def env_creator(args):
            env = gym.make(scenario_name)
            return env

        env = env_creator({})
        register_env("current-env", env_creator)
    elif env_name == "open_spiel":
        # '_zs_' suffix marks the envs that should be made zero-sum
        if scenario_name.endswith("_zs"):
            scenario_name = scenario_name[:-3]  # Drop the suffix
            env_wrapper = OpenSpielZSEnv
            logger.info(f"USING ZERO SUM VERSION OF ENV {scenario_name}")
        else:
            env_wrapper = OpenSpielEnv

        def create_os_env(_):
            return env_wrapper(pyspiel.load_game(scenario_name))

        register_env("current-env", create_os_env)
        env = create_os_env({})
    elif env_name == "pettingzoo":
        if scenario_name == "rps":

            def env_creator(args):
                return PettingZooEnv(rps_v2.env(num_actions=3, max_cycles=15))

        else:
            raise NotImplementedError(
                f"Currently pettingzoo env {scenario_name} is not supported!"
            )

        env = env_creator({})
        register_env("current-env", env_creator)
    elif env_name == "multicomp":

        def env_creator(args):
            # noinspection PyUnresolvedReferences
            import gym_compete  # noqa: F401 Necessary so gym_compete envs are registered

            env = gym.make(f"multicomp/{scenario_name}")

            shaping_params = default
            if (
                scheduler is not None
            ):  # Apply reward shaping with scheduler only if scheduler is provided
                env = apply_reward_wrapper(env, shaping_params, scheduler)
            # Apply wrapper for RLlib compatibility
            env = MujocoToRllibWrapper(env)
            if mujoco_state is not None:
                env = MujocoEnvFromStateWrapper(env, mujoco_state)
            return env

        env = env_creator({})
        register_env("current-env", env_creator)
    else:
        raise ValueError(f"environment {env_name} not supported")
    return env


def load_saved_weights(
    victim_path: Path, scenario_name: str, victim_name: str, trainer_cls
):
    victim_trainer = load_saved_checkpoint(scenario_name, trainer_cls, victim_path)
    victim_weights = victim_trainer.get_weights(victim_name)
    return victim_weights


def load_saved_checkpoint(scenario_name, trainer_cls, victim_path):
    config = get_saved_config(victim_path)
    # Changes to victim config for finetuning
    config["env"] = "current-env"
    config["num_workers"] = 0
    config["env_config"]["scenario_name"] = scenario_name
    config["multiagent"]["policy_map_capacity"] = 200
    config["multiagent"]["policy_map_cache"] = None
    if "evaluation_config" in config:
        config["evaluation_config"]["multiagent"]["policy_map_capacity"] = 200
        config["evaluation_config"]["multiagent"]["policy_map_cache"] = None
    # Restore the trained agent with this trainer
    new_trainer = create_trainer(trainer_cls, config)
    new_trainer.restore(checkpoint_path=str(victim_path))
    return new_trainer
