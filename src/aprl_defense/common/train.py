from pathlib import Path

import pyspiel

from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv
from ray.tune import register_env
from tqdm import tqdm
import wandb

from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.io import get_victim_config
from aprl_defense.envs.open_spiel_zs_env import OpenSpielZSEnv
from aprl_defense.pbt.utils import custom_eval_log
from aprl_defense.common.utils import create_trainer
from ext.envs.multiagent_particle_env import RLlibMultiAgentParticleEnv as MultiAgentParticleEnv


def run_trainer(trainer,
                checkpoint_freq,
                timesteps,
                artifact_manager: ArtifactManager,
                log_setup=None):
    next_checkpoint = checkpoint_freq

    pbar = tqdm(total=timesteps)

    timesteps_total = 0
    while timesteps_total < timesteps:
        results = trainer.train()

        timesteps_total = results['timesteps_total']

        # for policy in trainer.config['multiagent']['policies']:
        if log_setup is not None:
            for policy, name in log_setup:

                # Check that the metric for this policy was actually collected
                if policy not in results['policy_reward_mean']:
                    # raise ValueError(f"No result values collected. Is 'horizon' set?")
                    pass
                else:
                    value = results['policy_reward_mean'][policy]
                    wandb.log({f'{name}_reward': value,
                               'timestep': timesteps_total})

        custom_eval_log(results, timesteps_total, timesteps_total)

        # Save intermediate checkpoint if necessary
        if checkpoint_freq != -1 and timesteps_total > next_checkpoint:
            artifact_manager.save_new_checkpoint()

            next_checkpoint += checkpoint_freq  # We save the next checkpoint over this threshold

        pbar.update(timesteps_total - pbar.n)

    pbar.close()

    artifact_manager.save_new_checkpoint()
    print("checkpoint saved")
    return timesteps_total


def init_env(env_name: str, scenario_name: str, max_steps=None, one_hot_agents=None):
    if env_name == 'mpe':
        # Initialize environment
        def create_mpe_env(mpe_args):
            env = MultiAgentParticleEnv(max_steps=max_steps,
                                        one_hot_agents=one_hot_agents,
                                        **mpe_args)
            # env = Float64To32Wrapper(env)  # Apply float wrapper

            # The following only works with gym or pettingzoo envs
            # def action_wrapper(action, space):  # Convert discrete to one-hot actions, for mpe env
            #     one_hot = np.zeros((1, 5))
            #     discrete_action = action[0]
            #     one_hot[discrete_action] = 1.0
            #     return one_hot
            #
            # if alg != 'maddpg':  # MADDPG is the only alg that already returns one-hot activations
            #     env = action_lambda_v1(env,
            #                            action_wrapper,
            #                            lambda space: space)

            return env

        register_env('current-env', create_mpe_env)
        env = create_mpe_env({'scenario_name': scenario_name})
    elif env_name == 'open_spiel':
        # '_zs_' suffix marks the envs that should be made zero-sum
        if scenario_name.endswith('_zs'):
            scenario_name = scenario_name[:-3]  # Drop the suffix
            env_wrapper = OpenSpielZSEnv
            print(f'USING ZERO SUM VERSION OF ENV {scenario_name}')
        else:
            env_wrapper = OpenSpielEnv
        create_os_env = lambda _: env_wrapper(pyspiel.load_game(scenario_name))
        register_env("current-env", create_os_env)
        env = create_os_env({})
    else:
        raise ValueError(f"environment {env_name} not supported")
    return env


def load_saved_weights(victim_path: Path, scenario_name: str, victim_name: str, trainer_cls):
    victim_config = get_victim_config(victim_path)

    # Changes to victim config for finetuning
    victim_config['env'] = 'current-env'
    victim_config['env_config']['scenario_name'] = scenario_name

    # Restore the trained agent with this trainer
    victim_trainer = create_trainer(trainer_cls, victim_config)
    victim_trainer.restore(checkpoint_path=str(victim_path))
    victim_weights = victim_trainer.get_weights(victim_name)
    return victim_weights
