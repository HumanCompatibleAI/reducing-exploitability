from collections import defaultdict
from copy import deepcopy
from pathlib import Path

import numpy as np
from ray import cloudpickle as cloudpickle
from ray.tune import register_env
from tqdm import tqdm
import wandb

from aprl_defense.common.io import get_victim_config
from ext.envs.multiagent_particle_env import RLlibMultiAgentParticleEnv as MultiAgentParticleEnv


def run_trainer(trainer,
                checkpoint_freq,
                local_dir,
                timesteps,
                # writer,
                config,
                weight_list=None,
                save=True,
                log_setup=None,
                return_mean_rew=None,
                add_log_timesteps=0,
                progress_bar=True):
    timesteps_total = 0
    checkpoint_dir = local_dir / 'checkpoints'
    checkpoint_dir.mkdir(exist_ok=True, parents=True)
    next_checkpoint = checkpoint_freq
    with (checkpoint_dir / 'params.pkl').open(mode='wb') as file:
        cloudpickle.dump(config, file=file)

    rewards = defaultdict(list)
    if progress_bar:
        pbar = tqdm(total=timesteps)
    while timesteps_total < timesteps:
        results = trainer.train()

        timesteps_total = results['timesteps_total']

        for policy, value in results['policy_reward_mean'].items():
            rewards[policy].append(value)

        # Log mean policy reward
        # for policy in trainer.config['multiagent']['policies']:
        if log_setup is not None:
            for policy, name in log_setup:

                # Check that the metric for this policy was actually collected
                if policy not in results['policy_reward_mean']:
                    raise ValueError(f"No result values collected. Is 'horizon' set?")

                value = results['policy_reward_mean'][policy]
                wandb.log({f'{name}_reward': value,
                           'timestep': timesteps_total + add_log_timesteps})
                # writer.add_scalar(tag=f'policy_reward_mean/{policy}',
                #                   scalar_value=results['policy_reward_mean'][policy],
                #                   global_step=timesteps_total)

                # Log evaluation results if applicable
                if 'evaluation' in results and len(results['evaluation']['policy_reward_mean']) > 0:
                    for key, val in results['evaluation']['my_eval_vals'].items():
                        wandb.log({key: val,
                                   'timestep': timesteps_total + add_log_timesteps})

        # Save checkpoint if necessary
        if save and checkpoint_freq != -1 and weight_list is None and timesteps_total > next_checkpoint:
            trainer.save(checkpoint_dir=checkpoint_dir)
            next_checkpoint += checkpoint_freq  # We save the next checkpoint over this threshold
        elif weight_list is not None and timesteps_total > next_checkpoint:
            # This is the case where we don't save (many) intermediate checkpoints and instead save weights in the weight list
            weight_list.append(deepcopy(trainer.get_weights()))
        if progress_bar:
            pbar.update(timesteps_total - pbar.n)
    if progress_bar:
        pbar.close()
    if save:
        checkpoint = trainer.save(checkpoint_dir=checkpoint_dir)
        print("checkpoint saved at", checkpoint)
    if return_mean_rew is not None:
        mean_rew = np.mean(rewards[return_mean_rew])
        return timesteps_total, mean_rew
    else:
        return timesteps_total


def init_env(scenario_name, max_steps=None, one_hot_agents=None):
    # Initialize environment
    def create_mpe_env(mpe_args):
        env = MultiAgentParticleEnv(max_steps=max_steps,
                                    one_hot_agents=one_hot_agents,
                                    **mpe_args)

        # The following only works with gym or pettingzoo envs
        # def action_wrapper(action, space):  # Convert discrete to one-hot actions, for mpe env
        #     one_hot = np.zeros((1, 5))
        #     discrete_action = action[0]
        #     one_hot[discrete_action] = 1.0
        #     return one_hot
        #
        # if alg != 'maddpg':  # MADDPG is the only alg that already returns one-hot activations
        #     env = action_lambda_v1(env,
        #                            action_wrapper,
        #                            lambda space: space)

        return env

    register_env('mpe-push', create_mpe_env)
    env = create_mpe_env({'scenario_name': scenario_name})
    return env


def init_victim_weights(victim_path: Path, scenario_name: str, victim_name: str, trainer_cls):
    victim_config = get_victim_config(victim_path)

    # Changes to victim config for finetuning
    victim_config['env'] = 'mpe-push'
    victim_config['env_config']['scenario_name'] = scenario_name

    # Restore the trained agent with this trainer
    victim_trainer = trainer_cls(env='mpe-push', config=victim_config)
    victim_trainer.restore(checkpoint_path=str(victim_path))
    victim_weights = victim_trainer.get_weights(victim_name)
    return victim_weights
