from pathlib import Path

import gym
import pyspiel
from pettingzoo.classic import rps_v2
from ray.rllib.env import PettingZooEnv

from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv
from ray.tune import register_env
from tqdm import tqdm
import wandb

from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.io import get_saved_config
from aprl_defense.common.wrappers import MujocoToRllibWrapper
from aprl_defense.envs.open_spiel_zs_env import OpenSpielZSEnv
from aprl_defense.pbt.utils import custom_eval_log
from aprl_defense.common.utils import create_trainer
from ext.aprl.training.scheduling import Scheduler
from ext.aprl.training.shaping_wrappers import RewardShapingVecWrapper, apply_reward_wrapper
from ext.envs.multiagent_particle_env import RLlibMultiAgentParticleEnv as MultiAgentParticleEnv


def run_trainer(trainer,
                checkpoint_freq,
                timesteps,
                artifact_manager: ArtifactManager,
                log_setup=None):
    next_checkpoint = checkpoint_freq

    pbar = tqdm(total=timesteps)

    timesteps_total = 0
    while timesteps_total < timesteps:
        results = trainer.train()

        timesteps_total = results['timesteps_total']

        # for policy in trainer.config['multiagent']['policies']:
        if log_setup is not None:
            for policy, name in log_setup:

                # Check that the metric for this policy was actually collected
                if policy not in results['policy_reward_mean']:
                    # raise ValueError(f"No result values collected. Is 'horizon' set?")
                    pass
                else:
                    value = results['policy_reward_mean'][policy]
                    wandb.log({f'{name}_reward': value,
                               'timestep': timesteps_total})

        custom_eval_log(results, timesteps_total, timesteps_total)

        # Save intermediate checkpoint if necessary
        if checkpoint_freq != -1 and timesteps_total > next_checkpoint:
            artifact_manager.save_new_checkpoint()

            next_checkpoint += checkpoint_freq  # We save the next checkpoint over this threshold

        pbar.update(timesteps_total - pbar.n)

    pbar.close()

    # Save last checkpoint at the end
    artifact_manager.save_new_checkpoint()
    print("checkpoint saved")
    return timesteps_total


def init_env(env_name: str, scenario_name: str, max_steps=None, one_hot_agents=None):
    if env_name == 'mpe':
        # Initialize environment
        def create_mpe_env(mpe_args):
            env = MultiAgentParticleEnv(max_steps=max_steps,
                                        one_hot_agents=one_hot_agents,
                                        **mpe_args)
            # env = Float64To32Wrapper(env)  # Apply float wrapper

            # The following only works with gym or pettingzoo envs
            # def action_wrapper(action, space):  # Convert discrete to one-hot actions, for mpe env
            #     one_hot = np.zeros((1, 5))
            #     discrete_action = action[0]
            #     one_hot[discrete_action] = 1.0
            #     return one_hot
            #
            # if alg != 'maddpg':  # MADDPG is the only alg that already returns one-hot activations
            #     env = action_lambda_v1(env,
            #                            action_wrapper,
            #                            lambda space: space)

            return env

        register_env('current-env', create_mpe_env)
        env = create_mpe_env({'scenario_name': scenario_name})
    elif env_name == 'open_spiel':
        # '_zs_' suffix marks the envs that should be made zero-sum
        if scenario_name.endswith('_zs'):
            scenario_name = scenario_name[:-3]  # Drop the suffix
            env_wrapper = OpenSpielZSEnv
            print(f'USING ZERO SUM VERSION OF ENV {scenario_name}')
        else:
            env_wrapper = OpenSpielEnv
        create_os_env = lambda _: env_wrapper(pyspiel.load_game(scenario_name))
        register_env("current-env", create_os_env)
        env = create_os_env({})
    elif env_name == 'pettingzoo':
        if scenario_name == 'rps':
            def env_creator(args):
                return PettingZooEnv(rps_v2.env(num_actions=3, max_cycles=15))
        else:
            raise NotImplementedError(f'Currently pettingzoo env {scenario_name} is not supported!')

        env = env_creator({})
        register_env("current-env", env_creator)
    elif env_name == 'multicomp':
        def env_creator(args):

            env = gym.make(f'multicomp/{scenario_name}')

            shaping_params = {'weights': {'sparse': {}, 'dense': {}}}
            env = apply_reward_wrapper(env, shaping_params, Scheduler())
            # Apply wrapper for RLlib compatibility
            env = MujocoToRllibWrapper(env)
            return env

        env = env_creator({})
        register_env("current-env", env_creator)
    else:
        raise ValueError(f"environment {env_name} not supported")
    return env


def load_saved_weights(victim_path: Path, scenario_name: str, victim_name: str, trainer_cls):
    victim_trainer = load_saved_checkpoint(scenario_name, trainer_cls, victim_path)
    victim_weights = victim_trainer.get_weights(victim_name)
    return victim_weights


def load_saved_checkpoint(scenario_name, trainer_cls, victim_path):
    config = get_saved_config(victim_path)
    # Changes to victim config for finetuning
    config['env'] = 'current-env'
    config['num_workers'] = 0
    config['env_config']['scenario_name'] = scenario_name
    config['multiagent']['policy_map_capacity'] = 200
    config['multiagent']['policy_map_cache'] = None
    config['evaluation_config']['multiagent']['policy_map_capacity'] = 200
    config['evaluation_config']['multiagent']['policy_map_cache'] = None
    # Restore the trained agent with this trainer
    new_trainer = create_trainer(trainer_cls, config)
    new_trainer.restore(checkpoint_path=str(victim_path))
    return new_trainer
