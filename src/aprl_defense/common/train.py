from pathlib import Path

import pyspiel

from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv
from ray.tune import register_env
from tqdm import tqdm
import wandb

from aprl_defense.common.io import get_victim_config
from aprl_defense.common.rllib_io import save_params
from aprl_defense.pbt.utils import custom_eval_log
from aprl_defense.common.utils import create_trainer
from ext.envs.multiagent_particle_env import RLlibMultiAgentParticleEnv as MultiAgentParticleEnv


def run_trainer(trainer,
                checkpoint_freq,
                local_dir,
                timesteps,
                config,
                save=True,
                log_setup=None,
                progress_bar=True):
    checkpoint_dir = save_params(config, local_dir)

    next_checkpoint = checkpoint_freq

    pbar = tqdm(total=timesteps, disable=not progress_bar)

    timesteps_total = 0
    while timesteps_total < timesteps:
        results = trainer.train()

        timesteps_total = results['timesteps_total']

        # for policy in trainer.config['multiagent']['policies']:
        if log_setup is not None:
            for policy, name in log_setup:

                # Check that the metric for this policy was actually collected
                if policy not in results['policy_reward_mean']:
                    # raise ValueError(f"No result values collected. Is 'horizon' set?")
                    pass
                else:
                    value = results['policy_reward_mean'][policy]
                    wandb.log({f'{name}_reward': value,
                               'timestep': timesteps_total})

        custom_eval_log(results, timesteps_total, timesteps_total)

        # Save intermediate checkpoint if necessary
        if save and checkpoint_freq != -1 and timesteps_total > next_checkpoint:
            trainer.save(checkpoint_dir=checkpoint_dir)
            next_checkpoint += checkpoint_freq  # We save the next checkpoint over this threshold

        pbar.update(timesteps_total - pbar.n)

    pbar.close()

    if save:
        checkpoint = trainer.save(checkpoint_dir=checkpoint_dir)
        print("checkpoint saved at", checkpoint)
    return timesteps_total


def init_env(env_name: str, scenario_name, max_steps=None, one_hot_agents=None):

    if env_name == 'mpe':
        # Initialize environment
        def create_mpe_env(mpe_args):
            env = MultiAgentParticleEnv(max_steps=max_steps,
                                        one_hot_agents=one_hot_agents,
                                        **mpe_args)

            # The following only works with gym or pettingzoo envs
            # def action_wrapper(action, space):  # Convert discrete to one-hot actions, for mpe env
            #     one_hot = np.zeros((1, 5))
            #     discrete_action = action[0]
            #     one_hot[discrete_action] = 1.0
            #     return one_hot
            #
            # if alg != 'maddpg':  # MADDPG is the only alg that already returns one-hot activations
            #     env = action_lambda_v1(env,
            #                            action_wrapper,
            #                            lambda space: space)

            return env

        register_env('mpe-push', create_mpe_env)
        env = create_mpe_env({'scenario_name': scenario_name})
    elif env_name == 'open_spiel_env':
        create_os_env = lambda _: OpenSpielEnv(pyspiel.load_game(scenario_name))
        register_env("open_spiel_env",
                     create_os_env)
        env = create_os_env({})
    else:
        raise ValueError(f"environment {env_name} not supported")
    return env


def load_saved_weights(victim_path: Path, scenario_name: str, victim_name: str, trainer_cls):
    victim_config = get_victim_config(victim_path)

    # Changes to victim config for finetuning
    victim_config['env'] = 'mpe-push'
    victim_config['env_config']['scenario_name'] = scenario_name

    # Restore the trained agent with this trainer
    victim_trainer = create_trainer(trainer_cls, victim_config)
    victim_trainer.restore(checkpoint_path=str(victim_path))
    victim_weights = victim_trainer.get_weights(victim_name)
    return victim_weights
