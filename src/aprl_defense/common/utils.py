from collections import OrderedDict
from typing import List, Optional

import numpy as np
import ray
from gym.spaces import Box
from ray.rllib.agents.a3c import A3CTrainer, A2CTrainer
from ray.rllib.agents.ddpg import DDPGTrainer
from ray.rllib.agents.impala import ImpalaTrainer
from ray.rllib.agents.ppo import PPOTrainer, APPOTrainer, PPOTFPolicy
from ray.rllib.agents.sac import SACTrainer

# from ray.rllib.contrib.maddpg import MADDPGTrainer  # Bugs in the current version of RLLIB MADDPGTrainer
from ray.rllib.contrib.maddpg.maddpg_policy import MADDPGTFPolicy
from ray.rllib.env import PettingZooEnv
from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv
from ray.tune.logger import NoopLogger
from stable_baselines3.common.vec_env import VecEnv

import aprl_defense.configs.train
# from aprl_defense.agents.maddpg import MADDPGTrainer
from ext.aprl.envs.multi_agent import MultiWrapper
from ext.envs.multiagent_particle_env import RLlibMultiAgentParticleEnv

from gym_compete.new_envs.multi_agent_env import MultiAgentEnv as MujocoMultiAgentEnv


def is_one_hot_agent(agent_name: str):
    if agent_name == 'maddpg':  # MADDPG is the only one-hot agent
        return True
    return False


trainer_map = {
#    'maddpg': MADDPGTrainer,
    'ppo': PPOTrainer,
    'sac': SACTrainer,
    'a3c': A3CTrainer,
    'a2c': A2CTrainer,
    'ddpg': DDPGTrainer,  # Doesn't work in simple-push (discrete actions)
    'appo': APPOTrainer,
    'impala': ImpalaTrainer

}


# policies = {
#     'maddpg': MADDPGTFPolicy,
#     'ppo': PPOTFPolicy
# }

def trainer_cls_from_str(trainer_name: str):
    if trainer_name in trainer_map:
        return trainer_map[trainer_name]
    else:
        raise ValueError(f'Invalid RL algorithm: {trainer_name}')


def generate_multiagent_2_policies(env, use_local_critic: List[bool], policies_to_train: Optional[List[str]]) -> dict:
    num_policies = 2

    policy_ids = []
    for i in range(0, num_policies):
        policy_ids.append(f'policy_{i}')

    if isinstance(env, RLlibMultiAgentParticleEnv) or isinstance(env, OpenSpielEnv):
        # Currently we need to create the policies dict for MPE and OS manually

        action_spaces, observation_spaces, agent_ids = spaces_from_env(env)
        policies = {}
        for i in range(0, num_policies):
            policies[f'policy_{i}'] = (
                None,
                observation_spaces[i],
                action_spaces[i],
                {
                    'agent_id': i,
                    'use_local_critic': use_local_critic[i]
                    # "obs_space_dict": env.observation_space_dict,
                    # "act_space_dict": env.action_space_dict
                }
            )
    else:
        # For PZ we policies dict is created automatically, so we only need to pass the policy names
        agent_ids = list(env.agents)
        policies = set(policy_ids)

    assert len(agent_ids) == 2, 'Currently only 2-agent environments are supported'

    agent_to_policy = {agent_id: policy_id for agent_id, policy_id in zip(agent_ids, policy_ids)}

    policy_mapping_func = ray.tune.function(lambda agent_id, episode, **kwargs: agent_to_policy[agent_id])

    multiagent = {'policies': policies,
                  'policy_mapping_fn': policy_mapping_func,
                  'policies_to_train': policies_to_train}

    return multiagent


def spaces_from_env(env):
    """Returns two dictionaries where the element at key i corresponds the action and observation spaces respectively for agent with id i."""
    if isinstance(env, OpenSpielEnv):  # This env has a constant observation+action space for all agents
        agent_ids = list(range(env.num_agents))
        # New theory: The following seems to only be necessary in ray 1.6 onwards
        # Yep, for some reason there observations have np.float64, while the original observation space is np.float32. I'm not sure where exactly the cause of
        # this problem is, so here is my hacky workaround. Unfortunately this means we reset the seed, because the original seed is not saved. Alternatively,
        # we could set the ._no_random attribute, but this doesn't seem necessary
        new_observation_space = Box(low=env.observation_space.low, high=env.observation_space.high, shape=env.observation_space.shape,
                                    dtype=np.float64, seed=None)
        observation_spaces = {agent_id: new_observation_space for agent_id in agent_ids}
        action_spaces = {agent_id: env.action_space for agent_id in agent_ids}
    elif isinstance(env, RLlibMultiAgentParticleEnv):
        agent_ids = env.agent_ids
        observation_spaces = env.observation_space_dict
        action_spaces = env.action_space_dict
    elif isinstance(env, PettingZooEnv):
        agent_ids = list(env.agents)
        observation_spaces = env.observation_spaces
        action_spaces = env.action_spaces
    elif isinstance(env, VecEnv):
        agent_ids = list(env.unwrapped.keys)
        observation_spaces = env.observation_space
        action_spaces = env.action_space
    else:
        raise NotImplementedError(f'Environment {env} with type {type(env)} not supported!')
    return action_spaces, observation_spaces, agent_ids


def init_config(alg):
    # Get common config
    config = aprl_defense.configs.train.common
    # Update config with alg-specific settings
    config_for_alg = aprl_defense.configs.train.alg_specific.get(alg, {})
    config.update(config_for_alg)
    return config


def noop_logger_creator(config):
    """Creator functino for NoopLogger. Trainable receive a creator which has the single argument 'config'. Because this creates a NoopLogger the argument
    is never used"""
    return NoopLogger(config, "")


def create_trainer(trainer_cls, config):
    return trainer_cls(env='current-env', config=config, logger_creator=noop_logger_creator)


def policies_equal(policy_1, policy_2) -> bool:
    """Compare structural equality of two policies."""
    if ((isinstance(policy_1, OrderedDict) and isinstance(policy_2, OrderedDict))
            or (isinstance(policy_1, dict) and isinstance(policy_2, dict))):
        if not policy_1.keys() == policy_2.keys():
            return False
        for key in policy_1.keys():
            element_1: np.ndarray = policy_1[key]
            element_2: np.ndarray = policy_2[key]
            if not np.all(element_1 == element_2):
                return False
        return True
    else:
        raise NotImplementedError(f'Currently we only support RLLib policies which are ordered dicts (which applies to at least PPO)')
