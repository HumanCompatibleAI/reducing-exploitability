from collections import OrderedDict
from typing import List, Optional

import numpy as np
import ray
from gym.spaces import Box
from ray.rllib.agents.a3c import A3CTrainer, A2CTrainer
from ray.rllib.agents.ddpg import DDPGTrainer
from ray.rllib.agents.impala import ImpalaTrainer
from ray.rllib.agents.ppo import PPOTrainer, APPOTrainer, PPOTFPolicy
from ray.rllib.agents.sac import SACTrainer

# from ray.rllib.contrib.maddpg import MADDPGTrainer  # Bugs in the current version of RLLIB MADDPGTrainer
from ray.rllib.contrib.maddpg.maddpg_policy import MADDPGTFPolicy
from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv
from ray.tune.logger import NoopLogger

import aprl_defense.configs.train
from aprl_defense.agents.maddpg import MADDPGTrainer
from ext.envs.multiagent_particle_env import RLlibMultiAgentParticleEnv


def is_one_hot_agent(agent_name: str):
    if agent_name == 'maddpg':  # MADDPG is the only one-hot agent
        return True
    return False


trainer_map = {
    'maddpg': MADDPGTrainer,
    'ppo': PPOTrainer,
    'sac': SACTrainer,
    'a3c': A3CTrainer,
    'a2c': A2CTrainer,
    'ddpg': DDPGTrainer,  # Doesn't work in simple-push (discrete actions)
    'appo': APPOTrainer,
    'impala': ImpalaTrainer

}


# policies = {
#     'maddpg': MADDPGTFPolicy,
#     'ppo': PPOTFPolicy
# }

def trainer_cls_from_str(trainer_name: str):
    if trainer_name in trainer_map:
        return trainer_map[trainer_name]
    else:
        raise ValueError(f'Invalid RL algorithm: {trainer_name}')


def generate_multiagent(env, use_local_critic: List[bool], policies_to_train: Optional[List[str]]) -> dict:
    if isinstance(env, OpenSpielEnv):  # This env has a constant observation+action space for all agents
        # New theory: The following seems to only be necessary in ray 1.6 onwards
        # Yep, for some reason there observations have np.float64, while the original observation space is np.float32. I'm not sure where exactly the cause of
        # this problem is, so here is my hacky workaround. Unfortunately this means we reset the seed, because the original seed is not saved. Alternatively,
        # we could set the ._no_random attribute, but this doesn't seem necessary
        new_observation_space = Box(low=env.observation_space.low, high=env.observation_space.high, shape=env.observation_space.shape,
                                    dtype=np.float64, seed=None)
        observation_spaces = {agent_id: new_observation_space for agent_id in range(env.num_agents)}
        action_spaces = {agent_id: env.action_space for agent_id in range(env.num_agents)}
    elif isinstance(env, RLlibMultiAgentParticleEnv):
        observation_spaces = env.observation_space_dict
        action_spaces = env.action_space_dict

    num_policies = 2
    assert len(use_local_critic) == num_policies
    policies = {}
    for i in range(0, num_policies):
        policies[f'policy_{i}'] = (
            None,
            observation_spaces[i],
            action_spaces[i],
            {
                'agent_id': i,
                'use_local_critic': use_local_critic[i]
                # "obs_space_dict": env.observation_space_dict,
                # "act_space_dict": env.action_space_dict
            }
        )
    policy_ids = list(policies.keys())
    policy_mapping_func = ray.tune.function(lambda i, episode, **kwargs: policy_ids[i])

    multiagent = {'policies': policies,
                  'policy_mapping_fn': policy_mapping_func,
                  'policies_to_train': policies_to_train}

    return multiagent


def init_config(alg):
    # Get common config
    config = aprl_defense.configs.train.common
    # Update config with alg-specific settings
    config_for_alg = aprl_defense.configs.train.alg_specific.get(alg, {})
    config.update(config_for_alg)
    return config


def noop_logger_creator(config):
    """Creator functino for NoopLogger. Trainable receive a creator which has the single argument 'config'. Because this creates a NoopLogger the argument
    is never used"""
    return NoopLogger(config, "")


def create_trainer(trainer_cls, config):
    return trainer_cls(env='current-env', config=config, logger_creator=noop_logger_creator)


def policies_equal(policy_1, policy_2) -> bool:
    """Compare structural equality of two policies."""
    if isinstance(policy_1, OrderedDict) and isinstance(policy_2, OrderedDict):
        if not policy_1.keys() == policy_2.keys():
            return False
        for key in policy_1.keys():
            element_1: np.ndarray = policy_1[key]
            element_2: np.ndarray = policy_2[key]
            if not np.all(element_1 == element_2):
                return False
        return True
    else:
        raise NotImplementedError(f'Currently we only support RLLib policies which are ordered dicts (which applies to at least PPO)')
