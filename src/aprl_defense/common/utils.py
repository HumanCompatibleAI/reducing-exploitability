from typing import List, Optional

import ray
from ray.rllib.agents.a3c import A3CTrainer, A2CTrainer
from ray.rllib.agents.ddpg import DDPGTrainer
from ray.rllib.agents.impala import ImpalaTrainer
from ray.rllib.agents.ppo import PPOTrainer, APPOTrainer, PPOTFPolicy
from ray.rllib.agents.sac import SACTrainer

# from ray.rllib.contrib.maddpg import MADDPGTrainer  # Bugs in the current version of RLLIB MADDPGTrainer
from ray.rllib.contrib.maddpg.maddpg_policy import MADDPGTFPolicy
from ray.tune.logger import NoopLogger

import aprl_defense.configs.train
from aprl_defense.agents.maddpg import MADDPGTrainer


def is_one_hot_agent(agent_name: str):
    if agent_name == 'maddpg':  # MADDPG is the only one-hot agent
        return True
    return False


trainer_map = {
    'maddpg': MADDPGTrainer,
    'ppo': PPOTrainer,
    'sac': SACTrainer,
    'a3c': A3CTrainer,
    'a2c': A2CTrainer,
    'ddpg': DDPGTrainer,  # Doesn't work in simple-push (discrete actions)
    'appo': APPOTrainer,
    'impala': ImpalaTrainer

}


# policies = {
#     'maddpg': MADDPGTFPolicy,
#     'ppo': PPOTFPolicy
# }

def trainer_cls_from_str(trainer_name: str):
    if trainer_name in trainer_map:
        return trainer_map[trainer_name]
    else:
        raise ValueError(f'Invalid RL algorithm: {trainer_name}')


def generate_multiagent(env, use_local_critic: List[bool], policies_to_train: Optional[List[str]]) -> dict:
    num_policies = 2
    assert len(use_local_critic) == num_policies
    policies = {}
    for i in range(0, num_policies):
        policies[f'policy_{i}'] = (
            None,
            env.observation_space_dict[i],
            env.action_space_dict[i],
            {
                'agent_id': i,
                'use_local_critic': use_local_critic[i]
                # "obs_space_dict": env.observation_space_dict,
                # "act_space_dict": env.action_space_dict
            }
        )
    policy_ids = list(policies.keys())
    policy_mapping_func = ray.tune.function(lambda i: policy_ids[i])

    multiagent = {'policies': policies,
                  'policy_mapping_fn': policy_mapping_func,
                  'policies_to_train': policies_to_train}

    return multiagent


def init_config(alg):
    # Get common config
    config = aprl_defense.configs.train.common
    # Update config with alg-specific settings
    config_for_alg = aprl_defense.configs.train.alg_specific.get(alg, {})
    config.update(config_for_alg)
    return config


def noop_logger_creator(config):
    """Creator functino for NoopLogger. Trainable receive a creator which has the single argument 'config'. Because this creates a NoopLogger the argument
    is never used"""
    return NoopLogger(config, "")


def create_trainer(trainer_cls, config):
    return trainer_cls(env='mpe-push', config=config, logger_creator=noop_logger_creator)
