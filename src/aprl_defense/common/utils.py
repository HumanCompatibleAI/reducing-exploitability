from collections import OrderedDict
from typing import List, Optional, Dict

import numpy as np
import ray
from gym.spaces import Box
from ray.rllib import RolloutWorker, Policy, BaseEnv
from ray.rllib.agents import DefaultCallbacks
from ray.rllib.agents.a3c import A3CTrainer, A2CTrainer
from ray.rllib.agents.ddpg import DDPGTrainer
from ray.rllib.agents.impala import ImpalaTrainer
from ray.rllib.agents.ppo import PPOTrainer, APPOTrainer
from ray.rllib.agents.sac import SACTrainer

# from ray.rllib.contrib.maddpg import MADDPGTrainer  # Bugs in the current version of RLLIB MADDPGTrainer
from ray.rllib.env import PettingZooEnv
from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv
from ray.rllib.evaluation import MultiAgentEpisode
from ray.tune.logger import NoopLogger

import aprl_defense.configs.train
# from aprl_defense.agents.maddpg import MADDPGTrainer
from ext.envs.multiagent_particle_env import RLlibMultiAgentParticleEnv

trainer_map = {
    #    'maddpg': MADDPGTrainer,
    'ppo': PPOTrainer,
    'sac': SACTrainer,
    'a3c': A3CTrainer,
    'a2c': A2CTrainer,
    'ddpg': DDPGTrainer,  # Doesn't work in simple-push (discrete actions)
    'appo': APPOTrainer,
    'impala': ImpalaTrainer

}


def trainer_cls_from_str(trainer_name: str):
    if trainer_name in trainer_map:
        return trainer_map[trainer_name]
    else:
        raise ValueError(f'Invalid RL algorithm: {trainer_name}')


def generate_multiagent_2_policies(env, policies_to_train: Optional[List[str]]) -> dict:
    num_policies = 2

    policy_ids = []
    for i in range(0, num_policies):
        policy_ids.append(f'policy_{i}')

    if isinstance(env, PettingZooEnv):
        # For PZ the policies dict is created automatically, so we only need to pass the policy names
        agent_ids = list(env.agents)
        policies = set(policy_ids)
    else:  # if isinstance(env, RLlibMultiAgentParticleEnv) or isinstance(env, OpenSpielEnv):
        # Currently we need to create the policies dict for MPE and OS manually

        action_spaces, observation_spaces, agent_ids = spaces_from_env(env)
        policies = {}
        for i in range(0, num_policies):
            policies[f'policy_{i}'] = (
                None,
                observation_spaces[i],
                action_spaces[i],
                {
                    'agent_id': i,
                }
            )

    assert len(agent_ids) == 2, 'Currently only 2-agent environments are supported'

    agent_to_policy = {agent_id: policy_id for agent_id, policy_id in zip(agent_ids, policy_ids)}

    policy_mapping_func = ray.tune.function(lambda agent_id, episode, **kwargs: agent_to_policy[agent_id])

    multiagent = {'policies': policies,
                  'policy_mapping_fn': policy_mapping_func,
                  'policies_to_train': policies_to_train}

    return multiagent


def spaces_from_env(env):
    """Returns two dictionaries where the element at key i corresponds the action and observation spaces respectively for agent with id i."""
    if isinstance(env, OpenSpielEnv):  # This env has a constant observation+action space for all agents
        agent_ids = list(range(env.num_agents))
        # New theory: The following seems to only be necessary in ray 1.6 onwards
        # Yep, for some reason there observations have np.float64, while the original observation space is np.float32. I'm not sure where exactly the cause of
        # this problem is, so here is my hacky workaround. Unfortunately this means we reset the seed, because the original seed is not saved. Alternatively,
        # we could set the ._no_random attribute, but this doesn't seem necessary
        new_observation_space = Box(low=env.observation_space.low, high=env.observation_space.high, shape=env.observation_space.shape,
                                    dtype=np.float64)
        observation_spaces = {agent_id: new_observation_space for agent_id in agent_ids}
        action_spaces = {agent_id: env.action_space for agent_id in agent_ids}
    elif isinstance(env, RLlibMultiAgentParticleEnv):
        agent_ids = env.agent_ids
        observation_spaces = env.observation_space_dict
        action_spaces = env.action_space_dict
    elif isinstance(env, PettingZooEnv):
        agent_ids = list(env.agents)
        observation_spaces = env.observation_spaces
        action_spaces = env.action_spaces
    else:  # isinstance(env, gym.Env):  # Todo: better way to discern env type
        agent_ids = list(env.unwrapped.agents.keys())
        observation_spaces = env.observation_space
        action_spaces = env.action_space
    # else:
    #     raise NotImplementedError(f'Environment {env} with type {type(env)} not supported!')
    return action_spaces, observation_spaces, agent_ids


def get_base_train_config(alg):
    # Get common config
    config = aprl_defense.configs.train.common
    # Update config with alg-specific settings
    config_for_alg = aprl_defense.configs.train.alg_specific.get(alg, {})
    config.update(config_for_alg)
    return config


def noop_logger_creator(config):
    """Creator functino for NoopLogger. Trainable receive a creator which has the single argument 'config'. Because this creates a NoopLogger the argument
    is never used"""
    return NoopLogger(config, "")


def create_trainer(trainer_cls, config):
    return trainer_cls(env='current-env', config=config, logger_creator=noop_logger_creator)


def policies_equal(policy_1, policy_2) -> bool:
    """Compare structural equality of two policies."""
    if ((isinstance(policy_1, OrderedDict) and isinstance(policy_2, OrderedDict))
            or (isinstance(policy_1, dict) and isinstance(policy_2, dict))):
        if not policy_1.keys() == policy_2.keys():
            return False
        for key in policy_1.keys():
            element_1: np.ndarray = policy_1[key]
            element_2: np.ndarray = policy_2[key]
            if not np.all(element_1 == element_2):
                return False
        return True
    else:
        raise NotImplementedError(f'Currently we only support RLLib policies which are ordered dicts (which applies to at least PPO)')


def dict_from_tuple(tuple):
    """Create a dict with tuple index as key from given tuple."""
    dictionary = {}
    for i, e in enumerate(tuple):
        dictionary[i] = e

    return dictionary


class CustomMujocoMetricsCallbacks(DefaultCallbacks):
    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,
                       policies: Dict[str, Policy], episode: MultiAgentEpisode,
                       **kwargs):
        log_dict = {}
        for agent_id in range(worker.env.unwrapped.num_agents):
            last_info = episode.last_info_for(agent_id)
            for rew_type, rew in last_info['logged_agent_rewards'].items():
                log_dict[f'agent_{agent_id}_{rew_type}_reward'] = rew

            log_dict[f'agent_{agent_id}_dense_weight'] = last_info['dense_weight']

        episode.custom_metrics.update(log_dict)
