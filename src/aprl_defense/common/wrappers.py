import pickle

import gym
import numpy as np
import ray.rllib
from mujoco_py import MjSimState
from mujoco_py.builder import MujocoException
from ray.rllib.env.multi_agent_env import MultiAgentEnv


class Float64To32Wrapper(gym.core.ObservationWrapper):
    # Since ray 1.6 I encountered problems where the observations were float64 ndarrays, while the observation_space was set to float32 (as the default).
    # I didn't investigate further what the underlying problem is (maybe because np defaults to float64?)

    def observation(self, observation: np.ndarray):
        return observation.astype(np.float32)


class MujocoToRllibWrapper(ray.rllib.MultiAgentEnv):
    """Stable baselines (multi vec env as wrapped from Mujoco) to RLlib wrapper"""

    # def __init__(self, env):
    #     ObservationWrapper.__init__(self, env)

    def __init__(self, env):
        super().__init__()
        self.env = env
        # self.action_space = self.env.action_space
        # self.observation_space = self.env.observation_space
        # Iterate over tuples to create dicts for observation and action space
        observation_space = dict_from_tuple(env.observation_space)
        self.observation_space = gym.spaces.Dict(spaces=observation_space)
        action_space = dict_from_tuple(env.action_space)
        self.action_space = gym.spaces.Dict(spaces=action_space)
        # self.reward_range = self.env.reward_range
        self.metadata = self.env.metadata
        self.action_buffer = []
        self.obs_buffer = []
        self.reward_buffer = []

    def __getattr__(self, name):
        if name.startswith("_"):
            raise AttributeError(
                "attempted to get missing private attribute '{}'".format(name)
            )
        return getattr(self.env, name)

    # @property
    # def spec(self):
    #     return self.env.spec
    #
    # @classmethod
    # def class_name(cls):
    #     return cls.__name__

    def step(self, action):
        # Transform back from dict to sequence
        action = list(action.values())

        self.action_buffer.append(action)
        if len(self.action_buffer) >= 50:
            self.action_buffer.pop(0)

        if any([(a > 1_000_000_000).any() for a in action]):
            print("WARNING, encountered huge action")

        # for a in action:
        #     if np.isnan(a).any():
        #         print('Nan in action!')

        try:
            obs, rews, done, info = self.env.step(action)

            # for o in obs:
            #     if np.isnan(o).any():
            #         print('Nan in observation!')
            #
            # for r in rews:
            #     if np.isnan(r).any():
            #         print('Nan in rewards!')
        except MujocoException as e:
            # Dump the mujoco state
            self._dump_mujoco_state(self.env, 'MUJOCO_STATE_MujocoException.pkl', action)
            raise e
        except ValueError as e:
            # Dump the mujoco state
            self._dump_mujoco_state(self.env, 'MUJOCO_STATE_ValueError.pkl', action)
            raise e
        except Exception as e:
            self._dump_mujoco_state(self.env, f'MUJOCO_STATE_{type(e).__name__}.pkl', action)
            raise e

        self.obs_buffer.append(obs)
        self.reward_buffer.append(rews)
        if len(self.obs_buffer) >= 50:
            self.obs_buffer.pop(0)
        if len(self.reward_buffer) >= 50:
            self.reward_buffer.pop(0)

        # I think the most accurate way to map Mujoco's done to RLlib's done is by using agent_done
        # And I assume the original done represents done['__all__']
        done_dict = {}
        for key in info.keys():
            done_dict[key] = info[key]['agent_done']
        # We set to done for everyone when at least one agent is done. Otherwise we would run into problems because RLlib doesn't sample actions for agents that
        # are already done, but mujoco expects actions from all agents
        done_dict['__all__'] = True in done_dict.values()
        return dict_from_tuple(obs), dict_from_tuple(rews), done_dict, info

    def _dump_mujoco_state(self, env, file_name, last_action):
        last_obs = self.obs_buffer[-1]
        state = {'state_vector': env.state_vector(),
                 'last_action': last_action,
                 'last_obs': last_obs,
                 'action_buffer': self.action_buffer,
                 'obs_buffer': self.obs_buffer,
                 'reward_buffer': self.reward_buffer}
        # Handles some rare edge cases
        try:
            state['qacc'] = env.unwrapped.env_scene.data.qacc
        except AttributeError:
            pass
        try:
            state['qacc_warmstart'] = env.unwrapped.env_scene.data.qacc_warmstart
        except AttributeError:
            pass
        try:
            state['act'] = env.unwrapped.env_scene.data.act
        except AttributeError:
            pass
        file = open(file_name, mode='wb')
        pickle.dump(state, file)
        file.close()

    def reset(self, **kwargs):
        obs = self.env.reset(**kwargs)

        return dict_from_tuple(obs)

    def render(self, mode="human", **kwargs):
        return self.env.render(mode, **kwargs)

    def close(self):
        return self.env.close()

    # This causes problems but might be nicessary at some point
    # @property
    # def unwrapped(self):
    #     return self.env.unwrapped

    # def seed(self, seed=None):
    #     return self.env.seed(seed)
    #
    # def compute_reward(self, achieved_goal, desired_goal, info):
    #     return self.env.compute_reward(achieved_goal, desired_goal, info)
    #
    # def __str__(self):
    #     return "<{}{}>".format(type(self).__name__, self.env)
    #
    # def __repr__(self):
    #     return str(self)


class MujocoEnvFromStateWrapper(gym.Wrapper):
    """Stable baselines (multi vec env as wrapped from Mujoco) to RLlib wrapper"""

    # def __init__(self, env):
    #     ObservationWrapper.__init__(self, env)

    def __init__(self, env, state):
        super().__init__(env)
        self.env = env
        # self.action_space = self.env.action_space
        # self.observation_space = self.env.observation_space
        # Iterate over tuples to create dicts for observation and action space
        # observation_space = env.observation_space
        # self.observation_space = env.observation_space
        # action_space = env.action_space
        # self.action_space = env.action_space
        # self.reward_range = self.env.reward_range
        # self.metadata = self.env.metadata
        # self.action_buffer = []
        # self.obs_buffer = []
        # self.reward_buffer = []
        self.state_set = False
        self.state: dict = state

    # def __getattr__(self, name):
    #     if name.startswith("_"):
    #         raise AttributeError(
    #             "attempted to get missing private attribute '{}'".format(name)
    #         )
    #     return getattr(self.env, name)

    def step(self, action: list):
        if not self.state_set:
            self.state_set = True

            print(f'State keys: {self.state.keys()}')
            qacc = self.state['qacc']

            last_action = self.state['last_action']
            last_obs = self.state['last_obs']
            qpos = self.state['state_vector'][0:self.env.env_scene.model.nq]
            qvel = self.state['state_vector'][self.env.env_scene.model.nq:]
            state = MjSimState.from_flattened(self.state['state_vector'], self.env.env_scene.sim)
            self.env.env_scene.set_state(qpos, qvel)
            state.act = state['act']
            self.env.env_scene.sim.set_state(state)
            self.env.env_scene.sim.forward()
            action = last_action
        if any([(a > 1_000_000_000).any() for a in action]):
            print("WARNING")
        return self.env.step(action)

    def reset(self, **kwargs):
        obs = self.env.reset(**kwargs)

        return obs

    def render(self, mode="human", **kwargs):
        return self.env.render(mode, **kwargs)

    def close(self):
        return self.env.close()

    # @property
    # def unwrapped(self):
    #     return self.env.unwrapped


class AsymmetricPettingZooEnv(MultiAgentEnv):

    def __init__(self, env):
        self.env = env
        # agent idx list
        self.agents = self.env.possible_agents

        # Get dictionaries of obs_spaces and act_spaces
        self.observation_spaces = self.env.observation_spaces
        self.action_spaces = self.env.action_spaces

        # Get first observation space, assuming all agents have equal space
        self.observation_space = self.observation_spaces[self.agents[0]]

        # Get first action space, assuming all agents have equal space
        self.action_space = self.action_spaces[self.agents[0]]

        # assert all(obs_space == self.observation_space
        #            for obs_space
        #            in self.env.observation_spaces.values()), \
        #     "Observation spaces for all agents must be identical. Perhaps " \
        #     "SuperSuit's pad_observations wrapper can help (useage: " \
        #     "`supersuit.aec_wrappers.pad_observations(env)`"
        #
        # assert all(act_space == self.action_space
        #            for act_space in self.env.action_spaces.values()), \
        #     "Action spaces for all agents must be identical. Perhaps " \
        #     "SuperSuit's pad_action_space wrapper can help (useage: " \
        #     "`supersuit.aec_wrappers.pad_action_space(env)`"

        self.reset()

    # def __getattr__(self, name):
    #     if name.startswith("_"):
    #         raise AttributeError(
    #             "attempted to get missing private attribute '{}'".format(name)
    #         )
    #     return getattr(self.env, name)

    def reset(self):
        self.env.reset()
        return {
            self.env.agent_selection: self.env.observe(
                self.env.agent_selection)
        }

    def step(self, action):
        self.env.step(action[self.env.agent_selection])
        obs_d = {}
        rew_d = {}
        done_d = {}
        info_d = {}
        while self.env.agents:
            obs, rew, done, info = self.env.last()
            a = self.env.agent_selection
            obs_d[a] = obs
            rew_d[a] = rew
            done_d[a] = done
            info_d[a] = info
            if self.env.dones[self.env.agent_selection]:
                self.env.step(None)
            else:
                break

        all_done = not self.env.agents
        done_d["__all__"] = all_done

        return obs_d, rew_d, done_d, info_d

    def close(self):
        self.env.close()

    def seed(self, seed=None):
        self.env.seed(seed)

    def render(self, mode="human"):
        return self.env.render(mode)


def dict_from_tuple(tuple):
    """Create a dict with tuple index as key from given tuple."""
    dictionary = {}
    for i, e in enumerate(tuple):
        dictionary[i] = e

    return dictionary
