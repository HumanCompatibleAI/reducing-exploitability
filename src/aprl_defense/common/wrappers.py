import gym
import numpy as np
import ray.rllib
from gym.core import Wrapper, ObservationWrapper

from aprl_defense.common.utils import dict_from_tuple


class Float64To32Wrapper(gym.core.ObservationWrapper):
    # Since ray 1.6 I encountered problems where the observations were float64 ndarrays, while the observation_space was set to float32 (as the default).
    # I didn't investigate further what the underlying problem is (maybe because np defaults to float64?)

    def observation(self, observation: np.ndarray):
        return observation.astype(np.float32)


class SBToRLlibWrapper(ray.rllib.MultiAgentEnv):
    """Stable baselines (multi vec env as wrapped from Mujoco) to RLlib wrapper"""

    # def __init__(self, env):
    #     ObservationWrapper.__init__(self, env)

    def __init__(self, env):
        self.env = env
        # self.action_space = self.env.action_space
        # self.observation_space = self.env.observation_space
        # Iterate over tuples to create dicts for observation and action space
        observation_space = dict_from_tuple(env.observation_space)
        self.observation_space = gym.spaces.Dict(spaces=observation_space)
        action_space = dict_from_tuple(env.action_space)
        self.action_space = gym.spaces.Dict(spaces=action_space)
        # self.reward_range = self.env.reward_range
        self.metadata = self.env.metadata

    def __getattr__(self, name):
        if name.startswith("_"):
            raise AttributeError(
                "attempted to get missing private attribute '{}'".format(name)
            )
        return getattr(self.env, name)

    # @property
    # def spec(self):
    #     return self.env.spec
    #
    # @classmethod
    # def class_name(cls):
    #     return cls.__name__

    def step(self, action):
        # Transform back from dict to sequence
        action = list(action.values())
        obs, rews, done, info = self.env.step(action)

        # I think the most accurate way to map Mujocos done to RLlibs done is by using agent_done
        # And I assume the original done represents done['__all__']
        done_dict = {}
        for key in info.keys():
            done_dict[key] = info[key]['agent_done']
        done_dict['__all__'] = done
        return dict_from_tuple(obs), dict_from_tuple(rews), done_dict, info

    def reset(self, **kwargs):
        obs = self.env.reset(**kwargs)

        return dict_from_tuple(obs)

    def render(self, mode="human", **kwargs):
        return self.env.render(mode, **kwargs)

    def close(self):
        return self.env.close()

    @property
    def unwrapped(self):
        return self.env.unwrapped

    # def seed(self, seed=None):
    #     return self.env.seed(seed)
    #
    # def compute_reward(self, achieved_goal, desired_goal, info):
    #     return self.env.compute_reward(achieved_goal, desired_goal, info)
    #
    # def __str__(self):
    #     return "<{}{}>".format(type(self).__name__, self.env)
    #
    # def __repr__(self):
    #     return str(self)
