import json
from abc import ABC, abstractmethod
from copy import deepcopy
from typing import Optional

import ray
import wandb
from mujoco_py import MujocoException
from ray.rllib.agents import MultiCallbacks
from ray.tune import Trainable
from ray.tune.logger import pretty_print
from tqdm import tqdm

import aprl_defense
import aprl_defense.common.utils
import aprl_defense.configs.common
import aprl_defense.configs.train
from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.rllib_io import save_params
from aprl_defense.common.train import init_env, load_saved_weights, load_saved_checkpoint
from aprl_defense.common.utils import get_base_train_config, noop_logger_creator
from aprl_defense.common.utils import trainer_cls_from_str, CustomMujocoMetricsCallbacks, CustomStatsCallback
from aprl_defense.pbt.utils import custom_eval_log
from aprl_defense.trial.settings import RLSettings, TrialSettings
from ext.aprl.training.scheduling import Scheduler


class TrainingManager(ABC):
    def __init__(self,
                 trial_settings: TrialSettings,
                 rl_settings: RLSettings):
        self.trial = trial_settings
        self.rl = rl_settings

        # Initialize these later
        self.scheduler = None
        self.config = None
        self.trainer: Optional[Trainable] = None

        self._init_env()

        self.trainer_class = trainer_cls_from_str(self.rl.alg)

    def _init_env(self):
        if self.rl.env.startswith('mpe_'):
            scenario_name = self.rl.env[4:]  # Remove the prefix
            self.env_name = 'mpe'
        elif self.rl.env.startswith('gym_'):
            self.env_name = 'gym'
            scenario_name = self.rl.env[4:]
        elif self.rl.env.startswith('os_'):
            self.env_name = 'open_spiel'
            scenario_name = self.rl.env[3:]  # Remove the prefix
        elif self.rl.env.startswith('pz_'):
            scenario_name = self.rl.env[3:]  # Remove the prefix
            self.env_name = 'pettingzoo'
        elif self.rl.env.startswith('mc_'):
            scenario_name = self.rl.env[3:]  # Remove the prefix
            self.env_name = 'multicomp'
            # Use scheduler
            SchedulerActor = ray.remote(Scheduler)
            self.scheduler = SchedulerActor.remote()
        else:
            raise ValueError(f'Env {self.rl.env} not supported!')
        self.scenario_name = scenario_name

        self.env = init_env(self.env_name, self.scenario_name, self.scheduler, mujoco_state=self.rl.state)

    def _log_config(self, config, config_file_name):
        config_log = pretty_print(config)
        config_file_path = self.trial.out_path / config_file_name
        ray_config_file = open(config_file_path, mode='wt')
        ray_config_file.write(config_log)
        wandb.save(str(config_file_path))

    def train(self):
        # Config setup
        self.set_up_config()
        self._handle_config_override(self.trial.override)

        # Save the ray config in wandb
        self.trial.out_path.mkdir(exist_ok=True, parents=True)  # Create the out folder
        config_file_name = 'ray_config.txt'
        self._log_config(self.config, config_file_name)
        # Save config to be used with rllibs's checkpoint loading (rllib expects a pickled config object)
        save_params(self.config, self.trial.out_path)

        """Main function to be called to start training. Performs setup for config, trainer and starts training loop."""
        # Trainer setup
        self.set_up_trainer()
        # Set trainer for artifact manager, as ArtifactManager needs reference to the trainable
        self.trial.artifact_manager.trainer = self.trainer
        # The previously logged config only contains the values that are different from the default, use this config to see all config params that ray uses
        self._log_config(self.trainer.config, 'ray_config_after_init.txt')

        # Start training
        print(f"Starting run, output saved at {self.trial.out_path}")
        # Start the actual training
        self.start_training_loop()

    def set_up_config(self):
        print("Initializing config")
        self.config = get_base_train_config(self.rl.alg)
        if self.trial.num_workers is not None:
            self.config['num_workers'] = self.trial.num_workers

        self.config['train_batch_size'] = self.rl.train_batch_size

        multi_callbacks = self.get_callbacks()
        self.config['callbacks'] = multi_callbacks

    def get_callbacks(self):
        callbacks = [CustomStatsCallback]
        if self.env_name == 'multicomp':
            # Add mujoco logging callback
            # This callback logs the reward shaping
            callbacks.append(CustomMujocoMetricsCallbacks)
        multi_callbacks = MultiCallbacks(callbacks)
        return multi_callbacks

    @abstractmethod
    def set_up_trainer(self) -> None:
        pass

    def generic_trainer_setup(self, policies_to_train):
        """Can be used by subclasses to implement trainer setup"""

        self.trainer = create_ma_trainer(policies_to_train, self.config, self.scenario_name, self.env, self.trainer_class)

    @abstractmethod
    def start_training_loop(self):
        pass

    def _handle_config_override(self, override: Optional[str]):
        if override is not None:
            # Convert json string into dict
            override_params = json.loads(override)
            # Update internal config with new override vals
            self.config.update(override_params)
        if self.trial.override_f is not None:
            json_file = open(self.trial.override_f)
            # Convert json string into dict
            override_params = json.load(json_file)
            # Update internal config with new override vals
            self.config.update(override_params)

    def _run_trainer_helper(self, trainer,
                            checkpoint_freq,
                            max_timesteps,
                            artifact_manager: ArtifactManager,
                            log_setup=None):
        next_checkpoint = checkpoint_freq

        pbar = tqdm(total=max_timesteps)

        timesteps_total = 0
        while timesteps_total < max_timesteps:

            # last_obs = self.env.env.state['last_obs']
            # actions = []
            # done = False
            # while not done:
            #     for agent_id, a_obs in enumerate(last_obs):
            #         if a_obs is not None:
            #             policy_id = f'policy_{agent_id}'
            #             state = self.trainer.get_policy(policy_id).get_initial_state()
            #             a_action, p_state, _ = self.trainer.compute_single_action(
            #                 a_obs, policy_id=policy_id, state=state)
            #             actions.append(a_action)
            #             if (a_action > 1_000_000_000).any():
            #                 print('Warning')

            results = trainer.train()

            timesteps_total = results['timesteps_total']

            if self.scheduler is not None:
                frac_remaining = (max_timesteps - timesteps_total) / max_timesteps
                _ = ray.get(self.scheduler.get_val.remote('rew_shape', frac_remaining))  # Update frac remaining for scheduler

            # for policy in trainer.config['multiagent']['policies']:
            if log_setup is not None:
                for policy, name in log_setup:

                    # Check that the metric for this policy was actually collected
                    if policy not in results['policy_reward_mean']:
                        # raise ValueError(f"No result values collected. Is 'horizon' set?")
                        pass
                    else:
                        value = results['policy_reward_mean'][policy]
                        wandb.log({f'{name}_reward': value,
                                   'timestep': timesteps_total})
                if len(results['custom_metrics']) != 0:
                    log_dict = {'timestep': timesteps_total}
                    log_dict.update(results['custom_metrics'])
                    wandb.log(log_dict)

            custom_eval_log(results, timesteps_total, timesteps_total)

            # Save intermediate checkpoint if necessary
            if checkpoint_freq != -1 and timesteps_total > next_checkpoint:
                artifact_manager.save_new_checkpoint()

                next_checkpoint += checkpoint_freq  # We save the next checkpoint over this threshold

            pbar.update(timesteps_total - pbar.n)

        pbar.close()

        # Save last checkpoint at the end
        artifact_manager.save_new_checkpoint()
        print("checkpoint saved")
        return timesteps_total


class NormalTrainingManager(TrainingManager):

    def __init__(self,
                 trial_settings: TrialSettings,
                 rl_settings: RLSettings,
                 train_only_one_policy=False):
        super().__init__(trial_settings, rl_settings)
        self.train_only_one_policy = train_only_one_policy

    def start_training_loop(self):
        log_setup = [('policy_0', 'policy_0'), ('policy_1', 'policy_1')]
        try:
            self._run_trainer_helper(self.trainer,
                                     self.trial.checkpoint_freq,
                                     self.rl.max_timesteps,
                                     artifact_manager=self.trial.artifact_manager,
                                     log_setup=log_setup)
        except MujocoException as e:
            self.trial.artifact_manager.save_new_checkpoint()
            raise e

    def set_up_trainer(self):
        if self.train_only_one_policy:
            policies_to_train = ['policy_1']
        else:
            policies_to_train = None  # None == all policies train
        if self.trial.continue_artifact is None:
            self.generic_trainer_setup(policies_to_train)
        else:
            # Use saved trainer from checkpoint
            file = self.trial.artifact_manager.get_remote_checkpoint(self.trial.continue_artifact)
            additional_config = {'multiagent': aprl_defense.common.utils.generate_multiagent_2_policies(self.env, policies_to_train),
                                 'callbacks': self.get_callbacks()}
            self.trainer = load_saved_checkpoint(self.scenario_name, self.trainer_class, file, additional_config=additional_config)


class FinetuneManager(TrainingManager):
    def __init__(self,
                 trial_settings: TrialSettings,
                 rl_settings: RLSettings,
                 victim_artifact: str,
                 adversary_id,
                 specific_folder=False,
                 victim_policy_name=None):
        super().__init__(trial_settings, rl_settings)

        self.victim_artifact = victim_artifact
        self.adversary_id = adversary_id

        self.victim_id = 1 - adversary_id

        if victim_policy_name is None:
            self.victim_policy_name = f'policy_{self.victim_id}'
            print(f'Using {self.victim_policy_name} as automatic fallback because no victim policy name was provided')
        else:
            self.victim_policy_name = victim_policy_name

        self.specific_folder = specific_folder

    def start_training_loop(self):
        trainer = self.trainer
        adv_name = f'policy_{self.adversary_id}'
        new_victim_name = f'policy_{self.victim_id}'

        file = self.trial.artifact_manager.get_remote_checkpoint(self.victim_artifact)

        victim_weights = load_saved_weights(file, self.scenario_name, self.victim_policy_name, self.trainer_class)

        if self.victim_policy_name not in victim_weights:
            raise ValueError(f'No policy named {self.victim_policy_name} among victim weights. Please provide the correct name of the victim policy.')

        # Overwrite the weights for the agent that will not be trained (policy_1)
        trainer.set_weights({new_victim_name: victim_weights[self.victim_policy_name]})

        trainer.workers.sync_weights()  # IMPORTANT!!1!!1

        log_setup = [(adv_name, 'adversary')]
        self._run_trainer_helper(trainer,
                                 self.trial.checkpoint_freq,
                                 self.rl.max_timesteps,
                                 artifact_manager=self.trial.artifact_manager,
                                 log_setup=log_setup)

    def set_up_trainer(self):
        policies_to_train = [f'policy_{self.adversary_id}']
        self.generic_trainer_setup(policies_to_train)


class SingleAgentTrainingManager(TrainingManager):
    def __init__(self,
                 trial_settings: TrialSettings,
                 rl_settings: RLSettings):
        super().__init__(trial_settings, rl_settings)

    def start_training_loop(self):
        log_setup = [('policy_0', 'policy_0')]
        self._run_trainer_helper(self.trainer,
                                 self.trial.checkpoint_freq,
                                 self.rl.max_timesteps,
                                 artifact_manager=self.trial.artifact_manager,
                                 log_setup=log_setup)

    def set_up_trainer(self):
        if self.trial.continue_artifact is None:
            # This general setup only allows one type of algorithm
            if len(self.trainer_class) > 1:
                raise ValueError("Only one algorithm allowed.")
            self.config['env_config']['scenario_name'] = self.scenario_name
            # Create a new maddpg trainer which will be used to train the adversarial policy
            self.trainer = self.trainer_class(env='current-env', config=deepcopy(self.config), logger_creator=noop_logger_creator)
        else:
            # Use saved trainer from checkpoint
            file = self.trial.artifact_manager.get_remote_checkpoint(self.trial.continue_artifact)
            self.trainer = load_saved_checkpoint(self.scenario_name, self.trainer_class, file)


def create_ma_trainer(policies_to_train, config, scenario_name, env, trainer_cls):
    """Create new trainer with new weights"""
    multiagent = aprl_defense.common.utils.generate_multiagent_2_policies(env, policies_to_train)
    config['env_config']['scenario_name'] = scenario_name
    config['multiagent'] = multiagent
    # Update config with some settings that apply to all configs regardless of algorithm/trainer
    # common_config = aprl_defense.configs.common.train
    # config.update(common_config)
    # Create a new maddpg trainer which will be used to train the adversarial policy
    new_trainer = trainer_cls(env='current-env', config=deepcopy(config), logger_creator=noop_logger_creator)
    return new_trainer
