import json
from abc import ABC, abstractmethod
from typing import Optional

import ray
import wandb
from copy import deepcopy

from ray.rllib.agents import MultiCallbacks
from tqdm import tqdm

import aprl_defense.common.utils
from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.rllib_io import save_params
from aprl_defense.common.train import init_env, load_saved_weights
import aprl_defense
import aprl_defense.configs.train
import aprl_defense.configs.common

from ray.tune.logger import pretty_print

from aprl_defense.common.utils import init_config, noop_logger_creator

from aprl_defense.common.utils import is_one_hot_agent, trainer_cls_from_str, CustomMujocoMetricsCallbacks
from aprl_defense.pbt.utils import custom_eval_log
from aprl_defense.trial.settings import RLSettings, TrialSettings
from ext.aprl.training.scheduling import Scheduler


class TrainingManager(ABC):
    def __init__(self,
                 trial_settings: TrialSettings,
                 rl_settings: RLSettings):
        self.trial = trial_settings
        self.rl = rl_settings

        callbacks = []
        self.scheduler = None
        # Todo separate environments into source and scenario / env
        if self.rl.env.startswith('mpe_'):
            scenario_name = self.rl.env[4:]  # Remove the prefix
            self.env_name = 'mpe'
        elif self.rl.env.startswith('os_'):
            self.env_name = 'open_spiel'
            scenario_name = self.rl.env[3:]  # Remove the prefix
        elif self.rl.env.startswith('pz_'):
            scenario_name = self.rl.env[3:]  # Remove the prefix
            self.env_name = 'pettingzoo'
        elif self.rl.env.startswith('mc_'):
            scenario_name = self.rl.env[3:]  # Remove the prefix
            self.env_name = 'multicomp'
            callbacks.append(CustomMujocoMetricsCallbacks)

            SchedulerActor = ray.remote(Scheduler)

            self.scheduler = SchedulerActor.remote()
        else:
            raise ValueError(f'Env {self.rl.env} not supported!')

        self.scenario_name = scenario_name

        self.trainer_classes = [trainer_cls_from_str(alg) for alg in self.rl.algs]

        self.config = init_config(self.rl.algs[0])

        if self.trial.num_workers is not None:
            self.config['num_workers'] = self.trial.num_workers

        # Handle algorithm-specific arguments
        if self.rl.algs[0] == 'ppo' and self.rl.num_sgd_iter is not None:
            self.config['num_sgd_iter'] = self.rl.num_sgd_iter

        self.config['train_batch_size'] = self.rl.train_batch_size

        self.config['callbacks'] = MultiCallbacks(callbacks)

        self._handle_config_override(self.trial.override)

        if is_one_hot_agent(self.rl.algs[0]):  # maddpg is only alg that does not return discrete actions. Instead returns one-hot actions
            one_hot_agents = [True, True]
        else:
            one_hot_agents = [False, False]
        self.env = init_env(self.env_name, self.scenario_name, self.scheduler, one_hot_agents=one_hot_agents)

    def train(self):
        print(f"Saving output to {self.trial.out_path}")

        use_local_critic = self._get_use_local_critic()

        policies_to_train = self._get_policies_to_train()
        trainer = create_ma_trainer(policies_to_train, use_local_critic, self.config,
                                    self.scenario_name, self.env, self.trainer_classes)

        print('Config:')
        print(pretty_print(trainer.config))

        # ArtifactManager needs reference to the trainable
        self.trial.artifact_manager.trainer = trainer

        save_params(self.config, self.trial.out_path)

        # Start the actual training
        self._train_loop(trainer)

    @abstractmethod
    def _get_policies_to_train(self):
        raise NotImplementedError()

    @abstractmethod
    def _get_use_local_critic(self):
        raise NotImplementedError()

    @abstractmethod
    def _train_loop(self, trainer):
        raise NotImplementedError()

    def _handle_config_override(self, override: Optional[str]):
        if override is not None:
            # Convert json string into dict
            override_params = json.loads(override)
            # Update internal config with new override vals
            self.config.update(override_params)

    def _run_trainer_helper(self, trainer,
                            checkpoint_freq,
                            max_timesteps,
                            artifact_manager: ArtifactManager,
                            log_setup=None):
        next_checkpoint = checkpoint_freq

        pbar = tqdm(total=max_timesteps)

        timesteps_total = 0
        while timesteps_total < max_timesteps:
            results = trainer.train()

            timesteps_total = results['timesteps_total']

            if self.scheduler is not None:
                frac_remaining = (max_timesteps - timesteps_total) / max_timesteps
                _ = ray.get(self.scheduler.get_val.remote('rew_shape', frac_remaining))  # Update frac remaining for scheduler

            # for policy in trainer.config['multiagent']['policies']:
            if log_setup is not None:
                for policy, name in log_setup:

                    # Check that the metric for this policy was actually collected
                    if policy not in results['policy_reward_mean']:
                        # raise ValueError(f"No result values collected. Is 'horizon' set?")
                        pass
                    else:
                        value = results['policy_reward_mean'][policy]
                        wandb.log({f'{name}_reward': value,
                                   'timestep': timesteps_total})
                if len(results['custom_metrics']) != 0:
                    log_dict = {'timestep': timesteps_total}
                    log_dict.update(results['custom_metrics'])
                    wandb.log(log_dict)

            custom_eval_log(results, timesteps_total, timesteps_total)

            # Save intermediate checkpoint if necessary
            if checkpoint_freq != -1 and timesteps_total > next_checkpoint:
                artifact_manager.save_new_checkpoint()

                next_checkpoint += checkpoint_freq  # We save the next checkpoint over this threshold

            pbar.update(timesteps_total - pbar.n)

        pbar.close()

        # Save last checkpoint at the end
        artifact_manager.save_new_checkpoint()
        print("checkpoint saved")
        return timesteps_total


class NormalTrainingManager(TrainingManager):
    def _train_loop(self, trainer):
        log_setup = [('policy_0', 'policy_0'), ('policy_1', 'policy_1')]
        self._run_trainer_helper(trainer,
                                 self.trial.checkpoint_freq,
                                 self.rl.max_timesteps,
                                 artifact_manager=self.trial.artifact_manager,
                                 log_setup=log_setup)

    def _get_use_local_critic(self):
        use_local_critic = [False, False]
        return use_local_critic

    def _get_policies_to_train(self):
        policies_to_train = None  # None == all policies train
        return policies_to_train


class FinetuneManager(TrainingManager):
    def __init__(self,
                 trial_settings: TrialSettings,
                 rl_settings: RLSettings,
                 victim_artifact: str,
                 adversary_id,
                 specific_folder=False,
                 victim_policy_name=None):
        super().__init__(trial_settings, rl_settings)

        self.victim_artifact = victim_artifact
        self.adversary_id = adversary_id

        self.victim_id = 1 - adversary_id

        if victim_policy_name is None:
            self.victim_policy_name = f'policy_{self.victim_id}'
            print(f'Using {self.victim_policy_name} as automatic fallback because no victim policy name was provided')
        else:
            self.victim_policy_name = victim_policy_name

        self.specific_folder = specific_folder

    def _train_loop(self, trainer):
        adv_name = f'policy_{self.adversary_id}'
        new_victim_name = f'policy_{self.victim_id}'

        file = self.trial.artifact_manager.get_remote_checkpoint(self.victim_artifact)

        victim_weights = load_saved_weights(file, self.scenario_name, self.victim_policy_name, self.trainer_classes)

        if self.victim_policy_name not in victim_weights:
            raise ValueError(f'No policy named {self.victim_policy_name} among victim weights. Please provide the correct name of the victim policy.')

        # Overwrite the weights for the agent that will not be trained (policy_1)
        trainer.set_weights({new_victim_name: victim_weights[self.victim_policy_name]})

        trainer.workers.sync_weights()  # IMPORTANT!!1!!1

        log_setup = [(adv_name, 'adversary')]
        self._run_trainer_helper(trainer,
                                 self.trial.checkpoint_freq,
                                 self.rl.max_timesteps,
                                 artifact_manager=self.trial.artifact_manager,
                                 log_setup=log_setup)

    def _get_use_local_critic(self):
        use_local_critic = [False, False]
        # Finetune on saved model
        use_local_critic[self.adversary_id] = True  # Adversarial policy does not have access to shared critic
        return use_local_critic

    def _get_policies_to_train(self):
        policies_to_train = [f'policy_{self.adversary_id}']
        return policies_to_train


def create_ma_trainer(policies_to_train, use_local_critic, config, scenario_name, env, trainer_cls):
    """Create new trainer with new weights"""
    multiagent = aprl_defense.common.utils.generate_multiagent_2_policies(env, use_local_critic, policies_to_train)
    config['env_config']['scenario_name'] = scenario_name
    config['multiagent'] = multiagent
    # Update config with some settings that apply to all configs regardless of algorithm/trainer
    # common_config = aprl_defense.configs.common.train
    # config.update(common_config)
    # Create a new maddpg trainer which will be used to train the adversarial policy
    new_trainer = trainer_cls(env='current-env', config=deepcopy(config), logger_creator=noop_logger_creator)
    return new_trainer
