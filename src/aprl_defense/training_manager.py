import json
from abc import ABC, abstractmethod
from collections import defaultdict
from pathlib import Path
from typing import Optional

import numpy as np
import wandb
from copy import deepcopy
import random

from ray.rllib.agents import MultiCallbacks
from tqdm import tqdm

import aprl_defense.common.utils
from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.rllib_io import save_params
from aprl_defense.common.train import run_trainer, init_env, load_saved_weights
import aprl_defense
import aprl_defense.configs.train
import aprl_defense.configs.common

from ray.tune.logger import pretty_print

from aprl_defense.common.utils import init_config, noop_logger_creator

from aprl_defense.common.utils import is_one_hot_agent, trainer_cls_from_str, CustomMujocoMetricsCallbacks
from aprl_defense.pbt.utils import custom_eval_log


class TrainingManager(ABC):
    def __init__(self, env: str, log_dir: Path, alg: str, max_timesteps: int, checkpoint_freq: int, num_workers: Optional[int], override: str,
                 artifact_manager: ArtifactManager, num_sgd_iter: Optional[int], train_batch_size: int):
        self.max_timesteps = max_timesteps
        self.checkpoint_freq = checkpoint_freq
        self.log_dir = Path(log_dir).resolve() / wandb.run.id

        callbacks = []
        if env.startswith('mpe_'):
            scenario_name = env[4:]  # Remove the prefix
            self.env_name = 'mpe'
        elif env.startswith('os_'):
            self.env_name = 'open_spiel'
            scenario_name = env[3:]  # Remove the prefix
        elif env.startswith('pz_'):
            scenario_name = env[3:]  # Remove the prefix
            self.env_name = 'pettingzoo'
        elif env.startswith('mc_'):
            scenario_name = env[3:]  # Remove the prefix
            self.env_name = 'multicomp'
            callbacks.append(CustomMujocoMetricsCallbacks)
        else:
            raise ValueError(f'Env {env} not supported!')

        self.scenario_name = scenario_name

        self.alg = alg
        self.trainer_cls = trainer_cls_from_str(self.alg)

        self.config = init_config(alg)

        if num_workers is not None:
            self.config['num_workers'] = num_workers

        # Handle algorithm-specific arguments
        if self.alg == 'ppo' and num_sgd_iter is not None:
            self.config['num_sgd_iter'] = num_sgd_iter

        self.config['train_batch_size'] = train_batch_size

        self.config['callbacks'] = MultiCallbacks(callbacks)

        self._handle_config_override(override)

        if is_one_hot_agent(self.alg):  # maddpg is only alg that does not return discrete actions. Instead returns one-hot actions
            one_hot_agents = [True, True]
        else:
            one_hot_agents = [False, False]
        self.env = init_env(self.env_name, self.scenario_name, one_hot_agents=one_hot_agents)

        self.artifact_manager = artifact_manager

    def train(self):
        print(f"Saving output to {self.log_dir}")

        use_local_critic = self._get_use_local_critic()

        policies_to_train = self._get_policies_to_train()
        trainer = create_ma_trainer(policies_to_train, use_local_critic, self.config,
                                    self.scenario_name, self.env, self.trainer_cls)

        print('Config:')
        print(pretty_print(trainer.config))

        # ArtifactManager needs reference to the trainable
        self.artifact_manager.trainer = trainer

        save_params(self.config, self.log_dir)

        # Start the actual training
        self._train_loop(trainer)

    @abstractmethod
    def _get_policies_to_train(self):
        raise NotImplementedError()

    @abstractmethod
    def _get_use_local_critic(self):
        raise NotImplementedError()

    @abstractmethod
    def _train_loop(self, trainer):
        raise NotImplementedError()

    def _handle_config_override(self, override: Optional[str]):
        if override is not None:
            # Convert json string into dict
            override_params = json.loads(override)
            # Update internal config with new override vals
            self.config.update(override_params)


class NormalTrainingManager(TrainingManager):
    def _train_loop(self, trainer):
        log_setup = [('policy_0', 'policy_0'), ('policy_1', 'policy_1')]
        run_trainer(trainer, self.checkpoint_freq, self.max_timesteps, artifact_manager=self.artifact_manager, log_setup=log_setup)

    def _get_use_local_critic(self):
        use_local_critic = [False, False]
        return use_local_critic

    def _get_policies_to_train(self):
        policies_to_train = None  # None == all policies train
        return policies_to_train


class FinetuneManager(TrainingManager):
    def __init__(self, env: str,
                 log_dir: Path,
                 alg: str,
                 max_timesteps: int,
                 checkpoint_freq: int,
                 num_workers: Optional[int],
                 override: str,
                 artifact_manager: ArtifactManager,
                 num_sgd_iter: Optional[int],
                 train_batch_size: int,
                 victim_artifact: str,
                 adversary_id,
                 specific_folder=False,
                 victim_policy_name=None):
        super().__init__(env, log_dir, alg, max_timesteps, checkpoint_freq, num_workers, override, artifact_manager, num_sgd_iter, train_batch_size)

        self.victim_artifact = victim_artifact
        self.adversary_id = adversary_id

        self.victim_id = 1 - adversary_id

        if victim_policy_name is None:
            self.victim_policy_name = f'policy_{self.victim_id}'
            print(f'Using {self.victim_policy_name} as automatic fallback because no victim policy name was provided')
        else:
            self.victim_policy_name = victim_policy_name

        self.specific_folder = specific_folder

    def _train_loop(self, trainer):
        adv_name = f'policy_{self.adversary_id}'
        new_victim_name = f'policy_{self.victim_id}'

        file = self.artifact_manager.get_remote_checkpoint(self.victim_artifact)

        victim_weights = load_saved_weights(file, self.scenario_name, self.victim_policy_name, self.trainer_cls)

        if self.victim_policy_name not in victim_weights:
            raise ValueError(f'No policy named {self.victim_policy_name} among victim weights. Please provide the correct name of the victim policy.')

        # Overwrite the weights for the agent that will not be trained (policy_1)
        trainer.set_weights({new_victim_name: victim_weights[self.victim_policy_name]})

        trainer.workers.sync_weights()  # IMPORTANT!!1!!1

        log_setup = [(adv_name, 'adversary')]
        run_trainer(trainer, self.checkpoint_freq, self.max_timesteps, artifact_manager=self.artifact_manager, log_setup=log_setup)

    def _get_use_local_critic(self):
        use_local_critic = [False, False]
        # Finetune on saved model
        use_local_critic[self.adversary_id] = True  # Adversarial policy does not have access to shared critic
        return use_local_critic

    def _get_policies_to_train(self):
        policies_to_train = [f'policy_{self.adversary_id}']
        return policies_to_train


def create_ma_trainer(policies_to_train, use_local_critic, config, scenario_name, env, trainer_cls):
    """Create new trainer with new weights"""
    multiagent = aprl_defense.common.utils.generate_multiagent_2_policies(env, use_local_critic, policies_to_train)
    config['env_config']['scenario_name'] = scenario_name
    config['multiagent'] = multiagent
    # Update config with some settings that apply to all configs regardless of algorithm/trainer
    # common_config = aprl_defense.configs.common.train
    # config.update(common_config)
    # Create a new maddpg trainer which will be used to train the adversarial policy
    new_trainer = trainer_cls(env='current-env', config=deepcopy(config), logger_creator=noop_logger_creator)
    return new_trainer


class HardenManager(TrainingManager):
    def __init__(self, env, log_dir: Path, alg: str, max_timesteps: int, checkpoint_freq: int, num_workers: Optional[int], override: str,
                 artifact_manager: ArtifactManager, num_sgd_iter: Optional[int], train_batch_size: int, main_id):
        super().__init__(env, log_dir, alg, max_timesteps, checkpoint_freq, num_workers, override, artifact_manager, num_sgd_iter, train_batch_size)

        self.main_id = main_id

    def _train_loop(self, trainer):

        def run_trainer_harden(trainer,
                               checkpoint_freq,
                               local_dir,
                               timesteps,
                               # writer,
                               config,
                               weight_list=None,
                               save=True,
                               log_setup=None,
                               return_mean_rew=None,
                               add_log_timesteps=0,
                               progress_bar=True):
            timesteps_total = 0

            checkpoint_dir = save_params(config, local_dir)

            next_checkpoint = checkpoint_freq

            rewards = defaultdict(list)
            if progress_bar:
                pbar = tqdm(total=timesteps)
            while timesteps_total < timesteps:
                results = trainer.train()

                timesteps_total = results['timesteps_total']

                for policy, value in results['policy_reward_mean'].items():
                    rewards[policy].append(value)

                # Log mean policy reward
                # for policy in trainer.config['multiagent']['policies']:
                if log_setup is not None:
                    for policy, name in log_setup:

                        # Check that the metric for this policy was actually collected
                        if policy not in results['policy_reward_mean']:
                            # raise ValueError(f"No result values collected. Is 'horizon' set?")
                            pass
                        else:
                            value = results['policy_reward_mean'][policy]
                            wandb.log({f'{name}_reward': value,
                                       'timestep': timesteps_total + add_log_timesteps})
                            # writer.add_scalar(tag=f'policy_reward_mean/{policy}',
                            #                   scalar_value=results['policy_reward_mean'][policy],
                            #                   global_step=timesteps_total)

                custom_eval_log(results, timesteps_total + add_log_timesteps, timesteps_total)

                # Save checkpoint if necessary
                if save and checkpoint_freq != -1 and weight_list is None and timesteps_total > next_checkpoint:
                    trainer.save(checkpoint_dir=checkpoint_dir)
                    next_checkpoint += checkpoint_freq  # We save the next checkpoint over this threshold
                elif weight_list is not None and timesteps_total > next_checkpoint:
                    # This is the case where we don't save (many) intermediate checkpoints and instead save weights in the weight list
                    weight_list.append(deepcopy(trainer.get_weights()))
                if progress_bar:
                    pbar.update(timesteps_total - pbar.n)
            if progress_bar:
                pbar.close()
            if save:
                checkpoint = trainer.save(checkpoint_dir=checkpoint_dir)
                print("checkpoint saved at", checkpoint)
            if return_mean_rew is not None:
                mean_rew = np.mean(rewards[return_mean_rew])
                return timesteps_total, mean_rew
            else:
                return timesteps_total

        secondary_name = f'policy_{1 - self.main_id}'

        log_setup = [('policy_0', 'policy_0'), ('policy_1', 'policy_1')]

        num_swaps = 30
        episodes = self.max_timesteps / num_swaps
        weight_list = []
        episodes_total = run_trainer_harden(trainer, self.checkpoint_freq, self.log_dir, episodes, weight_list=weight_list, config=self.config,
                                            log_setup=log_setup)

        newest_weights = deepcopy(trainer.get_weights())

        for i in range(1, num_swaps):
            # Determine whether to do normal training or training against old policy for this run
            train_normal = random.choice([True, False])
            if train_normal:
                weights = deepcopy(newest_weights.copy())
            else:
                weights = deepcopy(random.choice(weight_list))

            trainer.set_weights({secondary_name: weights[secondary_name]})
            episodes_total += run_trainer_harden(trainer,
                                                 self.checkpoint_freq,
                                                 self.log_dir,
                                                 episodes * (i + 1),
                                                 self.config,
                                                 weight_list=weight_list,
                                                 log_setup=log_setup)

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return ['policy_0', 'policy_1']
