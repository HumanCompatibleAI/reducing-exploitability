import json
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional

import wandb
from tensorboardX import SummaryWriter
from copy import deepcopy
import random

import aprl_defense.common.utils
from aprl_defense.common.io import get_checkpoint_file
from aprl_defense.common.train import run_trainer, init_env, init_victim_weights
import aprl_defense
import aprl_defense.configs.train
import aprl_defense.configs.common

from ray.tune.logger import pretty_print

from aprl_defense.common.utils import init_config, noop_logger_creator

from aprl_defense.common.utils import is_one_hot_agent, trainer_cls_from_str


class TrainingManager(ABC):
    def __init__(self, env: str, log_dir: Path, alg: str, max_timesteps: int, checkpoint_freq: int, override: str):
        self.max_timesteps = max_timesteps
        self.checkpoint_freq = checkpoint_freq
        self.log_dir = Path(log_dir).resolve() / wandb.run.id

        self.env_name = env

        # self.writer = SummaryWriter(logdir=str(self.log_dir / 'tb'))

        self.scenario_name = 'simple_push'

        self.alg = alg
        self.trainer_cls = trainer_cls_from_str(self.alg)

        self.config = init_config(alg)

        self._handle_config_override(override)

        if is_one_hot_agent(self.alg):  # maddpg is only alg that does not return discrete actions. Instead returns one-hot actions
            one_hot_agents = [True, True]
        else:
            one_hot_agents = [False, False]
        self.env = init_env(self.env_name, self.scenario_name, one_hot_agents=one_hot_agents)

    def train(self):
        print(f"Saving output to {self.log_dir}")

        use_local_critic = self._get_use_local_critic()

        policies_to_train = self._get_policies_to_train()
        new_maddpg_trainer = create_ma_trainer(policies_to_train, use_local_critic, self.config,
                                               self.scenario_name, self.env, self.trainer_cls)

        print('Config:')
        print(pretty_print(new_maddpg_trainer.config))

        # Start the actual training
        self._train_loop(new_maddpg_trainer)

    @abstractmethod
    def _get_policies_to_train(self):
        raise NotImplementedError()

    @abstractmethod
    def _get_use_local_critic(self):
        raise NotImplementedError()

    @abstractmethod
    def _train_loop(self, trainer):
        raise NotImplementedError()

    def _handle_config_override(self, override: Optional[str]):
        if override is not None:
            # Convert json string into dict
            override_params = json.loads(override)
            # Update internal config with new override vals
            self.config.update(override_params)


class NormalTrainingManager(TrainingManager):
    def _train_loop(self, trainer):
        log_setup = [('policy_0', 'policy_0'), ('policy_1', 'policy_1')]
        run_trainer(trainer,
                    self.checkpoint_freq,
                    self.log_dir,
                    self.max_timesteps,
                    self.config,
                    log_setup=log_setup)

    def _get_use_local_critic(self):
        use_local_critic = [False, False]
        return use_local_critic

    def _get_policies_to_train(self):
        policies_to_train = None  # None == all policies train
        return policies_to_train


class FinetuneManager(TrainingManager):
    def __init__(self, env: str,
                 log_dir: Path,
                 alg: str,
                 max_timesteps: int,
                 checkpoint_freq: int,
                 override: str,
                 victim_path,
                 adversary_id,
                 specific_folder=False,
                 victim_policy_name=None):
        super().__init__(env, log_dir, alg, max_timesteps, checkpoint_freq, override)

        self.victim_path = victim_path
        self.adversary_id = adversary_id

        self.victim_id = 1 - adversary_id

        if victim_policy_name is None:
            self.victim_policy_name = f'policy_{self.victim_id}'
            print(f'Using {self.victim_policy_name} as automatic fallback because no victim policy name was provided')
        else:
            self.victim_policy_name = victim_policy_name

        self.specific_folder = specific_folder

    def _train_loop(self, trainer):
        adv_name = f'policy_{self.adversary_id}'
        new_victim_name = f'policy_{self.victim_id}'

        file = get_checkpoint_file(self.victim_path, specific_folder=self.specific_folder)

        victim_weights = init_victim_weights(Path(file), self.scenario_name, self.victim_policy_name, self.trainer_cls)
        # Overwrite the weights for the agent that will not be trained (policy_1)
        trainer.set_weights({new_victim_name: victim_weights[self.victim_policy_name]})
        log_setup = [(adv_name, 'adversary')]
        run_trainer(trainer, self.checkpoint_freq, self.log_dir, self.max_timesteps, self.config, log_setup=log_setup)

    def _get_use_local_critic(self):
        use_local_critic = [False, False]
        # Finetune on saved model
        use_local_critic[self.adversary_id] = True  # Adversarial policy does not have access to shared critic
        return use_local_critic

    def _get_policies_to_train(self):
        policies_to_train = [f'policy_{self.adversary_id}']
        return policies_to_train


def create_ma_trainer(policies_to_train, use_local_critic, config, scenario_name, env, trainer_cls):
    """Create new trainer with new weights"""
    multiagent = aprl_defense.common.utils.generate_multiagent(env, use_local_critic, policies_to_train)
    config['env_config']['scenario_name'] = scenario_name
    config['multiagent'] = multiagent
    # Update config with some settings that apply to all configs regardless of algorithm/trainer
    # common_config = aprl_defense.configs.common.train
    # config.update(common_config)
    # Create a new maddpg trainer which will be used to train the adversarial policy
    new_maddpg_trainer = trainer_cls(env='mpe-push', config=deepcopy(config), logger_creator=noop_logger_creator)
    return new_maddpg_trainer


class HardenManager(TrainingManager):
    def __init__(self, env, log_dir: str, alg: str, max_timesteps: int, checkpoint_freq: int, override: str, main_id):
        super().__init__(env, log_dir, alg, max_timesteps, checkpoint_freq, override)

        self.main_id = main_id

    def _train_loop(self, trainer):

        secondary_name = f'policy_{1 - self.main_id}'

        log_setup = [('policy_0', 'policy_0'), ('policy_1', 'policy_1')]

        num_swaps = 30
        episodes = self.max_timesteps / num_swaps
        weight_list = []
        episodes_total = run_trainer(trainer, self.checkpoint_freq, self.log_dir, episodes, weight_list=weight_list, config=self.config,
                                     log_setup=log_setup)

        newest_weights = deepcopy(trainer.get_weights())

        for i in range(1, num_swaps):
            # Determine whether to do normal training or training against old policy for this run
            train_normal = random.choice([True, False])
            if train_normal:
                weights = deepcopy(newest_weights.copy())
            else:
                weights = deepcopy(random.choice(weight_list))

            trainer.set_weights({secondary_name: weights[secondary_name]})
            episodes_total += run_trainer(trainer,
                                          self.checkpoint_freq,
                                          self.log_dir,
                                          episodes * (i + 1),
                                          self.config,
                                          weight_list=weight_list,
                                          log_setup=log_setup)

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return ['policy_0', 'policy_1']
