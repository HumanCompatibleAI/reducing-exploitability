from abc import ABC, abstractmethod
from pathlib import Path

import wandb
from tensorboardX import SummaryWriter
from copy import deepcopy
import random

import aprl_defense.common.utils
from aprl_defense.common.io import get_newest_checkpoint_file
from aprl_defense.common.train import run_trainer, init_env, init_victim_weights
import aprl_defense
import aprl_defense.configs.train
import aprl_defense.configs.common

from ray.tune.logger import pretty_print

from aprl_defense.common.utils import init_config

from aprl_defense.common.utils import is_one_hot_agent, trainer_from_str


class TrainingManager(ABC):
    def __init__(self, log_dir: str, alg: str, max_timesteps: int, checkpoint_freq: int):
        self.max_timesteps = max_timesteps
        self.checkpoint_freq = checkpoint_freq
        self.log_dir = Path(log_dir).resolve() / wandb.run.id

        # self.writer = SummaryWriter(logdir=str(self.log_dir / 'tb'))

        self.scenario_name = 'simple_push'

        self.alg = alg
        self.trainer_cls = trainer_from_str(self.alg)

        self.config = init_config(alg)

        if is_one_hot_agent(self.alg):  # maddpg is only alg that does not return discrete actions. Instead returns one-hot actions
            one_hot_agents = [True, True]
        else:
            one_hot_agents = [False, False]
        self.env = init_env(self.scenario_name, one_hot_agents=one_hot_agents)

    def train(self):
        print(f"Saving output to {self.log_dir}")

        use_local_critic = self._get_use_local_critic()

        policies_to_train = self._get_policies_to_train()
        new_maddpg_trainer = create_trainer(policies_to_train, use_local_critic, self.config,
                                            self.scenario_name, self.env, self.trainer_cls)

        print('Config:')
        print(pretty_print(new_maddpg_trainer.config))

        # Start the actual training
        self._train_loop(self.checkpoint_freq, self.max_timesteps, new_maddpg_trainer, self.config)

    @abstractmethod
    def _get_policies_to_train(self):
        raise NotImplementedError()

    @abstractmethod
    def _get_use_local_critic(self):
        raise NotImplementedError()

    @abstractmethod
    def _train_loop(self, checkpoint_freq, max_steps, trainer, config):
        raise NotImplementedError()


class NormalTrainingManager(TrainingManager):
    def _train_loop(self, checkpoint_freq, max_steps, trainer, config):
        log_setup = [('policy_0', 'policy_0'), ('policy_1', 'policy_1')]
        run_trainer(trainer,
                    checkpoint_freq,
                    self.log_dir,
                    max_steps,
                    config,
                    log_setup=log_setup)

    def _get_use_local_critic(self):
        use_local_critic = [False, False]
        return use_local_critic

    def _get_policies_to_train(self):
        policies_to_train = None  # None == all policies train
        return policies_to_train


# class EvalManager(TrainingManager):
#
#     def _train_loop(self, checkpoint_freq, max_steps, trainer, config):
#         run_trainer(trainer, checkpoint_freq, self.log_dir, max_steps, self.writer, config)
#
#     def _get_use_local_critic(self):
#         use_local_critic = [False, False]
#         return use_local_critic
#
#     def _get_policies_to_train(self):
#         policies_to_train = [False, False]  # Don't train any policies
#         return policies_to_train


class FinetuneManager(TrainingManager):
    def __init__(self, log_dir: str, alg: str, max_timesteps: int, checkpoint_freq: int, victim_path, adversary_id):
        super().__init__(log_dir, alg, max_timesteps, checkpoint_freq)

        self.victim_path = victim_path
        self.adversary_id = adversary_id

        self.victim_id = 1 - adversary_id

    def _train_loop(self, checkpoint_freq, max_steps, trainer, config):
        victim_name = f'policy_{self.victim_id}'
        adv_name = f'policy_{self.adversary_id}'

        file = get_newest_checkpoint_file(self.victim_path)

        victim_weights = init_victim_weights(Path(file), self.scenario_name, victim_name, self.trainer_cls)
        # Overwrite the weights for the agent that will not be trained (policy_1)
        trainer.set_weights(victim_weights)
        log_setup = [(adv_name, 'adversary')]
        run_trainer(trainer, checkpoint_freq, self.log_dir, max_steps, config, log_setup=log_setup)

    def _get_use_local_critic(self):
        use_local_critic = [False, False]
        # Finetune on saved model
        use_local_critic[self.adversary_id] = True  # Adversarial policy does not have access to shared critic
        return use_local_critic

    def _get_policies_to_train(self):
        policies_to_train = [f'policy_{self.adversary_id}']
        return policies_to_train


def create_trainer(policies_to_train, use_local_critic, config, scenario_name, env, trainer_cls):
    """Create new trainer with new weights"""
    multiagent = aprl_defense.common.utils.generate_multiagent(env, use_local_critic, policies_to_train)
    config['env_config']['scenario_name'] = scenario_name
    config['multiagent'] = multiagent
    # Update config with some settings that apply to all configs regardless of algorithm/trainer
    # common_config = aprl_defense.configs.common.train
    # config.update(common_config)
    # Create a new maddpg trainer which will be used to train the adversarial policy
    new_maddpg_trainer = trainer_cls(env='mpe-push', config=deepcopy(config))
    return new_maddpg_trainer


class HardenManager(TrainingManager):
    def __init__(self, log_dir: str, alg: str, max_timesteps: int, checkpoint_freq: int, main_id):
        super().__init__(log_dir, alg, max_timesteps, checkpoint_freq)

        self.main_id = main_id

    def _train_loop(self, checkpoint_freq, max_steps, trainer, config):

        secondary_name = f'policy_{1 - self.main_id}'

        log_setup = [('policy_0', 'policy_0'), ('policy_1', 'policy_1')]

        num_swaps = 30
        episodes = max_steps / num_swaps
        weight_list = []
        episodes_total = run_trainer(trainer, checkpoint_freq, self.log_dir, episodes, weight_list=weight_list, config=config,
                                     log_setup=log_setup)

        newest_weights = deepcopy(trainer.get_weights())

        for i in range(1, num_swaps):
            # Determine whether to do normal training or training against old policy for this run
            train_normal = random.choice([True, False])
            if train_normal:
                weights = deepcopy(newest_weights.copy())
            else:
                weights = deepcopy(random.choice(weight_list))

            trainer.set_weights({secondary_name: weights[secondary_name]})
            episodes_total += run_trainer(trainer,
                                          checkpoint_freq,
                                          self.log_dir,
                                          episodes * (i + 1),
                                          config,
                                          weight_list=weight_list,
                                          log_setup=log_setup)

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return ['policy_0', 'policy_1']
