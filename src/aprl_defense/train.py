import random
import sys
import time
from pathlib import Path

import gin
import ray
import wandb
from absl import app
from absl import flags

from aprl_defense.base_logger import logger
from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.pbt.single_trainer_pbt import SingleTrainerPBTManager
from aprl_defense.training_manager import (
    FinetuneManager,
    NormalTrainingManager,
    SingleAgentTrainingManager,
)
from aprl_defense.trial.settings import TrialSettings, RLSettings

flags.DEFINE_multi_string("f", None, "List of paths to the config files.")
flags.DEFINE_multi_string(
    "p", None, "Newline separated list of Gin parameter bindings."
)
flags.DEFINE_string("override", None, "Override RLlib config.")
flags.DEFINE_string("override_f", None, "Override RLlib config with a json file.")

FLAGS = flags.FLAGS


def main(argv):
    gin.parse_config_files_and_bindings(FLAGS.f, FLAGS.p)

    trial_settings = TrialSettings()
    rl_settings = RLSettings()

    out_path = trial_settings.out_path
    alg = rl_settings.alg

    # Determine seed
    if trial_settings.seed is None:
        seed = random.randrange(sys.maxsize)
    else:
        seed = trial_settings.seed
    trial_settings.seed = seed

    mode = trial_settings.mode

    if trial_settings.run_name is None:
        run_name = f"{mode}_{alg}"
    else:
        run_name = trial_settings.run_name

    wandb_mode = "online"

    if trial_settings.disable_log:
        wandb_mode = "disabled"

    wandb.init(
        project=trial_settings.wandb_project,
        name=run_name,
        config={},  # TODO: actually have hparams here
        mode=wandb_mode,
        group=trial_settings.wandb_group,  # Group by mode
        dir=Path(out_path).resolve(),  # Change wandb dir
        job_type=job_type_from_mode(mode),
        notes=trial_settings.description,
    )

    out_path = (Path(out_path) / trial_settings.wandb_project).resolve() / wandb.run.id
    trial_settings.out_path = out_path

    artifact_manager = ArtifactManager(
        save_remote=not trial_settings.disable_log,
        local_checkpoint_dir=Path(out_path).resolve(),
    )
    artifact_manager.init_saving_checkpoints(
        mode, env_name=rl_settings.env, metadata={}  # vars(args)
    )
    if trial_settings.policy_cache is not None:
        trial_settings.policy_cache = Path(trial_settings.policy_cache) / wandb.run.id

    train(
        mode,
        trial_settings,
        rl_settings,
        artifact_manager,
        override=FLAGS.override,
        override_f=FLAGS.override_f,
    )


def train(
    mode,
    trial_settings: TrialSettings,
    rl_settings: RLSettings,
    artifact_manager: ArtifactManager,
    override: str,
    override_f: str,
):
    ray.init(local_mode=trial_settings.ray_local, num_cpus=trial_settings.num_cpus)

    if mode == "normal":
        training_manager = NormalTrainingManager(
            trial_settings,
            rl_settings,  # train_only_one_policy
            artifact_manager,
            override,
            override_f,
        )
    elif mode == "single-agent":
        training_manager = SingleAgentTrainingManager(
            trial_settings, rl_settings, artifact_manager
        )
    elif mode == "finetune":
        training_manager = FinetuneManager(
            trial_settings, rl_settings, artifact_manager, override, override_f
        )
    elif mode == "single-trainer-pbt":
        training_manager = SingleTrainerPBTManager(
            trial_settings, rl_settings, artifact_manager, override, override_f
        )

    else:  # Anything else is unsupported
        raise ValueError(f"Illegal argument for mode: {mode}")

    logger.info(f"Training with mode {mode}")
    time_start = time.time()
    training_manager.train()
    logger.info(f"Training of took {(time.time() - time_start) / 60} minutes")


def job_type_from_mode(mode: str):
    if mode == "finetune":
        return "attack"
    else:
        return "train"


if __name__ == "__main__":
    app.run(main)
