import argparse
import time
from pathlib import Path

import ray

from aprl_defense.common.utils import trainer_map
from aprl_defense.pbt.single_trainer_pbt import SingleTrainerPBTManager
from aprl_defense.pbt_scheduler import random_scheduler, random_choice_scheduler
from aprl_defense.training_manager import FinetuneManager, NormalTrainingManager
import wandb

from aprl_defense.pbt.separate_pbt import SeparatePBTManager


def main():
    args = parse_args()
    out_path = args.out_path
    alg = args.alg
    timesteps = args.max_timesteps
    checkpoint_freq = args.checkpoint_freq
    mode = args.mode

    if args.name is None:
        run_name = f'{mode}_{alg}'
    else:
        run_name = args.name

    wandb_mode = 'online'

    if args.disable_log:
        wandb_mode = 'disabled'

    wandb.init(project=args.wandb_project,
               name=run_name,
               config=args,
               mode=wandb_mode,
               # group=mode,  # Group by mode
               dir=Path(out_path).resolve(),  # Change wandb dir
               job_type=job_type_from_mode(mode),
               notes=args.description
               )

    out_path = Path(out_path) / args.wandb_project

    artifact = wandb.Artifact(type=f'selfplay_{args.env}',
                              name=wandb.run.id,
                              metadata=args
                              )

    train(mode,
          out_path,
          alg,
          timesteps,
          checkpoint_freq,
          env=args.env,
          artifact=artifact,
          victim_path=args.victim_path,
          cycle_ops=args.cycle_ops,
          num_ops=args.num_ops,
          harden_id=args.harden_id,
          adversary_id=args.adv_id,
          iter_steps=args.iter_steps,
          op_experience_fraction=args.op_experience_fraction,
          override=args.override,
          local_mode=args.ray_local,
          specific_folder=args.specific_folder,
          baseline_path=args.baseline_path,
          baseline_policy_name=args.baseline_policy_name,
          num_workers=args.num_workers,
          episodes_per_worker=args.episodes_per_worker,
          victim_policy_name=args.victim_policy_name,
          )


def train(mode,
          out_path: Path,
          alg,
          timesteps,
          checkpoint_freq,
          env,
          artifact: wandb.Artifact,
          local_mode=False,
          victim_path="",
          baseline_path="",
          cycle_ops=False,
          num_ops=10,
          harden_id=0,
          adversary_id=1,
          iter_steps=None,
          op_experience_fraction=1,
          override=None,
          specific_folder=False,
          baseline_policy_name='policy_1',
          num_workers=None,
          episodes_per_worker=10,
          victim_policy_name=None):
    ray.init(local_mode=local_mode)

    if mode == 'normal':
        training_manager = NormalTrainingManager(env=env,
                                                 log_dir=out_path,
                                                 alg=alg,
                                                 max_timesteps=timesteps,
                                                 checkpoint_freq=checkpoint_freq,
                                                 override=override,
                                                 artifact=artifact)
    elif mode == 'finetune':
        training_manager = FinetuneManager(env,
                                           log_dir=out_path,
                                           alg=alg,
                                           max_timesteps=timesteps,
                                           checkpoint_freq=checkpoint_freq,
                                           override=override,
                                           artifact=artifact,
                                           victim_path=victim_path,
                                           adversary_id=adversary_id,
                                           specific_folder=specific_folder,
                                           victim_policy_name=victim_policy_name, )
    elif mode == 'separate-pbt':
        training_manager = SeparatePBTManager(env=env,
                                              log_dir=out_path,
                                              alg=alg,
                                              max_timesteps=timesteps,
                                              checkpoint_freq=checkpoint_freq,
                                              override=override,
                                              artifact=artifact,
                                              main_id=harden_id,
                                              scheduler_func=random_scheduler,
                                              num_ops=num_ops,
                                              op_experience_factor=op_experience_fraction,
                                              baseline_checkpoint=baseline_path,
                                              specific_folder=specific_folder,
                                              baseline_policy_name=baseline_policy_name,
                                              num_workers=num_workers,
                                              episodes_per_worker=episodes_per_worker)
    elif mode == 'single-trainer-pbt':
        training_manager = SingleTrainerPBTManager(env=env,
                                                   log_dir=out_path,
                                                   alg=alg,
                                                   max_timesteps=timesteps,
                                                   checkpoint_freq=checkpoint_freq,
                                                   override=override,
                                                   artifact=artifact,
                                                   main_id=harden_id,
                                                   scheduler_func=random_scheduler,
                                                   main_steps=iter_steps,
                                                   num_ops=num_ops,
                                                   op_experience_factor=int(op_experience_fraction),
                                                   baseline_checkpoint=baseline_path,
                                                   specific_folder=specific_folder,
                                                   baseline_policy_name=baseline_policy_name,
                                                   num_workers=num_workers,
                                                   episodes_per_worker=episodes_per_worker)

    else:  # Anything else is unsupported
        raise ValueError(f'Illegal argument for mode: {mode}')

    print(f'Training with mode {mode}')
    time_start = time.time()
    training_manager.train()
    print(f"Training of {alg} took {(time.time() - time_start) / 60} minutes")


def parse_args():
    parser = argparse.ArgumentParser("Adversarial Policy")
    parser.add_argument('--wandb-project', type=str, default="ap-defense-3-parallel")
    parser.add_argument('--description', type=str, default="")
    parser.add_argument('--victim-path', type=str)
    parser.add_argument('--baseline-path', type=str)
    parser.add_argument('--mode', choices=['finetune', 'pbt', 'pbt-v2', 'separate-pbt', 'single-trainer-pbt', 'normal', 'eval', 'improved-pbt'],
                        default='normal')
    parser.add_argument('--out-path', type=str, default='/scratch/pavel/out/')
    parser.add_argument('--alg', choices=trainer_map.keys(), default='ppo')
    parser.add_argument('--max-timesteps', type=int, default=1_500_000)
    parser.add_argument('--checkpoint-freq', type=int, default=100000)
    parser.add_argument('--disable-log', action='store_true')
    parser.add_argument('--cycle-ops', action='store_true')
    parser.add_argument('--num-ops', type=int, default=10)
    parser.add_argument('--name', type=str, default=None)
    parser.add_argument('--harden-id', type=int, default=0)
    parser.add_argument('--adv-id', type=int, default=1)
    parser.add_argument('--iter-steps', type=int, default=25)
    parser.add_argument('--op-experience-fraction', type=int, default=1)
    parser.add_argument('--override', type=str, default=None)
    parser.add_argument('--specific-folder', action='store_true')
    parser.add_argument('--ray-local', action='store_true')
    parser.add_argument('--baseline-policy-name', type=str, default="policy_1")
    parser.add_argument('--victim-policy-name', type=str, default=None)
    parser.add_argument('--env', choices=['mpe_simple_push', 'open_spiel_env', 'mpe_simple_push_comm'], default="mpe_simple_push")
    parser.add_argument('--num-workers', type=int, default=None)  # None means it is taken from the configs in the configs folder
    parser.add_argument('--episodes-per-worker', type=int, default=10)

    return parser.parse_args()


def job_type_from_mode(mode: str):
    if mode == "finetune":
        return 'attack'
    else:
        return 'train'


if __name__ == '__main__':
    main()
