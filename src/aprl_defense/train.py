import os
import random
import time

import gin
import ray
from absl import app
from absl import flags
from dotenv import load_dotenv

from aprl_defense.base_logger import logger
from aprl_defense.training_managers.base_training_manager import (
    BaseTrainingManager,
)
from aprl_defense.training_managers.pbt_manager import PBTManager
from aprl_defense.training_managers.simple_training_manager import (
    SelfplayTrainingManager,
    AttackManager,
    SingleAgentTrainingManager,
)
from aprl_defense.trial.settings import TrialSettings, RLSettings

# The supported flags:
# -f to provide a gin config
# -p to overwrite gin parameters
# --override and --override_f to override the RLlib config only, either with a json string or a file respectively
flags.DEFINE_multi_string("f", None, "List of paths to the config files.")
flags.DEFINE_multi_string(
    "p", None, "Newline separated list of Gin parameter bindings."
)
flags.DEFINE_string("override", None, "Override RLlib config.")
flags.DEFINE_string("override_f", None, "Override RLlib config with a json file.")

FLAGS = flags.FLAGS


def main(argv):
    gin.parse_config_files_and_bindings(FLAGS.f, FLAGS.p)

    # Determine out_path and update trial_settings if necessary
    # Default is overwritten by cli param, is overwritten by environment variable
    # Env var is overwritten by dotenv file
    load_dotenv()
    out_path = os.environ.get("POLICY_DEFENSE_OUT", None)
    if out_path is None:
        trial_settings = TrialSettings()
    else:
        trial_settings = TrialSettings(
            out_path=out_path
        )  # Explicitly passing out_path overwrites both gin and default path
    rl_settings = RLSettings()

    # Determine seed
    if trial_settings.seed is None:
        trial_settings.seed = random.randrange(
            2**32 - 1
        )  # Numpy has this as e max number for seed

    mode = trial_settings.mode

    override = FLAGS.override
    override_f = FLAGS.override_f

    ray.init(local_mode=trial_settings.ray_local, num_cpus=trial_settings.num_cpus)

    training_manager: BaseTrainingManager
    if mode == "selfplay":
        training_manager = SelfplayTrainingManager(
            trial_settings,
            rl_settings,
            override,
            override_f,
        )
    elif mode == "single-agent":
        training_manager = SingleAgentTrainingManager(
            trial_settings, rl_settings, override, override_f
        )
    elif mode == "attack":
        training_manager = AttackManager(
            trial_settings, rl_settings, override, override_f
        )
    elif mode == "pbt":
        training_manager = PBTManager(trial_settings, rl_settings, override, override_f)
    else:  # Anything else is unsupported
        raise ValueError(f"Illegal argument for mode: {mode}")

    logger.info(f"Training with mode {mode}")
    time_start = time.time()
    training_manager.train()
    logger.info(f"Training of took {(time.time() - time_start) / 60} minutes")


if __name__ == "__main__":
    app.run(main)
