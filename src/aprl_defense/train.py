import argparse
import time
from pathlib import Path
from typing import List

import ray

from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.utils import trainer_map
from aprl_defense.pbt.single_trainer_pbt import SingleTrainerPBTManager
from aprl_defense.pbt_scheduler import random_scheduler
from aprl_defense.training_manager import FinetuneManager, NormalTrainingManager
import wandb

from aprl_defense.pbt.separate_pbt import SeparatePBTManager


def main():
    args = parse_args()
    out_path = args.out_path
    alg = args.alg
    timesteps = args.max_timesteps
    checkpoint_freq_M = args.checkpoint_freq_M
    num_checkpoints = args.num_checkpoints

    # Calculate checkpoint freq
    if checkpoint_freq_M is None and num_checkpoints is not None:
        checkpoint_freq = timesteps / num_checkpoints
    elif checkpoint_freq_M is not None and num_checkpoints is None:
        checkpoint_freq = checkpoint_freq_M * 1_000_000
    elif checkpoint_freq_M is None and num_checkpoints is None:
        num_checkpoints = 10
        print(f'No checkpoint freq given, setting to {num_checkpoints} checkpoints overall')
        checkpoint_freq = timesteps / num_checkpoints
    else:  # checkpoint_freq_M is not None and num_checkpoints is not None:
        raise ValueError("Can't set both checkpoint frequency and number of checkpoints! Choose one.")

    mode = args.mode

    if args.name is None:
        run_name = f'{mode}_{alg}'
    else:
        run_name = args.name

    wandb_mode = 'online'

    if args.disable_log:
        wandb_mode = 'disabled'

    wandb.init(project=args.wandb_project,
               name=run_name,
               config=args,
               mode=wandb_mode,
               group=args.wandb_group,  # Group by mode
               dir=Path(out_path).resolve(),  # Change wandb dir
               job_type=job_type_from_mode(mode),
               notes=args.description
               )

    out_path = Path(out_path) / args.wandb_project

    artifact_manager = ArtifactManager(save_remote=not args.disable_log,
                                       mode=mode,
                                       env_name=args.env,
                                       metadata=vars(args),
                                       local_checkpoint_dir=Path(out_path).resolve() / wandb.run.id)

    train(mode,
          out_path,
          alg,
          timesteps,
          checkpoint_freq,
          env=args.env,
          artifact_manager=artifact_manager,
          victim_artifact=args.victim_artifact,
          num_ops=args.num_ops,
          harden_id=args.harden_id,
          adversary_id=args.adv_id,
          op_experience_fraction=args.op_experience_fraction,
          override=args.override,
          local_mode=args.ray_local,
          specific_folder=args.specific_folder,
          baseline_artifacts=args.baseline_artifacts,
          baseline_policy_name=args.baseline_policy_name,
          num_workers=args.num_workers,
          victim_policy_name=args.victim_policy_name,
          train_batch_size=args.train_batch_size,
          new_op_interval=args.new_op_interval,
          )


def train(mode,
          out_path: Path,
          alg,
          timesteps,
          checkpoint_freq,
          env,
          artifact_manager: ArtifactManager,
          local_mode=False,
          victim_artifact="",
          baseline_artifacts=None,
          num_ops=10,
          harden_id=0,
          adversary_id=1,
          op_experience_fraction=1,
          override=None,
          specific_folder=False,
          baseline_policy_name='policy_1',
          num_workers=None,
          victim_policy_name=None,
          num_sgd_iter=None,
          train_batch_size=4000,
          new_op_interval=-1):
    ray.init(local_mode=local_mode)

    if mode == 'normal':
        training_manager = NormalTrainingManager(env=env,
                                                 log_dir=out_path,
                                                 alg=alg,
                                                 max_timesteps=timesteps,
                                                 checkpoint_freq=checkpoint_freq,
                                                 num_workers=num_workers,
                                                 override=override,
                                                 artifact_manager=artifact_manager,
                                                 num_sgd_iter=num_sgd_iter,
                                                 train_batch_size=train_batch_size)
    elif mode == 'finetune':
        training_manager = FinetuneManager(env,
                                           log_dir=out_path,
                                           alg=alg,
                                           max_timesteps=timesteps,
                                           checkpoint_freq=checkpoint_freq,
                                           num_workers=num_workers,
                                           override=override,
                                           artifact_manager=artifact_manager,
                                           victim_artifact=victim_artifact,
                                           adversary_id=adversary_id,
                                           specific_folder=specific_folder,
                                           victim_policy_name=victim_policy_name,
                                           num_sgd_iter=num_sgd_iter,
                                           train_batch_size=train_batch_size)
    # elif mode == 'separate-pbt':
    #     training_manager = SeparatePBTManager(env=env,
    #                                           log_dir=out_path,
    #                                           alg=alg,
    #                                           max_timesteps=timesteps,
    #                                           checkpoint_freq=checkpoint_freq,
    #                                           override=override,
    #                                           artifact_manager=artifact_manager,
    #                                           main_id=harden_id,
    #                                           scheduler_func=random_scheduler,
    #                                           num_ops=num_ops,
    #                                           op_experience_factor=op_experience_fraction,
    #                                           baseline_artifact=baseline_artifact,
    #                                           specific_folder=specific_folder,
    #                                           baseline_policy_name=baseline_policy_name,
    #                                           num_workers=num_workers,
    #                                           episodes_per_worker=episodes_per_worker)
    elif mode == 'single-trainer-pbt':
        training_manager = SingleTrainerPBTManager(env=env,
                                                   log_dir=out_path,
                                                   alg=alg,
                                                   max_timesteps=timesteps,
                                                   checkpoint_freq=checkpoint_freq,
                                                   num_workers=num_workers,
                                                   override=override,
                                                   artifact_manager=artifact_manager,
                                                   main_id=harden_id,
                                                   num_ops=num_ops,
                                                   op_experience_factor=int(op_experience_fraction),
                                                   baseline_artifacts=baseline_artifacts,
                                                   specific_folder=specific_folder,
                                                   baseline_policy_name=baseline_policy_name,
                                                   num_sgd_iter=num_sgd_iter,
                                                   train_batch_size=train_batch_size,
                                                   new_op_interval=new_op_interval)

    else:  # Anything else is unsupported
        raise ValueError(f'Illegal argument for mode: {mode}')

    print(f'Training with mode {mode}')
    time_start = time.time()
    training_manager.train()
    print(f"Training of {alg} took {(time.time() - time_start) / 60} minutes")


def parse_args():
    parser = argparse.ArgumentParser("Adversarial Policy")
    parser.add_argument('--wandb-project', type=str, default="chaiberkeley/pbrl-defense")
    parser.add_argument('--wandb-group', type=str, default="dev")
    parser.add_argument('--description', type=str, default="")
    parser.add_argument('--victim-artifact', type=str)
    parser.add_argument('--baseline-artifacts', nargs="+", default=None)
    parser.add_argument('--mode', choices=['finetune', 'pbt', 'pbt-v2', 'separate-pbt', 'single-trainer-pbt', 'normal', 'eval', 'improved-pbt'],
                        default='normal')
    parser.add_argument('--out-path', type=Path, default=Path('/scratch/pavel/out/'))
    parser.add_argument('--alg', choices=trainer_map.keys(), default='ppo')
    parser.add_argument('--max-timesteps', type=int, default=1_500_000)
    parser.add_argument('--num-checkpoints', type=int, default=None)
    parser.add_argument('--checkpoint-freq-M', type=int, default=None)
    parser.add_argument('--disable-log', action='store_true')
    parser.add_argument('--num-ops', type=int, default=10)
    parser.add_argument('--name', type=str, default=None)
    parser.add_argument('--harden-id', type=int, default=0)
    parser.add_argument('--adv-id', type=int, default=1)
    parser.add_argument('--op-experience-fraction', type=int, default=1)
    parser.add_argument('--override', type=str, default=None)
    parser.add_argument('--specific-folder', action='store_true')
    parser.add_argument('--ray-local', action='store_true')
    parser.add_argument('--baseline-policy-name', type=str, default="policy_1")
    parser.add_argument('--victim-policy-name', type=str, default=None)
    parser.add_argument('--env', type=str, default="mpe_simple_push")
    parser.add_argument('--num-workers', type=int, default=None)  # None means it is taken from the configs in the configs folder
    parser.add_argument('--num-sgd-iter', type=int, default=4)
    parser.add_argument('--new-op-interval', type=int, default=-1)
    parser.add_argument('--train-batch-size', type=int, default=4000)

    return parser.parse_args()


def job_type_from_mode(mode: str):
    if mode == "finetune":
        return 'attack'
    else:
        return 'train'


if __name__ == '__main__':
    main()
