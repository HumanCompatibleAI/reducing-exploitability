import random
import sys
import time
from pathlib import Path

import gin
import ray
import wandb
from absl import app
from absl import flags

from aprl_defense.base_logger import logger
from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.pbt.single_trainer_pbt import SingleTrainerPBTManager
from aprl_defense.training_manager import (
    FinetuneManager,
    NormalTrainingManager,
    SingleAgentTrainingManager,
)
from aprl_defense.trial.settings import TrialSettings, RLSettings

flags.DEFINE_multi_string("f", None, "List of paths to the config files.")
flags.DEFINE_multi_string(
    "p", None, "Newline separated list of Gin parameter bindings."
)
flags.DEFINE_string("override", None, "Override RLlib config.")
flags.DEFINE_string("override_f", None, "Override RLlib config with a json file.")

FLAGS = flags.FLAGS


def main(argv):
    gin.parse_config_files_and_bindings(FLAGS.f, FLAGS.p)

    trial_settings = TrialSettings(
        # out_path=out_path,
        # artifact_manager=artifact_manager,
        # checkpoint_freq=checkpoint_freq,
        # seed=seed,
        # num_workers=args.num_workers,
        # override=args.override,
        # override_f=args.override_f,
        # continue_artifact=args.continue_artifact,
    )
    rl_settings = RLSettings(
        # env=args.env,
        # alg=alg,
        # max_timesteps=timesteps,
        # train_batch_size=args.train_batch_size,
    )

    out_path = trial_settings.out_path
    alg = rl_settings.alg

    # Determine seed
    if trial_settings.seed is None:
        seed = random.randrange(sys.maxsize)
    else:
        seed = trial_settings.seed

    # evaluation_freq = (
    #     args.eval_freq_M * 1_000_000
    #     if args.eval_freq_M is not None
    #     else checkpoint_freq
    # )

    mode = trial_settings.mode

    if trial_settings.run_name is None:
        run_name = f"{mode}_{alg}"
    else:
        run_name = trial_settings.run_name

    wandb_mode = "online"

    if trial_settings.disable_log:
        wandb_mode = "disabled"

    wandb.init(
        project=trial_settings.wandb_project,
        name=run_name,
        config={},  # TODO: actually have hparams here
        mode=wandb_mode,
        group=trial_settings.wandb_group,  # Group by mode
        dir=Path(out_path).resolve(),  # Change wandb dir
        job_type=job_type_from_mode(mode),
        notes=trial_settings.description,
    )

    out_path = (Path(out_path) / trial_settings.wandb_project).resolve() / wandb.run.id

    artifact_manager = ArtifactManager(
        save_remote=not trial_settings.disable_log,
        local_checkpoint_dir=Path(out_path).resolve(),
    )
    artifact_manager.init_saving_checkpoints(
        mode, env_name=rl_settings.env, metadata={}  # vars(args)
    )
    if trial_settings.policy_cache is None:
        policy_cache = None
    else:
        policy_cache = Path(trial_settings.policy_cache) / wandb.run.id

    train(
        mode,
        trial_settings,
        rl_settings,
        artifact_manager,
        override=FLAGS.override,
        override_f=FLAGS.override_f
        # evaluation_freq=evaluation_freq,
        # victim_artifact=args.victim_artifact,
        # num_ops=args.num_ops,
        # harden_id=args.harden_id,
        # adversary_id=args.adv_id,
        # op_experience_fraction=args.op_experience_fraction,
        # local_mode=args.ray_local,
        # specific_folder=args.specific_folder,
        # baseline_artifacts=args.baseline_artifacts,
        # baseline_policy_name=args.baseline_policy_name,
        # victim_policy_name=args.victim_policy_name,
        # new_op_interval=args.new_op_interval,
        # policy_cache=policy_cache,
        # num_cpus=args.num_cpus,
        # train_only_one_policy=args.train_only_one_policy,
    )


def train(
    mode,
    trial_settings: TrialSettings,
    rl_settings: RLSettings,
    artifact_manager: ArtifactManager,
    override: str,
    override_f: str,
    # evaluation_freq,
    # local_mode=False,
    # victim_artifact="",
    # baseline_artifacts=None,
    # num_ops=10,
    # harden_id=0,
    # adversary_id=1,
    # op_experience_fraction=1,
    # specific_folder=False,
    # baseline_policy_name="policy_1",
    # victim_policy_name=None,
    # new_op_interval=-1,
    # policy_cache=None,
    # num_cpus=None,
    # train_only_one_policy=False,
):
    ray.init(local_mode=trial_settings.ray_local, num_cpus=trial_settings.num_cpus)

    if mode == "normal":
        training_manager = NormalTrainingManager(
            trial_settings,
            rl_settings,  # train_only_one_policy
            artifact_manager,
            override,
            override_f,
        )
    elif mode == "single-agent":
        training_manager = SingleAgentTrainingManager(
            trial_settings, rl_settings, artifact_manager
        )
    elif mode == "finetune":
        training_manager = FinetuneManager(
            trial_settings,
            rl_settings,
            artifact_manager,
            override,
            override_f
            # victim_artifact=victim_artifact,
            # adversary_id=adversary_id,
            # specific_folder=specific_folder,
            # victim_policy_name=victim_policy_name,
        )
    elif mode == "single-trainer-pbt":
        training_manager = SingleTrainerPBTManager(
            trial_settings,
            rl_settings,
            override,
            override_f
            # evaluation_freq=evaluation_freq,
            # main_id=harden_id,
            # num_ops=num_ops,
            # op_experience_factor=op_experience_fraction,
            # baseline_artifacts=baseline_artifacts,
            # specific_folder=specific_folder,
            # baseline_policy_name=baseline_policy_name,
            # new_op_interval=new_op_interval,
            # policy_cache=policy_cache,
        )

    else:  # Anything else is unsupported
        raise ValueError(f"Illegal argument for mode: {mode}")

    logger.info(f"Training with mode {mode}")
    time_start = time.time()
    training_manager.train()
    logger.info(f"Training of took {(time.time() - time_start) / 60} minutes")


def parse_args():
    pass
    # flags.DEFINE_multi_string(
    #     "gin_files",
    #     [],
    #     "List of paths to gin configuration files (e.g."
    #     '"dopamine/agents/dqn/dqn.gin").',
    # )
    # flags.DEFINE_multi_string(
    #     "gin_bindings",
    #     [],
    #     "Gin bindings to override the values set in the config files "
    #     '(e.g. "DQNAgent.epsilon_train=0.1",'
    #     '      "create_environment.game_name="Pong"").',
    # )
    #
    # FLAGS = flags.FLAGS
    # gin_files = FLAGS.gin_files
    # gin_bindings = FLAGS.gin_bindings
    # run_experiment.load_gin_configs(gin_files, gin_bindings)
    #
    # parser = argparse.ArgumentParser("Adversarial Policy")
    # parser.add_argument(
    #     "--mode",
    #     choices=[
    #         "finetune",
    #         "pbt",
    #         "pbt-v2",
    #         "separate-pbt",
    #         "single-trainer-pbt",
    #         "normal",
    #         "eval",
    #         "improved-pbt",
    #         "single-agent",
    #     ],
    #     default="normal",
    # )
    #
    # return parser.parse_args()


def job_type_from_mode(mode: str):
    if mode == "finetune":
        return "attack"
    else:
        return "train"


if __name__ == "__main__":
    app.run(main)
