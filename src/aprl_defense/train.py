import argparse
import time
from pathlib import Path

import ray
import wandb

from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.utils import trainer_map
from aprl_defense.pbt.single_trainer_pbt import SingleTrainerPBTManager
from aprl_defense.training_manager import (
    FinetuneManager,
    NormalTrainingManager,
    SingleAgentTrainingManager,
)
from aprl_defense.trial.settings import TrialSettings, RLSettings


def main():
    args = parse_args()
    out_path = args.out_path
    alg = args.alg
    timesteps = args.max_timesteps
    checkpoint_freq_M = args.checkpoint_freq_M
    num_checkpoints = args.num_checkpoints

    # Calculate checkpoint freq
    if checkpoint_freq_M is None and num_checkpoints is not None:
        checkpoint_freq = timesteps / num_checkpoints
    elif checkpoint_freq_M is not None and num_checkpoints is None:
        checkpoint_freq = int(checkpoint_freq_M * 1_000_000)
    elif checkpoint_freq_M is None and num_checkpoints is None:
        num_checkpoints = 10
        print(
            f"No checkpoint freq given, setting to {num_checkpoints} checkpoints overall"
        )
        checkpoint_freq = timesteps / num_checkpoints
    else:  # checkpoint_freq_M is not None and num_checkpoints is not None:
        raise ValueError(
            "Can't set both checkpoint frequency and number of checkpoints! Choose one."
        )

    evaluation_freq = (
        args.eval_freq_M * 1_000_000
        if args.eval_freq_M is not None
        else checkpoint_freq
    )

    mode = args.mode

    if args.name is None:
        run_name = f"{mode}_{alg}"
    else:
        run_name = args.name

    wandb_mode = "online"

    if args.disable_log:
        wandb_mode = "disabled"

    wandb.init(
        project=args.wandb_project,
        name=run_name,
        config=args,
        mode=wandb_mode,
        group=args.wandb_group,  # Group by mode
        dir=Path(out_path).resolve(),  # Change wandb dir
        job_type=job_type_from_mode(mode),
        notes=args.description,
    )

    out_path = (Path(out_path) / args.wandb_project).resolve() / wandb.run.id

    artifact_manager = ArtifactManager(
        save_remote=not args.disable_log,
        local_checkpoint_dir=Path(out_path).resolve(),
    )
    artifact_manager.init_saving_checkpoints(
        mode, env_name=args.env, metadata=vars(args)
    )
    if args.policy_cache is None:
        policy_cache = None
    else:
        policy_cache = Path(args.policy_cache) / wandb.run.id

    trial_settings = TrialSettings(
        out_path=out_path,
        artifact_manager=artifact_manager,
        checkpoint_freq=checkpoint_freq,
        num_workers=args.num_workers,
        override=args.override,
        override_f=args.override_f,
        continue_artifact=args.continue_artifact,
    )
    rl_settings = RLSettings(
        env=args.env,
        alg=alg,
        max_timesteps=timesteps,
        train_batch_size=args.train_batch_size,
    )

    train(
        mode,
        trial_settings,
        rl_settings,
        evaluation_freq=evaluation_freq,
        victim_artifact=args.victim_artifact,
        num_ops=args.num_ops,
        harden_id=args.harden_id,
        adversary_id=args.adv_id,
        op_experience_fraction=args.op_experience_fraction,
        local_mode=args.ray_local,
        specific_folder=args.specific_folder,
        baseline_artifacts=args.baseline_artifacts,
        baseline_policy_name=args.baseline_policy_name,
        victim_policy_name=args.victim_policy_name,
        new_op_interval=args.new_op_interval,
        policy_cache=policy_cache,
        num_cpus=args.num_cpus,
        train_only_one_policy=args.train_only_one_policy,
    )


def train(
    mode,
    trial_settings: TrialSettings,
    rl_settings: RLSettings,
    evaluation_freq,
    local_mode=False,
    victim_artifact="",
    baseline_artifacts=None,
    num_ops=10,
    harden_id=0,
    adversary_id=1,
    op_experience_fraction=1,
    specific_folder=False,
    baseline_policy_name="policy_1",
    victim_policy_name=None,
    new_op_interval=-1,
    policy_cache=None,
    num_cpus=None,
    train_only_one_policy=False,
):
    ray.init(local_mode=local_mode, num_cpus=num_cpus)

    if mode == "normal":
        training_manager = NormalTrainingManager(
            trial_settings, rl_settings, train_only_one_policy
        )
    elif mode == "single-agent":
        training_manager = SingleAgentTrainingManager(trial_settings, rl_settings)
    elif mode == "finetune":
        training_manager = FinetuneManager(
            trial_settings,
            rl_settings,
            victim_artifact=victim_artifact,
            adversary_id=adversary_id,
            specific_folder=specific_folder,
            victim_policy_name=victim_policy_name,
        )
    elif mode == "single-trainer-pbt":
        training_manager = SingleTrainerPBTManager(
            trial_settings,
            rl_settings,
            evaluation_freq=evaluation_freq,
            main_id=harden_id,
            num_ops=num_ops,
            op_experience_factor=op_experience_fraction,
            baseline_artifacts=baseline_artifacts,
            specific_folder=specific_folder,
            baseline_policy_name=baseline_policy_name,
            new_op_interval=new_op_interval,
            policy_cache=policy_cache,
        )

    else:  # Anything else is unsupported
        raise ValueError(f"Illegal argument for mode: {mode}")

    print(f"Training with mode {mode}")
    time_start = time.time()
    training_manager.train()
    print(f"Training of took {(time.time() - time_start) / 60} minutes")


def parse_args():
    parser = argparse.ArgumentParser("Adversarial Policy")
    parser.add_argument("--wandb-project", type=str, default="pbrl-defense")
    parser.add_argument("--wandb-group", type=str, default="dev")
    parser.add_argument("--description", type=str, default="")
    parser.add_argument("--victim-artifact", type=str)
    parser.add_argument("--continue-artifact", type=str)
    parser.add_argument("--baseline-artifacts", nargs="+", default=None)
    parser.add_argument(
        "--mode",
        choices=[
            "finetune",
            "pbt",
            "pbt-v2",
            "separate-pbt",
            "single-trainer-pbt",
            "normal",
            "eval",
            "improved-pbt",
            "single-agent",
        ],
        default="normal",
    )
    parser.add_argument("--out-path", type=Path, default=Path("/scratch/pavel/out/"))
    parser.add_argument("--alg", choices=trainer_map.keys(), default="ppo")
    parser.add_argument("--max-timesteps", type=int, default=1_500_000)
    parser.add_argument("--num-checkpoints", type=int, default=None)
    parser.add_argument("--checkpoint-freq-M", type=float, default=None)
    parser.add_argument("--eval-freq-M", type=float, default=None)
    parser.add_argument("--disable-log", action="store_true")
    parser.add_argument("--num-ops", type=int, default=10)
    parser.add_argument("--name", type=str, default=None)
    parser.add_argument("--harden-id", type=int, default=0)
    parser.add_argument("--adv-id", type=int, default=1)
    parser.add_argument("--op-experience-fraction", type=float, default=1)
    parser.add_argument("--override", type=str, default=None)
    parser.add_argument("--override-f", type=str, default=None)
    parser.add_argument("--specific-folder", action="store_true")
    parser.add_argument("--ray-local", action="store_true")
    parser.add_argument("--baseline-policy-name", type=str, default="policy_1")
    parser.add_argument("--victim-policy-name", type=str, default=None)
    parser.add_argument("--env", type=str, default="mpe_simple_push")
    parser.add_argument(
        "--num-workers", type=int, default=None
    )  # None means it is taken from the configs in the configs folder
    parser.add_argument("--new-op-interval", type=int, default=-1)
    parser.add_argument("--train-batch-size", type=int, default=4000)
    parser.add_argument("--policy-cache", type=str, default=None)
    parser.add_argument("--num-cpus", type=int, default=None)
    parser.add_argument("--train-only-one-policy", action="store_true")

    return parser.parse_args()


def job_type_from_mode(mode: str):
    if mode == "finetune":
        return "attack"
    else:
        return "train"


if __name__ == "__main__":
    main()
