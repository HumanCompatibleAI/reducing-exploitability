import math
from copy import deepcopy

import ray
import wandb
from ray.rllib.evaluation.metrics import collect_episodes, summarize_episodes
from tqdm import tqdm
from ray.rllib.agents import Trainer
from ray.rllib.evaluation import collect_metrics
from ray.rllib.evaluation.worker_set import WorkerSet

import aprl_defense.configs.train
import aprl_defense.configs.eval
from aprl_defense.common.train import run_trainer
from aprl_defense.training_manager import TrainingManager


class FixedIterPBTManager(TrainingManager):
    def __init__(self, log_dir: str, alg: str, max_timesteps: int, checkpoint_freq: int, override: str, main_id: int, scheduler_func, iter_steps=25000,
                 num_agents=10, evaluation_num_workers=10):
        super().__init__(log_dir, alg, max_timesteps, checkpoint_freq, override)

        self.iter_steps = 25000 if iter_steps is None else iter_steps
        self.main_id = main_id
        self.scheduler_func = scheduler_func
        self.num_agents = num_agents
        self.iter_steps = iter_steps

        self.stored_agents = {
            'current': [],
            'old': []
        }

        # Update config with settings that are specific to PBT
        self.config.update(aprl_defense.configs.train.pbt)

        self.config.update(aprl_defense.configs.eval.online_eval)

        # Apply conditional eval settings
        self.config.update({
            "custom_eval_function": create_pbt_eval_func(self.stored_agents, self.main_id),

            # Enable evaluation with given frequency
            "evaluation_interval": self.checkpoint_freq // 25,
        })

    def _train_loop(self, checkpoint_freq, max_steps, trainer, config):
        print(f'Running PBT')

        checkpoint_dir = self.log_dir / 'checkpoints'
        checkpoint_dir.mkdir(exist_ok=True, parents=True)

        # Initialize PBT training
        primary_name = f'policy_{self.main_id}'
        sec_id = 1 - self.main_id
        secondary_name = f'policy_{sec_id}'
        # Table that contains infos for each agent, used to determine next opponent.
        # Currently temporary simply represented by a dict
        agent_infos = {'num_agents': self.num_agents}
        for i in range(self.num_agents):
            self.stored_agents['current'].append(None)
            self.stored_agents['old'].append([])

        log_setup = [(primary_name, 'main'), (secondary_name, f'opponent_{sec_id}')]

        iter_lengths = []
        num_checkpoints_collected = 0
        timesteps_total = 0
        iterations = 0
        pbar = tqdm(total=max_steps)
        while timesteps_total < max_steps:
            # Determine id of secondary agent for this training iterations
            sec_id = self.scheduler_func(agent_infos)

            # Get stored weights, initialize new ones if necessary
            agent_weights = _get_agent_weights(sec_id, self.stored_agents, config, secondary_name, self.trainer_cls)
            # Use these weights in the trainer (for secondary agent)
            trainer.set_weights({secondary_name: agent_weights})

            # Log which opponent is being played against
            wandb.log({f'opponent_id': sec_id,
                       'timestep': timesteps_total + 1})  # The + 1 ensures this timestep does not have duplicate values for op_id
            timesteps_before = timesteps_total
            timesteps_total = run_trainer(trainer,
                                          -1,
                                          self.log_dir,
                                          timesteps_total + self.iter_steps,
                                          config,
                                          log_setup=log_setup,
                                          save=False,
                                          progress_bar=False)

            # Log which opponent was played against
            wandb.log({f'opponent_id': sec_id,
                       'timestep': timesteps_total})

            actual_iter_length = timesteps_total - timesteps_before  # In steps
            iter_lengths.append(actual_iter_length)

            running_avg_iter_length = sum(iter_lengths) / len(iter_lengths)
            wandb.log({f'avg_iter_length': running_avg_iter_length,
                       'timestep': timesteps_total})

            # Store updated weights
            new_weights = deepcopy(trainer.get_weights()[secondary_name])
            self.stored_agents['current'][sec_id] = new_weights

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * checkpoint_freq
            if timesteps_total > next_checkpoint_at:
                trainer.save(checkpoint_dir=checkpoint_dir)
                num_checkpoints_collected += 1

                # Update old opponents
                for i in range(len(self.stored_agents['current'])):
                    weights = _get_agent_weights(i, self.stored_agents, self.config, secondary_name, self.trainer_cls)
                    weights_copy = deepcopy(weights)
                    self.stored_agents['old'][i].append(weights_copy)

            iterations += 1
            pbar.update(timesteps_total - pbar.n)
        pbar.close()

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return None  # Trains all


def _get_agent_weights(id, stored_agents, config, sec_name, trainer_cls):
    agent = stored_agents['current'][id]
    if agent is None:
        agent = _init_new_weights(trainer_cls, config)[sec_name]
    return agent


def _init_new_weights(trainer_cls, config):
    # Create a new maddpg trainer which will be used to train the adversarial policy
    trainer = trainer_cls(env='mpe-push', config=config)
    new_weights = deepcopy(trainer.get_weights())
    return new_weights


def create_pbt_eval_func(stored_agents, main_id):
    sec_id = 1 - main_id
    main_name = f'policy_{main_id}'
    secondary_name = f'policy_{sec_id}'

    # Define the function
    def pbt_eval_function(trainer: Trainer, eval_workers: WorkerSet):
        """Custom evaluation function for PBT.
        Args:
            trainer (Trainer): trainer class to evaluate.
            eval_workers (WorkerSet): evaluation workers.
        Returns:
            metrics (dict): evaluation metrics dict.
        """

        remote_workers = eval_workers.remote_workers()
        if any([agent is None for agent in stored_agents['current']]):
            # As long as not all agents have been trained against at least once, we don't start evaluation yet
            metrics = collect_metrics(eval_workers.local_worker(),
                                      remote_workers)
            return metrics  # We return an essentially empty metrics dict (contains a bunch of nans)

        num_opponents = len(stored_agents['current'])

        my_eval_vals = {}

        for old_i in range(len(stored_agents['old'][0])):
            # This lambda returns the weights of opponent with given index for the old_ith generation of saved weights
            old_weights = lambda op_i: stored_agents['old'][op_i][old_i]

            metrics = _evaluate_metrics_helper(num_opponents, remote_workers, trainer, old_weights)

            my_eval_vals[f'eval_old_{old_i}'] = metrics['policy_reward_mean'][main_name]

        # Simply return the current agent with the given opponent id
        current_weights = lambda op_i: stored_agents['current'][op_i]

        metrics = _evaluate_metrics_helper(num_opponents, remote_workers, trainer, current_weights)

        my_eval_vals[f'eval_current'] = metrics['policy_reward_mean'][main_name]

        # TODO: Should we also log separate metrics for each opponent_id?

        metrics['my_eval_vals'] = my_eval_vals
        return metrics

    def _evaluate_metrics_helper(num_opponents, remote_workers, trainer, weight_func):
        """Helper function that runs eval workers for all agents that can be received from the function.
        weight_func is lambda that receives the opponent index and returns the opponent weights that are supposed to be evaluated."""
        num_workers = len(remote_workers)

        # If num_workers < num_agents we need to do multiple passes of eval
        num_eval_passes = math.ceil(num_opponents / num_workers)

        all_eps = []
        for pass_i in range(num_eval_passes):
            opponent_i = pass_i * num_workers
            # Number of workers to use this pass, either num_workers or the number of agents that are left
            num_workers_this_pass = num_workers if opponent_i + num_workers <= num_opponents else num_opponents - opponent_i

            worker_slice = remote_workers[:num_workers_this_pass]  # Iterate over first n_w_t_p workers only

            # Set the weights for the workers
            for worker in worker_slice:
                weights = weight_func(opponent_i)
                # weights = stored_agents['old'][agent_i][old_i]
                worker.set_weights.remote(({secondary_name: weights}))
                opponent_i += 1

            # Run the eval episodes
            for _ in range(trainer.config['evaluation_num_episodes']):
                # Calling .sample() runs exactly one episode per worker due to how the
                # eval workers are configured.
                ray.get([w.sample.remote() for w in worker_slice])

            # Collect the accumulated episodes on the workers
            episodes, _ = collect_episodes(remote_workers=worker_slice, timeout_seconds=99999)  # TODO timeout? what's up with that

            all_eps += episodes
        # All eps collected for this set of agents

        # Summarize the episode stats into a metrics dict
        # You can compute metrics from the episodes manually, or use the
        # convenient `summarize_episodes()` utility:
        metrics = summarize_episodes(all_eps)
        # Note that the above two statements are the equivalent of:
        # metrics = collect_metrics(None, worker_slice)

        # Some sanity checks
        assert metrics['episodes_this_iter'] == trainer.config['evaluation_num_episodes'] * num_opponents
        return metrics

    # Return it
    return pbt_eval_function
