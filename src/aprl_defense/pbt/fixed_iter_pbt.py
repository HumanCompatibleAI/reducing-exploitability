from copy import deepcopy

import wandb

from src.aprl_defense.common.train import run_trainer
from src.aprl_defense.training_manager import TrainingManager


class FixedIterPBTManager(TrainingManager):
    def __init__(self, log_dir: str, alg: str, max_timesteps: int, checkpoint_freq: int, main_id: int, scheduler_func, iter_steps=25000,
                 num_agents=10):
        super().__init__(log_dir, alg, max_timesteps, checkpoint_freq)

        self.iter_steps = 25000 if iter_steps is None else iter_steps
        self.main_id = main_id
        self.scheduler_func = scheduler_func
        self.num_agents = num_agents
        self.iter_steps = iter_steps

    def _train_loop(self, checkpoint_freq, max_steps, trainer, config):
        print(f'Running PBT')

        checkpoint_dir = self.log_dir / 'checkpoints'
        checkpoint_dir.mkdir(exist_ok=True, parents=True)

        # Initialize PBT training
        primary_name = f'policy_{self.main_id}'
        sec_id = 1 - self.main_id
        secondary_name = f'policy_{sec_id}'
        # Table that contains infos for each agent, used to determine next opponent.
        # Currently temporary simply represented by a dict
        agent_infos = {'num_agents': self.num_agents}
        stored_agents = []
        for i in range(self.num_agents):
            stored_agents.append(None)

        log_setup = [(primary_name, 'main'), (secondary_name, f'opponent_{sec_id}')]

        num_checkpoints_collected = 0

        timesteps_total = 0
        iterations = 0
        while timesteps_total < max_steps:
            # Determine id of secondary agent for this training iterations
            sec_id = self.scheduler_func(agent_infos)

            # Get stored weights, initialize new ones if necessary
            agent_weights = _get_agent(sec_id, stored_agents, config, secondary_name, self.trainer_cls)
            # Use these weights in the trainer (for secondary agent)
            trainer.set_weights({secondary_name: agent_weights})

            # Log which opponent is being played against
            wandb.log({f'opponent_id': sec_id,
                       'timestep': timesteps_total + 1})  # The + 1 ensures this timestep does not have duplicate values for op_id

            timesteps_total = run_trainer(trainer,
                                          -1,
                                          self.log_dir,
                                          timesteps_total + self.iter_steps,
                                          config,
                                          log_setup=log_setup,
                                          save=False)

            # Log which opponent was played against
            wandb.log({f'opponent_id': sec_id,
                       'timestep': timesteps_total})

            # Store updated weights
            new_weights = deepcopy(trainer.get_weights()[secondary_name])
            stored_agents[sec_id] = new_weights

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * checkpoint_freq
            if timesteps_total > next_checkpoint_at:
                trainer.save(checkpoint_dir=checkpoint_dir)
                num_checkpoints_collected += 1

            iterations += 1

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return None  # Trains all


def _get_agent(id, stored_agents, config, sec_name, trainer_cls):
    agent = stored_agents[id]
    if agent is None:
        agent = _init_new_weights(trainer_cls, config)[sec_name]
    return agent


def _init_new_weights(trainer_cls, config):
    # Create a new maddpg trainer which will be used to train the adversarial policy
    trainer = trainer_cls(env='mpe-push', config=config)
    new_weights = deepcopy(trainer.get_weights())
    return new_weights
