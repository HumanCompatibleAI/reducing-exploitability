from copy import deepcopy
from pathlib import Path

import ray
import wandb
from ray.tune.logger import pretty_print
from tqdm import tqdm

import aprl_defense.configs.train
from aprl_defense.common.io import get_checkpoint_file
from aprl_defense.common.train import run_trainer, init_victim_weights
from aprl_defense.common.utils import create_trainer
from aprl_defense.pbt.fixed_iter_pbt import _get_agent_weights
from aprl_defense.pbt.utils import create_pbt_eval_func, custom_eval_log
from aprl_defense.pbt_scheduler import random_policy_scheduler
from aprl_defense.training_manager import TrainingManager, create_ma_trainer


class SeparatePBTManager(TrainingManager):
    def __init__(self, log_dir: str,
                 alg: str,
                 max_timesteps: int,
                 checkpoint_freq: int,
                 override: str,
                 main_id: int,
                 scheduler_func,
                 main_steps: int = 25,
                 num_ops: int = 10,
                 op_experience_fraction: float = 1.,
                 baseline_checkpoint=None,
                 specific_folder=False,
                 baseline_policy_name='policy_1'):
        super().__init__(log_dir, alg, max_timesteps, checkpoint_freq, override)

        self.main_id = main_id
        self.opponent_id = 1 - main_id
        # self.main_policy_name = f'policy_{self.main_id}'
        self.scheduler_func = scheduler_func
        self.num_ops = num_ops
        self.main_iter_steps = main_steps

        self.baseline_checkpoint = baseline_checkpoint
        self.specific_folder = specific_folder

        # Calculate the number of steps each opponent is trained for
        self.op_iter_steps = op_experience_fraction * self.num_ops * self.main_iter_steps

        self.stored_agents = {
            'current': [None] * self.num_ops,
            'old': [[] for _ in range(self.num_ops)]
        }

        # Update config with settings that are specific to PBT
        self.config.update(aprl_defense.configs.train.pbt)

        # Config for catching up does not need eval
        self.catch_up_config = deepcopy(self.config)

        self.config.update(aprl_defense.configs.eval.online_eval)

        # Apply conditional eval settings
        self.config.update({
            "custom_eval_function": create_pbt_eval_func(self.stored_agents, self.main_id),

            # Enable evaluation
            "evaluation_interval": self.checkpoint_freq // 25,
        })
        self.main_policy = 'main'
        self.opponent_policies = [f'op_{i}' for i in range(self.num_ops)]

        self.opponent_trainer = None

        self.baseline_policy_name = baseline_policy_name

        self.agent_infos = {'num_agents': self.num_ops,
                       'opponent_timesteps': [0] * self.num_ops,
                       'deactivated': [False] * self.num_ops,
                       'opponent_policies': self.opponent_policies}

    def train(self):
        print(f"Saving output to {self.log_dir}")

        use_local_critic = False  # Only applies to MADDPG agent.

        policies_to_train = None  # Trains all

        policies = {}

        for policy in [self.main_policy] + self.opponent_policies:
            agent_id = self.main_id if policy == self.main_policy else self.opponent_id
            obs_space = self.env.observation_space_dict[agent_id]
            act_space = self.env.action_space_dict[agent_id]
            policies[policy] = (
                None,
                obs_space,
                act_space,
                {
                    'agent_id': agent_id,
                    'use_local_critic': use_local_critic
                }
            )
        # The scheduler decides which opponent policy is taken when a new policy is chosen for opponent agent
        scheduler = random_policy_scheduler

        # Using a closure directly doesn't work, but this weird construction does
        def make(main_id, main_policy, agent_infos):
            def policy_mapping_function(agent_id):
                if agent_id == main_id:
                    return main_policy
                else:  # The agent that is controlled by one of the opponent agents
                    # return random_policy_scheduler(self.agent_infos)
                    return scheduler(agent_infos)

            return policy_mapping_function

        # Convert to tune func. TODO: is this necessary?
        ray_map_func = ray.tune.function(make(self.main_id, self.main_policy, self.agent_infos))

        multiagent = {'policies': policies,
                      'policy_mapping_fn': ray_map_func,
                      'policies_to_train': [self.main_policy]}

        self.config['env_config']['scenario_name'] = self.scenario_name
        self.config['multiagent'] = multiagent
        conf_copy = deepcopy(self.config)
        main_trainer = create_trainer(self.trainer_cls, conf_copy)

        self.config['multiagent']['policies_to_train'] = self.opponent_policies
        self.opponent_trainer = create_trainer(self.trainer_cls, conf_copy)

        print('Config:')
        print(pretty_print(main_trainer.config))

        # Start the actual training
        self._train_loop(self.checkpoint_freq, self.max_timesteps, main_trainer, self.config)

    def _train_loop(self, checkpoint_freq, max_steps, main_trainer, config):
        print(f'Running PBT')

        # Initialize PBT training
        # secondary_id = 1 - self.main_id
        # secondary_name = f'policy_{secondary_id}'
        # Table that contains infos for each agent, used to determine next opponent.
        # Currently temporary simply represented by a dict

        # Create second trainer that is only for training the secondary agent
        # sec_pol_to_train = [secondary_name]

        # Initialize agent weights not necessary but can be helpful for debugging
        # for i in tqdm(range(self.num_ops), desc="Initializing policies"):
        #     # This is a noop, unless the weights are not initialized yet in which case they will be
        #     self.stored_agents['current'][i] = _get_agent_weights(i, self.stored_agents, self.config, secondary_name, self.trainer_cls)

        # Initialize trainer to 'catch up' secondary policies
        # train_op_trainer = create_ma_trainer(policies_to_train=sec_pol_to_train,
        #                                      use_local_critic=self._get_use_local_critic(),
        #                                      config=self.catch_up_config,
        #                                      scenario_name=self.scenario_name,
        #                                      env=self.env,
        #                                      trainer_cls=self.trainer_cls)

        checkpoint_dir = self.log_dir / 'checkpoints'
        checkpoint_dir.mkdir(exist_ok=True, parents=True)

        if self.baseline_checkpoint is not None:
            file = get_checkpoint_file(self.baseline_checkpoint, specific_folder=self.specific_folder)

            baseline_weights = init_victim_weights(Path(file), self.scenario_name, self.baseline_policy_name, self.trainer_cls)
            self.stored_agents['baseline'] = [baseline_weights]

        # Initial opponent logging
        # for sec_id in range(self.num_ops):
        #     wandb.log({f'op_active_{sec_id}': 0, 'timestep': 0})  # Set all opponents as initally disabled

        timesteps_main_total = 0
        opponent_timesteps = 0
        iterations = 0
        num_checkpoints_collected = 0
        pbar = tqdm(total=max_steps)
        while timesteps_main_total < max_steps:
            # Determine id of secondary agent for this training iterations

            # Get stored weights, initialize new ones if necessary
            # sec_weights = _get_agent_weights(sec_id, self.stored_agents, config, secondary_name, self.trainer_cls)
            main_weights = main_trainer.get_weights(self.main_policy)

            # Sync main weights
            self.opponent_trainer.set_weights(main_weights)
            # train_op_trainer.set_weights({secondary_name: sec_weights})

            # Log which opponent is active
            # wandb.log({f'op_active_{sec_id}': 0,
            #            'timestep': timesteps_main_total + opponent_timesteps + 1})  # The + 1 ensures this timestep does not have duplicate values for op_id
            # wandb.log({f'op_active_{sec_id}': 1,
            #            'timestep': timesteps_main_total + opponent_timesteps + 2})  # The + 1 ensures this timestep does not have duplicate values for op_id


            results = self.opponent_trainer.train()

            timesteps_op_trainer = results['timesteps_total']

            # Sync opponent weights
            for op in self.opponent_policies:
                weights = self.opponent_trainer.get_weights(op)
                main_trainer.set_weights(weights)

            results = main_trainer.train()
            timesteps_main_total = results['timesteps_total']

            # Sanity check
            if self.main_policy not in results['policy_reward_mean']:
                # raise ValueError(f"No result values collected. Is 'horizon' set?")
                pass
            else:
                # Log results for iteration
                for policy, value in results['policy_reward_mean'].items():
                    wandb.log({policy: value,
                               'timestep': timesteps_main_total + timesteps_op_trainer})

                custom_eval_log(results, timesteps_main_total + timesteps_op_trainer)

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * checkpoint_freq
            if timesteps_main_total > next_checkpoint_at:
                main_trainer.save(checkpoint_dir=checkpoint_dir)
                num_checkpoints_collected += 1

                # # Update stored old opponents
                # for i in range(len(self.stored_agents['current'])):
                #     weights = _get_agent_weights(i, self.stored_agents, self.config, secondary_name, self.trainer_cls)
                #     weights_copy = deepcopy(weights)
                #     self.stored_agents['old'][i].append(weights_copy)

            # This log makes it easier to see how many main timesteps passed
            wandb.log({f'main_steps': iterations,
                       'timestep': timesteps_main_total})  # The + 1 ensures this timestep does not have duplicate values for op_id

            iterations += 1
            pbar.update(timesteps_main_total - pbar.n)
        pbar.close()

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return [self.main_policy_name]
