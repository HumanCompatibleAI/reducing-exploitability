from copy import deepcopy

import wandb
from tqdm import tqdm

import aprl_defense.configs.train
from aprl_defense.common.train import run_trainer
from aprl_defense.pbt.fixed_iter_pbt import _get_agent_weights, create_pbt_eval_func
from aprl_defense.training_manager import TrainingManager, create_trainer


class SeparatePBTManager(TrainingManager):
    def __init__(self, log_dir: str, alg: str, max_timesteps: int, checkpoint_freq: int, override: str, main_id: int, scheduler_func, iter_steps=25,
                 num_ops=10, cycle_agents=False, op_iter_steps=None):
        super().__init__(log_dir, alg, max_timesteps, checkpoint_freq, override)

        self.main_id = main_id
        self.main_policy_name = f'policy_{self.main_id}'
        self.scheduler_func = scheduler_func
        self.num_ops = num_ops
        self.main_iter_steps = iter_steps
        self.op_iter_steps = iter_steps if op_iter_steps is None else op_iter_steps
        self.cycle_agents = cycle_agents

        self.stored_agents = {
            'current': [None] * self.num_ops,
            'old': [[]] * self.num_ops
        }

        # Update config with settings that are specific to PBT
        self.config.update(aprl_defense.configs.train.pbt)

        # Config for catching up does not need eval
        self.catch_up_config = deepcopy(self.config)

        self.config.update(aprl_defense.configs.eval.online_eval)

        # Apply conditional eval settings
        self.config.update({
            "custom_eval_function": create_pbt_eval_func(self.stored_agents, self.main_id),

            # Enable evaluation
            "evaluation_interval": self.checkpoint_freq // 25,
        })

    def _train_loop(self, checkpoint_freq, max_steps, train_main_trainer, config):
        print(f'Running PBT')

        # Initialize PBT training
        secondary_id = 1 - self.main_id
        secondary_name = f'policy_{secondary_id}'
        # Table that contains infos for each agent, used to determine next opponent.
        # Currently temporary simply represented by a dict
        agent_infos = {'num_agents': self.num_ops,
                       'opponent_timesteps': [0] * self.num_ops,
                       'deactivated': [False] * self.num_ops}

        # Create second trainer that is only for training the secondary agent
        sec_pol_to_train = [secondary_name]

        # Initialize trainer to 'catch up' secondary policies
        train_op_trainer = create_trainer(policies_to_train=sec_pol_to_train,
                                          use_local_critic=self._get_use_local_critic(),
                                          config=self.catch_up_config,
                                          scenario_name=self.scenario_name,
                                          env=self.env,
                                          trainer_cls=self.trainer_cls)

        checkpoint_dir = self.log_dir / 'checkpoints'
        checkpoint_dir.mkdir(exist_ok=True, parents=True)

        # Initial opponent logging
        for sec_id in range(self.num_ops):
            wandb.log({f'op_active_{sec_id}': 0, 'timestep': 0})  # Set all opponents as initally disabled

        timesteps_main_total = 0
        opponent_timesteps = 0
        iterations = 0
        num_checkpoints_collected = 0
        pbar = tqdm(total=max_steps)
        while timesteps_main_total < max_steps:
            # Determine id of secondary agent for this training iterations
            sec_id = self.scheduler_func(agent_infos)

            # Get stored weights, initialize new ones if necessary
            sec_weights = _get_agent_weights(sec_id, self.stored_agents, config, secondary_name, self.trainer_cls)
            main_weights = train_main_trainer.get_weights(self.main_policy_name)

            # Update op trainer weights
            train_op_trainer.set_weights(main_weights)
            train_op_trainer.set_weights({secondary_name: sec_weights})

            # Log which opponent is active
            wandb.log({f'op_active_{sec_id}': 0,
                       'timestep': timesteps_main_total + opponent_timesteps + 1})  # The + 1 ensures this timestep does not have duplicate values for op_id
            wandb.log({f'op_active_{sec_id}': 1,
                       'timestep': timesteps_main_total + opponent_timesteps + 2})  # The + 1 ensures this timestep does not have duplicate values for op_id

            log_setup = [(secondary_name, f'opponent_{sec_id}')]
            opponent_timesteps = run_trainer(train_op_trainer,
                                             -1,
                                             self.log_dir,
                                             opponent_timesteps + self.op_iter_steps,
                                             config=config,
                                             save=False,
                                             log_setup=log_setup,
                                             add_log_timesteps=timesteps_main_total,
                                             progress_bar=False)

            sec_weights = train_op_trainer.get_weights()[secondary_name]

            # Use these weights in the trainer (for secondary agent)
            train_main_trainer.set_weights({secondary_name: sec_weights})

            log_setup = [(self.main_policy_name, 'main')]

            timesteps_main_total, mean_rew = run_trainer(train_main_trainer,
                                                         -1,
                                                         self.log_dir,
                                                         self.main_iter_steps + timesteps_main_total,
                                                         config,
                                                         log_setup=log_setup,
                                                         return_mean_rew=self.main_policy_name,
                                                         add_log_timesteps=opponent_timesteps,
                                                         progress_bar=False,
                                                         save=False)

            # Log which opponent is active
            wandb.log({f'op_active_{sec_id}': 1,
                       'timestep': timesteps_main_total + opponent_timesteps - 1})  # The + 1 ensures this timestep does not have duplicate values for op_id
            wandb.log({f'op_active_{sec_id}': 0,
                       'timestep': timesteps_main_total + opponent_timesteps})  # The + 1 ensures this timestep does not have duplicate values for op_id

            # Store updated weights
            new_weights = deepcopy(train_main_trainer.get_weights()[secondary_name])
            self.stored_agents['current'][sec_id] = new_weights

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * checkpoint_freq
            if timesteps_main_total > next_checkpoint_at:
                train_main_trainer.save(checkpoint_dir=checkpoint_dir)
                num_checkpoints_collected += 1

                # Update old opponents
                for i in range(len(self.stored_agents['current'])):
                    weights = _get_agent_weights(i, self.stored_agents, self.config, secondary_name, self.trainer_cls)
                    weights_copy = deepcopy(weights)
                    self.stored_agents['old'][i].append(weights_copy)

            iterations += 1
            pbar.update(timesteps_main_total - pbar.n)
        pbar.close()

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return [self.main_policy_name]
