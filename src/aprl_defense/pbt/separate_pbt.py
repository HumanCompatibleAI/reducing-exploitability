from copy import deepcopy
from pathlib import Path

import ray
import wandb
from ray import cloudpickle
from ray.tune.logger import pretty_print
from tqdm import tqdm

import aprl_defense.configs.train
from aprl_defense.common.io import get_checkpoint_file
from aprl_defense.common.train import run_trainer, load_saved_weights
from aprl_defense.common.utils import create_trainer
from aprl_defense.pbt.fixed_iter_pbt import _get_agent_weights
from aprl_defense.pbt.utils import create_pbt_eval_func, custom_eval_log, create_pbt_eval_func_separate
from aprl_defense.pbt_scheduler import random_policy_scheduler
from aprl_defense.training_manager import TrainingManager, create_ma_trainer


class SeparatePBTManager(TrainingManager):
    def __init__(self, env,
                 log_dir: Path,
                 alg: str,
                 max_timesteps: int,
                 checkpoint_freq: int,
                 override: str,
                 main_id: int,
                 scheduler_func,
                 num_ops: int = 10,
                 op_experience_factor: int = 1,
                 baseline_checkpoint=None,
                 specific_folder=False,
                 baseline_policy_name='policy_1',
                 num_workers=None,
                 episodes_per_worker=10):
        super().__init__(env, log_dir, alg, max_timesteps, checkpoint_freq, override)

        self.main_id = main_id
        self.opponent_id = 1 - main_id

        self.scheduler_func = scheduler_func
        self.num_ops = num_ops

        self.baseline_checkpoint = baseline_checkpoint
        self.specific_folder = specific_folder

        # Calculate the number of steps each opponent is trained for
        self.op_iter_factor = op_experience_factor * self.num_ops

        self.eval_agents = {
            'gen': []
        }

        # Update config with settings that are specific to PBT
        self.config.update(aprl_defense.configs.train.pbt)

        # Override PBT settings if given
        if num_workers is not None:
            self.config['num_workers'] = num_workers

        self.config['train_batch_size'] = self.config['num_workers'] * 25 * episodes_per_worker if self.config['num_workers'] > 0 else 25 * episodes_per_worker

        # Config for catching up does not need eval
        self.catch_up_config = deepcopy(self.config)

        self.config.update(aprl_defense.configs.eval.online_eval)

        # Apply conditional eval settings
        self.config.update({
            "custom_eval_function": create_pbt_eval_func_separate(self.eval_agents, self.main_id),

        })
        self.main_policy = 'main'
        self.opponent_policies = [f'op_{i}' for i in range(self.num_ops)]

        self.opponent_trainer = None

        self.baseline_policy_name = baseline_policy_name

        self.agent_infos = {'num_agents': self.num_ops,
                            'opponent_timesteps': [0] * self.num_ops,
                            'deactivated': [False] * self.num_ops,
                            'opponent_policies': self.opponent_policies}

        self._handle_config_override(override)

        # Enable evaluation
        self.config.update({"evaluation_interval": min(self.checkpoint_freq // self.config['train_batch_size'], 1)})

    def train(self):
        print(f"Saving output to {self.log_dir}")

        use_local_critic = False  # Only applies to MADDPG agent.

        # Add baseline policy to eval agents
        if self.baseline_checkpoint is not None:
            file = get_checkpoint_file(self.baseline_checkpoint, specific_folder=self.specific_folder)

            baseline_weights = load_saved_weights(Path(file), self.scenario_name, self.baseline_policy_name, self.trainer_cls)
            self.eval_agents['baseline'] = baseline_weights

        policies = {}

        for policy in [self.main_policy] + self.opponent_policies + ['eval_op']:
            agent_id = self.main_id if policy == self.main_policy else self.opponent_id
            obs_space = self.env.observation_space_dict[agent_id]
            act_space = self.env.action_space_dict[agent_id]
            policies[policy] = (
                None,
                obs_space,
                act_space,
                {
                    'agent_id': agent_id,
                    'use_local_critic': use_local_critic
                }
            )
        # The scheduler decides which opponent policy is taken when a new policy is chosen for opponent agent
        scheduler = random_policy_scheduler

        # Using a closure directly doesn't work, but this weird construction does
        def make_train_fn(main_id, main_policy, agent_infos):
            def policy_mapping_function(agent_id, episode, **kwargs):
                if agent_id == main_id:
                    return main_policy
                else:  # The agent that is controlled by one of the opponent agents
                    # return random_policy_scheduler(self.agent_infos)
                    return scheduler(agent_infos)

            return policy_mapping_function

        def make_eval_fn(main_id, main_policy):
            def policy_mapping_function(agent_id, episode, **kwargs):
                if agent_id == main_id:
                    return main_policy
                else:  # The agent that is controlled by one of the opponent agents
                    return 'eval_op'

            return policy_mapping_function

        multiagent = {'policies': policies,
                      'policy_mapping_fn': make_train_fn(self.main_id, self.main_policy, self.agent_infos),
                      'policies_to_train': [self.main_policy]}

        self.config['env_config']['scenario_name'] = self.scenario_name
        self.config['multiagent'] = multiagent

        # Update eval config
        self.config['evaluation_config'] = {}
        self.config["evaluation_config"]['multiagent'] = deepcopy(multiagent)
        self.config["evaluation_config"]['multiagent']['policy_mapping_fn'] = make_eval_fn(self.main_id, self.main_policy)

        conf_copy = deepcopy(self.config)
        main_trainer = create_trainer(self.trainer_cls, conf_copy)

        # These are the settings that need to be changed for the opponent trainer
        # The iter factor determines how much more training is needed for opponents
        if self.op_iter_factor > 0:  # op_iter_factor < 0 means op_exp_factor < 0 which means disable increaseing op experience
            self.config['train_batch_size'] = self.config['train_batch_size'] * self.op_iter_factor

        self.config['evaluation_interval'] = None  # Opponent trainer does not need evaluation
        # Number episodes each time evaluation runs.
        self.config["evaluation_num_episodes"] = 0
        self.config["evaluation_num_workers"] = 0
        self.config["evaluation_parallel_to_training"] = False

        self.config['multiagent']['policies_to_train'] = self.opponent_policies
        self.opponent_trainer = create_trainer(self.trainer_cls, self.config)

        print('Opponent config:')
        print(pretty_print(self.config))

        # Start the actual training
        self._train_loop(main_trainer)

    def _train_loop(self, main_trainer):
        print(f'Running PBT')

        checkpoint_dir = self.log_dir / 'checkpoints'
        checkpoint_dir.mkdir(exist_ok=True, parents=True)

        # Save params file
        with (checkpoint_dir / 'params.pkl').open(mode='wb') as file:
            cloudpickle.dump(self.config, file=file)

        # Initial opponent logging
        # for sec_id in range(self.num_ops):
        #     wandb.log({f'op_active_{sec_id}': 0, 'timestep': 0})  # Set all opponents as initally disabled

        timesteps_main_total = 0
        iterations = 0
        num_checkpoints_collected = 0
        pbar = tqdm(total=self.max_timesteps)
        while timesteps_main_total < self.max_timesteps:
            # Determine id of secondary agent for this training iterations

            # Get stored weights, initialize new ones if necessary
            # sec_weights = _get_agent_weights(sec_id, self.stored_agents, config, secondary_name, self.trainer_cls)
            main_weights = main_trainer.get_weights(self.main_policy)
            # Sync main weights
            self.opponent_trainer.set_weights(main_weights)

            self.opponent_trainer.workers.sync_weights()
            # AFAICT in newer versions of RLLib we will be able to specify which weights to sync
            # self.opponent_trainer.workers.sync_weights(policies=[self.main_policy])

            # train_op_trainer.set_weights({secondary_name: sec_weights})

            # Log which opponent is active
            # wandb.log({f'op_active_{sec_id}': 0,
            #            'timestep': timesteps_main_total + opponent_timesteps + 1})  # The + 1 ensures this timestep does not have duplicate values for op_id
            # wandb.log({f'op_active_{sec_id}': 1,
            #            'timestep': timesteps_main_total + opponent_timesteps + 2})  # The + 1 ensures this timestep does not have duplicate values for op_id

            results = self.opponent_trainer.train()

            timesteps_op_trainer = results['timesteps_total']

            self._log(results, timesteps_main_total, timesteps_op_trainer)

            # Sync opponent weights
            for op in self.opponent_policies:
                weights = self.opponent_trainer.get_weights(op)
                main_trainer.set_weights(weights)

            main_trainer.workers.sync_weights()
            # main_trainer.workers.sync_weights(policies=self.opponent_policies)

            results = main_trainer.train()
            timesteps_main_total = results['timesteps_total']

            self._log(results, timesteps_main_total, timesteps_op_trainer)

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * self.checkpoint_freq
            if timesteps_main_total > next_checkpoint_at:
                main_trainer.save(checkpoint_dir=checkpoint_dir)
                num_checkpoints_collected += 1

                next_gen = []
                # Update stored old opponents
                for op_policy in self.opponent_policies:
                    weights = self.opponent_trainer.get_weights(op_policy)
                    weights_copy = deepcopy(weights)
                    next_gen.append(weights_copy)
                # Append the list of the next generation of opponents
                self.eval_agents['gen'].append(next_gen)

            # This log makes it easier to see how many main timesteps passed
            wandb.log({f'main_steps': iterations,
                       'timestep': timesteps_main_total})  # The + 1 ensures this timestep does not have duplicate values for op_id

            iterations += 1
            pbar.update(timesteps_main_total - pbar.n)
        pbar.close()

    def _log(self, results, timesteps_main_total, timesteps_op_trainer):

        if self.main_policy not in results['policy_reward_mean']:  # Sanity check
            # raise ValueError(f"No result values collected. Is 'horizon' set?")
            pass
        else:
            # Log results for iteration
            for policy, value in results['policy_reward_mean'].items():
                wandb.log({policy: value,
                           'timestep': timesteps_main_total + timesteps_op_trainer,
                           'timestep_main': timesteps_main_total})  # Logging multiple x-axes

            custom_eval_log(results, timesteps_main_total + timesteps_op_trainer, timesteps_main_total)

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return [self.main_policy_name]
