from collections import defaultdict, deque
from copy import deepcopy
from typing import List

import wandb
from tqdm import tqdm

import aprl_defense.configs.train
import aprl_defense.configs.eval
from aprl_defense.common.train import load_saved_weights
from aprl_defense.common.utils import create_trainer, spaces_from_env
from aprl_defense.pbt.utils import custom_eval_log, create_pbt_eval_func_separate, create_policy_mapping_function
from aprl_defense.training_manager import TrainingManager
from aprl_defense.trial.settings import TrialSettings, RLSettings


class SingleTrainerPBTManager(TrainingManager):
    def __init__(self,
                 trial_settings: TrialSettings,
                 rl_settings: RLSettings,
                 evaluation_freq: int,
                 main_id: int,
                 num_ops: int = 10,
                 op_experience_factor: float = 1.,
                 baseline_artifacts=None,
                 specific_folder=False,
                 baseline_policy_name='policy_1',
                 new_op_interval=-1,
                 policy_cache=None):
        super().__init__(trial_settings, rl_settings)

        # Misc checks
        if len(self.rl.algs) > 1:
            raise ValueError(f"PBT only accepts a single alg, received {self.rl.algs}")

        self.evaluation_freq = evaluation_freq

        self.main_id = main_id
        self.opponent_id = 1 - main_id

        self.num_ops = num_ops
        # self.main_iter_steps = main_steps

        self.baseline_artifacts = baseline_artifacts
        self.specific_folder = specific_folder

        # Calculate what is the factor of iterations that have to be performed more for opponents compared to the main agent
        # Iterations scale with the number of opponents. If op_expericence_factor os -1, set to imbalanced mode where all opponents have together get as many
        # timesteps as the main agent alone
        self.op_iteration_factor = int(op_experience_factor * self.num_ops if op_experience_factor > 0 else 1)

        self.eval_agents = {
            'gen': []
        }
        self.main_policy = 'main'

        self.opponent_policies = [f'op_{i}' for i in range(self.num_ops)]

        op_fraction = 1
        self.num_ops_per_iteration = int(len(self.opponent_policies) * op_fraction)
        if self.num_ops_per_iteration <= 0:  # Always train at least one op
            self.num_ops_per_iteration = 1

        self.baseline_policy_name = baseline_policy_name

        self.agent_infos = {'num_agents': self.num_ops,
                            'opponent_timesteps': [0] * self.num_ops,
                            'deactivated': [False] * self.num_ops,
                            'opponent_policies': self.opponent_policies}

        self.main_agent_id = None
        self.op_agent_id = None

        self.new_op_interval = new_op_interval

        self.policy_episode_num = defaultdict(int)

        if policy_cache is None:
            raise ValueError('Please provide a location for the policy cache')
        self.policy_cache = policy_cache

        # Add baseline policy to eval agents
        if self.baseline_artifacts is not None:
            self.eval_agents['baselines'] = [[]]
            for baseline_artifact in self.baseline_artifacts:
                file = self.trial.artifact_manager.get_remote_checkpoint(baseline_artifact)

                baseline_weights = load_saved_weights(file, self.scenario_name, self.baseline_policy_name, self.trainer_classes[0])
                self.eval_agents['baselines'][0].append(baseline_weights)

    def set_up_config(self):
        # Perform config setup of ancestor
        super().set_up_config()

        # Perform config setup for PBT

        # Update config with settings that are specific to PBT
        self.config.update(aprl_defense.configs.train.pbt)

        self.config.update(aprl_defense.configs.eval.online_eval)

        # Apply conditional eval settings
        self.config.update({
            "custom_eval_function": create_pbt_eval_func_separate(self.eval_agents, self.main_id),

            # Set evaluation interval to enable evaluation
            # The interval is with respect to training itereations, i.e. calls of trainer.train()
            # Currently the `checkpoint_freq` also determines the evaluation interval
            # Consequently, num_ops + 1 intervals equal 1 training iteration for main, i.e. main trains batch_size steps
            "evaluation_interval": (self.op_iteration_factor + 1) * max(self.evaluation_freq // self.config['train_batch_size'], 1),
        })

        use_local_critic = False  # Only applies to MADDPG agent.

        action_spaces, observation_spaces, agent_ids = spaces_from_env(self.env)
        self.main_agent_id = agent_ids[self.main_id]
        self.op_agent_id = agent_ids[self.opponent_id]

        policies = {}
        for policy in [self.main_policy] + self.opponent_policies + ['eval_op']:
            agent_id = self.main_agent_id if policy == self.main_policy else self.op_agent_id

            obs_space = observation_spaces[agent_id]
            act_space = action_spaces[agent_id]
            policies[policy] = (
                None,
                obs_space,
                act_space,
                {
                    'agent_id': agent_id,
                    'use_local_critic': use_local_critic,
                    # 'multiagent': {'policy_map_capacity': 4}
                }
            )
        # Apparently fields don't work but variables do in this closure. Values are constant anyway, so it doesn't matter
        main_agent_id = self.main_agent_id
        main_policy = self.main_policy

        def policy_mapping_fn_eval(agent_id, episode, **kwargs):
            if agent_id == main_agent_id:
                return main_policy
            else:  # The agent that is controlled by one of the opponent agents
                return 'eval_op'

        # We reduce main memory usage by setting this lower. Each worker might have up to this number of policies in main memory. Because we distribute the
        # opponents to the workers deterministically, we only need a subset of all opponents at each worker

        worker_policy_map_capacity = max(1, self.num_ops // self.config['num_workers']) + 2  # + 2 for eval_op and main

        print(f"Creating policy cache folder at {self.policy_cache}")
        self.policy_cache.mkdir(parents=True, exist_ok=True)

        multiagent = {'policies': policies,
                      'policy_map_capacity': worker_policy_map_capacity,
                      'policy_mapping_fn': None,
                      'policy_map_cache': str(self.policy_cache),
                      # I'm not 100% positive but it seems like only policies which were declared trainable in this config settings at time of initializing the
                      # trainer can actually be trained. Even if we change this setting later online during training, only a subset of these policies provided
                      # here will be updated. As such it is important that this setting is set correctly and it can't be corrected by chaning this setting
                      # online.
                      'policies_to_train': [main_policy] + self.opponent_policies

                      }

        self.config['env_config']['scenario_name'] = self.scenario_name
        self.config['multiagent'] = multiagent

        # Update eval config
        self.config['evaluation_config'] = {}
        self.config["evaluation_config"]['multiagent'] = deepcopy(multiagent)
        self.config["evaluation_config"]['multiagent']['policy_mapping_fn'] = policy_mapping_fn_eval
        self.config['evaluation_config']['rollout_fragment_length'] = 25

    def set_up_trainer(self) -> None:
        self.trainer = create_trainer(self.trainer_classes[0], self.config)

    def start_training_loop(self):
        # Special policy map setup for more memory efficient PBT on many workers
        driver_policy_capacity = self.num_ops + 2  # + 2 for eval_op and main

        def increase_driver_policy_capacity(worker):
            if worker.worker_index == 0:
                worker.policy_map.deque = deque(maxlen=driver_policy_capacity)
                # TODO copy from original deque
                for item in worker.policy_map.cache:
                    worker.policy_map.deque.append(item)

        self.trainer.workers.foreach_worker(increase_driver_policy_capacity)

        print('Running PBT')
        # save_params(self.config, self.trial.out_path)

        worker_id_to_opponent_pols = self._distribute_ops_to_workers()
        self._set_mapping_fns(worker_id_to_opponent_pols)

        timesteps_main = 0
        timesteps_total = 0
        next_eval_at = self.evaluation_freq

        iterations = 0
        num_checkpoints_collected = 0
        num_ops_added = 0
        pbar = tqdm(total=self.rl.max_timesteps)
        while timesteps_main < self.rl.max_timesteps:

            # ===== OPPONENT TRAINING =====
            self._add_new_policy_if_necessary(num_ops_added, timesteps_main)

            self._to_train_opponent_training(worker_id_to_opponent_pols)
            # Train a random opponent for enough timesteps that there are on average as many timesteps as the main agent trains for each opponent
            results = self._train(iterations=self.op_iteration_factor)
            timesteps_total = results['timesteps_total']

            # ===== MAIN TRAINING =====y
            timesteps_before = timesteps_total
            self._to_train_main_training()
            results = self._train()

            timesteps_total = results['timesteps_total']

            main_additional = timesteps_total - timesteps_before

            timesteps_main += main_additional

            if self.main_policy not in results['policy_reward_mean']:  # Sanity check
                raise ValueError("No result values collected. Is 'horizon' set?")
            else:
                # Log results for iteration
                for policy, value in results['policy_reward_mean'].items():
                    wandb.log({policy: value,
                               'timestep': timesteps_main,
                               'timestep_agg': timesteps_total})  # Logging multiple x-axes
                for policy, num_episodes in self.policy_episode_num.items():
                    wandb.log({f"num_episodes_{policy}": num_episodes,
                               'timestep': timesteps_main,
                               'timestep_agg': timesteps_total})  # Logging multiple x-axes

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * self.trial.checkpoint_freq
            if timesteps_main > next_checkpoint_at:
                self.trial.artifact_manager.save_new_checkpoint()

                num_checkpoints_collected += 1

            if timesteps_main > next_eval_at:
                next_eval_at += self.evaluation_freq

                next_gen = []
                # Update stored old opponents
                for op_policy in self.opponent_policies:
                    weights = self.trainer.get_weights(op_policy)
                    weights_copy = deepcopy(weights)
                    next_gen.append(weights_copy)
                # Append the list of the next generation of opponents
                self.eval_agents['gen'].append(next_gen)

                custom_eval_log(results, timesteps_total, timesteps_main)

            iterations += 1
            pbar.update(timesteps_main - pbar.n)
        pbar.close()

        self.trial.artifact_manager.save_new_checkpoint()
        print("Saved final checkpoint")

    def _distribute_ops_to_workers(self) -> List[List[str]]:
        """Creates a list of the following form:
        worker_id, op_pols running on that worker
          index 0 -->  [op0, op1]
          index 1 -->  [op2, op3] etc."""

        num_workers = self.config['num_workers']
        worker_id_to_opponent_pols: List[List[str]] = []

        # The total number of workers is num_workers + 1, as the driver, which does not perform any rollouts, gets assigned the index 0
        for _ in range(num_workers + 1):
            worker_id_to_opponent_pols.append([])

        if self.num_ops >= num_workers:
            for op_i, op_pol in enumerate(self.opponent_policies):
                # Distribute opponents onto workers 1 ... num_workers
                worker_id = op_i % num_workers
                worker_id_to_opponent_pols[worker_id + 1].append(op_pol)
        else:  # Fewer opponents than workers -> one opponent might train on multiple workers
            for worker_i in range(num_workers):
                op_i = worker_i % self.num_ops
                worker_id_to_opponent_pols[worker_i + 1].append(self.opponent_policies[op_i])

        # Worker with id 0 is main driver and is not assigned anything, as it does not perform rollouts

        return worker_id_to_opponent_pols

    def _set_mapping_fns(self, worker_id_to_opponent_pols):
        # This is necessary, otherwise the closure will not work, for whatever reason
        main_agent_id = self.main_agent_id
        main_policy = self.main_policy

        def mapping_fns(worker):
            worker_id = worker.worker_index
            ops = worker_id_to_opponent_pols[worker_id]
            map_fn = create_policy_mapping_function(main_agent_id, main_policy, ops)
            worker.set_policy_mapping_fn(map_fn)

        self.trainer.workers.foreach_worker(mapping_fns)

    def _to_train_opponent_training(self, worker_id_to_opponent_pols):
        """Set to_train and mapping functions for each worker separately, so each worker only needs access to few policies at a time. This way less policies
        have to be kept in memory"""

        self.currently_training = self.opponent_policies
        opponent_policies = self.opponent_policies

        # Set policies_to_train and policy_mapping_fn according to this setup
        def to_train(worker):
            worker_id = worker.worker_index
            if worker_id != 0:
                worker.set_policies_to_train(worker_id_to_opponent_pols[worker_id])
            else:
                worker.set_policies_to_train(opponent_policies)

        self.trainer.workers.foreach_worker(to_train)

        # Sync before training in case there were changes before
        self.trainer.workers.sync_weights(policies=[self.main_policy])

    def _to_train_main_training(self):

        self.currently_training = [self.main_policy]
        main_policy = self.main_policy
        # Enable training of given policy
        self.trainer.workers.foreach_worker(lambda worker: worker.set_policies_to_train([main_policy]))

        # Sync before training in case there were changes before
        self.trainer.workers.sync_weights(policies=[self.main_policy])

    def _train(self, iterations: int = 1):
        """Setup for training and perform training with given number of iterations. Setup consists of setting which policy to train, setting given
        policy_map_fn, syncing the weights. """

        if iterations < 1:
            raise ValueError(f'Train for at least 1 iterations, iterations was set to {iterations}')

        # Train main agent for given number of iterations
        for _ in range(iterations):
            results = self.trainer.train()
            self._collect_stats(results)

        return results

    def _add_new_policy_if_necessary(self, num_ops_added, timesteps_main):
        """Add new opponent policy if the interval has been reached. If self.new_op_interval is set to -1 never add new policies."""

        if self.new_op_interval > -1 and timesteps_main > (num_ops_added + 1) * self.new_op_interval:
            # If I understand correctly, when adding a new policy for non-standard environments RLlib can't automatically determine obs and action space
            # This is why we manually determin and apply them here
            action_spaces, observation_spaces, agent_ids = spaces_from_env(self.env)
            new_pol_id = f'op_{self.num_ops}'
            self.num_ops += 1
            num_ops_added += 1

            # This policy is for a new opponent, so spaces will be determined by opponent_id
            _ = self.trainer.add_policy(  # Return value is the new policy
                policy_id=new_pol_id,
                policy_cls=type(self.trainer.get_policy(self.opponent_policies[-1])),
                observation_space=observation_spaces[self.op_agent_id],
                action_space=action_spaces[self.op_agent_id]
            )

            # Track the new policy as one of the active policies
            self.opponent_policies.append(new_pol_id)

    def _collect_stats(self, results: dict):
        for pol in self.opponent_policies + [self.main_policy]:
            if pol in self.currently_training:
                key = f'policy_{pol}_reward'
                if key in results['hist_stats']:
                    num_episodes = len(results['hist_stats'][key])  # AFAICT this is the only way to get the number of episodes per policy
                    self.policy_episode_num[pol] += num_episodes
