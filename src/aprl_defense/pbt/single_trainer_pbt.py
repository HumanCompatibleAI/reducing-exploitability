import random
from copy import deepcopy, copy
from pathlib import Path
from random import shuffle
from typing import Optional

import wandb
from tqdm import tqdm

import aprl_defense.configs.train
import aprl_defense.configs.eval
from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.rllib_io import save_params
from aprl_defense.common.train import load_saved_weights
from aprl_defense.common.utils import create_trainer, spaces_from_env
from aprl_defense.pbt.utils import custom_eval_log, create_pbt_eval_func_separate
from aprl_defense.training_manager import TrainingManager


class SingleTrainerPBTManager(TrainingManager):
    def __init__(self, env,
                 log_dir: Path,
                 alg: str,
                 max_timesteps: int,
                 checkpoint_freq: int,
                 num_workers: Optional[int],
                 override: str,
                 artifact_manager: ArtifactManager,
                 num_sgd_iter: Optional[int],
                 main_id: int,
                 num_ops: int = 10,
                 op_experience_factor: int = 1,
                 baseline_artifact=None,
                 specific_folder=False,
                 baseline_policy_name='policy_1',
                 episodes_per_worker=10):
        super().__init__(env, log_dir, alg, max_timesteps, checkpoint_freq, num_workers, override, artifact_manager, num_sgd_iter)

        self.main_id = main_id
        self.opponent_id = 1 - main_id

        self.num_ops = num_ops
        # self.main_iter_steps = main_steps

        self.baseline_artifact = baseline_artifact
        self.specific_folder = specific_folder

        # Calculate what is the factor of iterations that have to be performed more for opponents compared to the main agent
        self.op_iteration_factor = op_experience_factor

        self.eval_agents = {
            'gen': []
        }

        # Update config with settings that are specific to PBT
        self.config.update(aprl_defense.configs.train.pbt)

        # Override PBT settings if given
        if num_workers is not None:
            self.config['num_workers'] = num_workers

        self.config['train_batch_size'] = self.config['num_workers'] * 25 * episodes_per_worker

        # Config for catching up does not need eval
        self.catch_up_config = deepcopy(self.config)

        self.config.update(aprl_defense.configs.eval.online_eval)

        # Apply conditional eval settings
        self.config.update({
            "custom_eval_function": create_pbt_eval_func_separate(self.eval_agents, self.main_id),

            # Enable evaluation
            "evaluation_interval": 16,  # self.checkpoint_freq // 25,
        })
        self.main_policy = 'main'
        self.opponent_policies = [f'op_{i}' for i in range(self.num_ops)]

        op_fraction = 1
        self.num_ops_per_iteration = int(len(self.opponent_policies) * op_fraction)
        if self.num_ops_per_iteration <= 0:  # Always train at least one op
            self.num_ops_per_iteration = 1

        self._determine_opponents()

        self.baseline_policy_name = baseline_policy_name

        self.agent_infos = {'num_agents': self.num_ops,
                            'opponent_timesteps': [0] * self.num_ops,
                            'deactivated': [False] * self.num_ops,
                            'opponent_policies': self.opponent_policies}

        # This is already called in the parent class earlier, but calling this here ate the end again allwos overriding settings that get changed only for this
        # sub-class
        self._handle_config_override(override)

        self.main_agent_id = None

        # self.config['callbacks'] = make_pbt_callback(self.main_id, self.main_policy, self.opponent_policies, self.op_iteration_factor, 1)

    def train(self):
        print(f"Saving output to {self.log_dir}")

        use_local_critic = False  # Only applies to MADDPG agent.

        # Add baseline policy to eval agents
        if self.baseline_artifact is not None:
            file = self.artifact_manager.get_remote_checkpoint(self.baseline_artifact)

            baseline_weights = load_saved_weights(file, self.scenario_name, self.baseline_policy_name, self.trainer_cls)
            self.eval_agents['baseline'] = baseline_weights

        action_spaces, observation_spaces, agent_ids = spaces_from_env(self.env)

        self.main_agent_id = agent_ids[self.main_id]
        policies = {}

        for policy in [self.main_policy] + self.opponent_policies + ['eval_op']:
            agent_id = agent_ids[self.main_id] if policy == self.main_policy else agent_ids[self.opponent_id]
            obs_space = observation_spaces[agent_id]
            act_space = action_spaces[agent_id]
            policies[policy] = (
                None,
                obs_space,
                act_space,
                {
                    'agent_id': agent_id,
                    'use_local_critic': use_local_critic
                }
            )
        # Apparently fields don't work but variables do in this closure. Values are constant anyway, so it doesn't matter
        main_agent_id = self.main_agent_id
        main_policy = self.main_policy

        def policy_mapping_fn_eval(agent_id, episode, **kwargs):
            if agent_id == main_agent_id:
                return main_policy
            else:  # The agent that is controlled by one of the opponent agents
                return 'eval_op'

        multiagent = {'policies': policies,
                      'policy_mapping_fn': None,
                      # I'm not 100% positive but it seems like only policies which were declared trainable in this config settings at time of initializing the
                      # trainer can actually be trained. Even if we change this setting later online during training, only a subset of these policies provided
                      # here will be updated. As such it is important that this setting is set correctly and it can't be corrected by chaning this setting
                      # online.
                      'policies_to_train': [main_policy] + self.opponent_policies

                      }

        self.config['env_config']['scenario_name'] = self.scenario_name
        self.config['multiagent'] = multiagent

        # Update eval config
        self.config['evaluation_config'] = {}
        self.config["evaluation_config"]['multiagent'] = deepcopy(multiagent)
        self.config["evaluation_config"]['multiagent']['policy_mapping_fn'] = policy_mapping_fn_eval

        print('Config:')
        print(self.config)

        conf_copy = deepcopy(self.config)
        main_trainer = create_trainer(self.trainer_cls, conf_copy)

        # Set trainer for artifact manager
        self.artifact_manager.trainer = main_trainer

        # Start the actual training
        self._train_loop(main_trainer)

    def _train_loop(self, trainer):
        print(f'Running PBT')

        save_params(self.config, self.log_dir)

        # Convert constant fields into values, otherwise the closures won't work as policy mapping functions
        main_agent_id = self.main_agent_id
        main_policy = self.main_policy
        opponent_policies = copy(self.opponent_policies)  # List of strings -> no deepcopy necessary

        timesteps_main = 0
        iterations = 0
        num_checkpoints_collected = 0
        pbar = tqdm(total=self.max_timesteps)
        while timesteps_main < self.max_timesteps:

            # ===== OPPONENT TRAINING =====

            trainer.workers.sync_weights()
            # test_weights = deepcopy(trainer.get_weights('main')['main'])
            # Iterate over all opponents and train each separately
            for op in self.opponent_policies:
                trainer.workers.foreach_worker(lambda worker: worker.set_policies_to_train([op]))

                # This policy mapping function deterministically always lets the main agent play against the current specific op
                def policy_mapping_train_op(agent_id, episode, **kwargs):
                    """Policy mapping function that deterministically chooses based on worker id and episode id"""
                    if agent_id == main_agent_id:
                        return main_policy
                    else:  # The agent that is controlled by one of the opponent agents
                        return op

                trainer.workers.foreach_worker(lambda worker: worker.set_policy_mapping_fn(policy_mapping_train_op))

                for _ in range(self.op_iteration_factor):  # This factor will cause opponents to be trained multiple times
                    results = trainer.train()

            # ===== MAIN TRAINING =====
            # This policy mapping function chooses random opponents
            def policy_mapping_train_main(agent_id, episode, **kwargs):
                """Policy mapping function that deterministically chooses based on worker id and episode id"""
                if agent_id == main_agent_id:
                    return main_policy
                else:  # The agent that is controlled by one of the opponent agents
                    op_policy = random.choice(opponent_policies)
                    return op_policy

            # Enable training of main agent
            trainer.workers.foreach_worker(lambda worker: worker.set_policies_to_train([main_policy]))
            # Set the policy mapping function for training the main agent
            trainer.workers.foreach_worker(lambda worker: worker.set_policy_mapping_fn(policy_mapping_train_main))

            trainer.workers.sync_weights()

            # assert np.array_equal(test_weights, trainer.get_weights('main')['main'])

            # Train main agent for only one iteration
            results = trainer.train()
            timesteps_total = results['timesteps_total']
            timesteps_main = int(timesteps_total / (self.op_iteration_factor + 1))

            if self.main_policy not in results['policy_reward_mean']:  # Sanity check
                raise ValueError(f"No result values collected. Is 'horizon' set?")
            else:
                # Log results for iteration
                for policy, value in results['policy_reward_mean'].items():
                    wandb.log({policy: value,
                               'timestep_main': timesteps_main,
                               'timestep': timesteps_total})  # Logging multiple x-axes

                custom_eval_log(results, timesteps_total, timesteps_main)

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * self.checkpoint_freq
            if timesteps_total > next_checkpoint_at:

                self.artifact_manager.save_new_checkpoint()

                num_checkpoints_collected += 1

                next_gen = []
                # Update stored old opponents
                for op_policy in self.opponent_policies:
                    weights = trainer.get_weights(op_policy)
                    weights_copy = deepcopy(weights)
                    next_gen.append(weights_copy)
                # Append the list of the next generation of opponents
                self.eval_agents['gen'].append(next_gen)

            iterations += 1
            pbar.update(timesteps_main - pbar.n)
        pbar.close()

        self.artifact_manager.save_new_checkpoint()
        print(f"Saved final checkpoint")

    def _determine_opponents(self):
        shuffle(self.opponent_policies)
        self.chosen_ops = self.opponent_policies[:self.num_ops_per_iteration]

    def _get_use_local_critic(self):
        raise NotImplementedError()  # Not needed

    def _get_policies_to_train(self):
        raise NotImplementedError()  # Not needed
