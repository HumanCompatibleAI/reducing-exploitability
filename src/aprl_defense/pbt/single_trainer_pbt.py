import random
from copy import deepcopy
from pathlib import Path
from random import shuffle
from typing import Optional, Callable, List

import wandb
from ray.tune import Trainable
from tqdm import tqdm

import aprl_defense.configs.train
import aprl_defense.configs.eval
from aprl_defense.common.artifact_manager import ArtifactManager
from aprl_defense.common.rllib_io import save_params
from aprl_defense.common.train import load_saved_weights
from aprl_defense.common.utils import create_trainer, spaces_from_env, policies_equal
from aprl_defense.pbt.utils import custom_eval_log, create_pbt_eval_func_separate
from aprl_defense.training_manager import TrainingManager


class SingleTrainerPBTManager(TrainingManager):
    def __init__(self, env,
                 log_dir: Path,
                 alg: str,
                 max_timesteps: int,
                 checkpoint_freq: int,
                 num_workers: Optional[int],
                 override: str,
                 artifact_manager: ArtifactManager,
                 num_sgd_iter: Optional[int],
                 train_batch_size: int,
                 main_id: int,
                 num_ops: int = 10,
                 op_experience_factor: float = 1.,
                 baseline_artifacts=None,
                 specific_folder=False,
                 baseline_policy_name='policy_1',
                 new_op_interval=-1):
        super().__init__(env, log_dir, alg, max_timesteps, checkpoint_freq, num_workers, override, artifact_manager, num_sgd_iter, train_batch_size)

        self.main_id = main_id
        self.opponent_id = 1 - main_id

        self.num_ops = num_ops
        # self.main_iter_steps = main_steps

        self.baseline_artifacts = baseline_artifacts
        self.specific_folder = specific_folder

        # Calculate what is the factor of iterations that have to be performed more for opponents compared to the main agent
        # Iterations scale with the number of opponents. If op_expericence_factor os -1, set to imbalanced mode where all opponents have together get as many
        # timesteps as the main agent alone
        self.op_iteration_factor = op_experience_factor * self.num_ops if op_experience_factor > 0 else 1

        self.eval_agents = {
            'gen': []
        }

        # Update config with settings that are specific to PBT
        self.config.update(aprl_defense.configs.train.pbt)

        # self.config['train_batch_size'] = self.config['num_workers'] * 25 * episodes_per_worker

        # Config for catching up does not need eval
        self.catch_up_config = deepcopy(self.config)

        self.config.update(aprl_defense.configs.eval.online_eval)

        # Apply conditional eval settings
        self.config.update({
            "custom_eval_function": create_pbt_eval_func_separate(self.eval_agents, self.main_id),

            # Set evaluation interval to enable evaluation
            # The interval is with respect to training itereations, i.e. calls of trainer.train()
            # Currently the `checkpoint_freq` also determines the evaluation interval
            # Consequently, num_ops + 1 intervals equal 1 training iteration for main, i.e. main trains batch_size steps
            "evaluation_interval": (self.num_ops + 1) * min(self.checkpoint_freq // self.config['train_batch_size'], 1),  # self.checkpoint_freq // 25,
        })
        self.main_policy = 'main'
        self.opponent_policies = [f'op_{i}' for i in range(self.num_ops)]

        op_fraction = 1
        self.num_ops_per_iteration = int(len(self.opponent_policies) * op_fraction)
        if self.num_ops_per_iteration <= 0:  # Always train at least one op
            self.num_ops_per_iteration = 1

        self._determine_opponents()

        self.baseline_policy_name = baseline_policy_name

        self.agent_infos = {'num_agents': self.num_ops,
                            'opponent_timesteps': [0] * self.num_ops,
                            'deactivated': [False] * self.num_ops,
                            'opponent_policies': self.opponent_policies}

        # This is already called in the parent class earlier, but calling this here ate the end again allwos overriding settings that get changed only for this
        # sub-class
        self._handle_config_override(override)

        self.main_agent_id = None
        self.op_agent_id = None

        self.new_op_interval = new_op_interval

        # Initialize this trainer later
        self.trainer: Optional[Trainable] = None

        # self.config['callbacks'] = make_pbt_callback(self.main_id, self.main_policy, self.opponent_policies, self.op_iteration_factor, 1)

    def train(self):
        print(f"Saving output to {self.log_dir}")

        use_local_critic = False  # Only applies to MADDPG agent.

        # Add baseline policy to eval agents
        if self.baseline_artifacts is not None:
            self.eval_agents['baselines'] = [[]]
            for baseline_artifact in self.baseline_artifacts:
                file = self.artifact_manager.get_remote_checkpoint(baseline_artifact)

                baseline_weights = load_saved_weights(file, self.scenario_name, self.baseline_policy_name, self.trainer_cls)
                self.eval_agents['baselines'][0].append(baseline_weights)

        action_spaces, observation_spaces, agent_ids = spaces_from_env(self.env)

        self.main_agent_id = agent_ids[self.main_id]
        self.op_agent_id = agent_ids[self.opponent_id]
        policies = {}

        for policy in [self.main_policy] + self.opponent_policies + ['eval_op']:
            agent_id = self.main_agent_id if policy == self.main_policy else self.op_agent_id

            obs_space = observation_spaces[agent_id]
            act_space = action_spaces[agent_id]
            policies[policy] = (
                None,
                obs_space,
                act_space,
                {
                    'agent_id': agent_id,
                    'use_local_critic': use_local_critic,
                }
            )
        # Apparently fields don't work but variables do in this closure. Values are constant anyway, so it doesn't matter
        main_agent_id = self.main_agent_id
        main_policy = self.main_policy

        def policy_mapping_fn_eval(agent_id, episode, **kwargs):
            if agent_id == main_agent_id:
                return main_policy
            else:  # The agent that is controlled by one of the opponent agents
                return 'eval_op'

        multiagent = {'policies': policies,
                      'policy_map_capacity': 200,
                      'policy_mapping_fn': None,
                      # I'm not 100% positive but it seems like only policies which were declared trainable in this config settings at time of initializing the
                      # trainer can actually be trained. Even if we change this setting later online during training, only a subset of these policies provided
                      # here will be updated. As such it is important that this setting is set correctly and it can't be corrected by chaning this setting
                      # online.
                      'policies_to_train': [main_policy] + self.opponent_policies

                      }

        self.config['env_config']['scenario_name'] = self.scenario_name
        self.config['multiagent'] = multiagent

        # Update eval config
        self.config['evaluation_config'] = {}
        self.config["evaluation_config"]['multiagent'] = deepcopy(multiagent)
        self.config["evaluation_config"]['multiagent']['policy_mapping_fn'] = policy_mapping_fn_eval
        self.config['evaluation_config']['rollout_fragment_length'] = 25

        # print('Config:')
        # print(self.config)

        conf_copy = deepcopy(self.config)
        self.trainer = create_trainer(self.trainer_cls, conf_copy)

        # Set trainer for artifact manager
        self.artifact_manager.trainer = self.trainer

        # Start the actual training
        self._train_loop(None)

    def _train_loop(self, _):
        print(f'Running PBT')

        save_params(self.config, self.log_dir)

        # Convert constant fields into values, otherwise the closures won't work as policy mapping functions
        main_agent_id = self.main_agent_id
        main_policy = self.main_policy

        # TODO: does everything work correctly here?
        opponent_policies = self.opponent_policies  # copy necessary? List of strings -> no deepcopy necessary

        # This policy mapping function chooses random opponents
        def policy_map_main_vs_random_op(agent_id, episode, **kwargs):
            """Policy mapping function that deterministically chooses based on worker id and episode id"""
            if agent_id == main_agent_id:
                return main_policy
            else:  # The agent that is controlled by one of the opponent agents
                op_policy = random.choice(opponent_policies)
                return op_policy

        timesteps_main = 0
        timesteps_total = 0
        iterations = 0
        num_checkpoints_collected = 0
        num_ops_added = 0
        pbar = tqdm(total=self.max_timesteps)
        while timesteps_main < self.max_timesteps:

            # ===== OPPONENT TRAINING =====
            self._add_new_policy_if_necessary(num_ops_added, timesteps_main)

            self.trainer.workers.sync_weights()

            # Train a random opponent for enough timesteps that there are on average as many timesteps as the main agent trains for each opponent
            _ = self._setup_and_train(self.opponent_policies, policy_map_main_vs_random_op, iterations=self.op_iteration_factor)

            # ===== MAIN TRAINING =====y
            results = self._setup_and_train([main_policy], policy_map_main_vs_random_op)

            timesteps_total_new = results['timesteps_total']

            total_additional = timesteps_total_new - timesteps_total  # Number of additional timesteps in this iteration
            timesteps_total = timesteps_total_new

            # Calculate additional timesteps for main agent from total additional timesteps
            main_additional = int(total_additional / (self.op_iteration_factor * self.num_ops + 1))

            # timesteps_main += main_additional
            timesteps_main = timesteps_total / 2

            if self.main_policy not in results['policy_reward_mean']:  # Sanity check
                raise ValueError(f"No result values collected. Is 'horizon' set?")
            else:
                # Log results for iteration
                for policy, value in results['policy_reward_mean'].items():
                    wandb.log({policy: value,
                               'timestep': timesteps_main,
                               'timestep_agg': timesteps_total})  # Logging multiple x-axes

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * self.checkpoint_freq
            if timesteps_main > next_checkpoint_at:

                self.artifact_manager.save_new_checkpoint()

                num_checkpoints_collected += 1

                next_gen = []
                # Update stored old opponents
                for op_policy in self.opponent_policies:
                    weights = self.trainer.get_weights(op_policy)
                    weights_copy = deepcopy(weights)
                    next_gen.append(weights_copy)
                # Append the list of the next generation of opponents
                self.eval_agents['gen'].append(next_gen)

                custom_eval_log(results, timesteps_total, timesteps_main)

            iterations += 1
            pbar.update(timesteps_main - pbar.n)
        pbar.close()

        self.artifact_manager.save_new_checkpoint()
        print(f"Saved final checkpoint")

    def _setup_and_train(self, policies_to_train: List[str], policy_map_fn: Callable, iterations: int = 1):
        """Setup for training and perform training with given number of iterations. Setup consists of setting which policy to train, setting given
        policy_map_fn, syncing the weights. """
        # Setup
        # Enable training of given policy
        self.trainer.workers.foreach_worker(lambda worker: worker.set_policies_to_train(policies_to_train))
        # Set the given policy mapping function
        self.trainer.workers.foreach_worker(lambda worker: worker.set_policy_mapping_fn(policy_map_fn))
        # Sync before training in case there were changes before
        self.trainer.workers.sync_weights()

        if iterations < 1:
            raise ValueError(f'Train for at least 1 iterations, iterations was set to {iterations}')

        # Train main agent for given number of iterations
        for _ in range(iterations):
            results = self.trainer.train()

        return results

    def _add_new_policy_if_necessary(self, num_ops_added, timesteps_main):
        """Add new opponent policy if the interval has been reached. If self.new_op_interval is set to -1 never add new policies."""

        if self.new_op_interval > -1 and timesteps_main > (num_ops_added + 1) * self.new_op_interval:
            # If I understand correctly, when adding a new policy for non-standard environments RLlib can't automatically determine obs and action space
            # This is why we manually determin and apply them here
            action_spaces, observation_spaces, agent_ids = spaces_from_env(self.env)
            new_pol_id = f'op_{self.num_ops}'
            self.num_ops += 1
            num_ops_added += 1

            # This policy is for a new opponent, so spaces will be determined by opponent_id
            new_policy = self.trainer.add_policy(
                policy_id=new_pol_id,
                policy_cls=type(self.trainer.get_policy(self.opponent_policies[-1])),
                observation_space=observation_spaces[self.op_agent_id],
                action_space=action_spaces[self.op_agent_id]
                # policy_mapping_fn=policy_mapping_fn,  # Not necessary as function will be changed further down anyway
                # policies_to_train=self.trainable_policies,  # This, too, is changed later anyway
            )

            # Track the new policy as one of the active policies
            self.opponent_policies.append(new_pol_id)

            # Syncing not needed, as syncing is done anyway before training is started (due to likely changes in policies after training)
            # main_state = trainer.get_policy(policy_id).get_state()
            # new_policy.set_state(main_state)
            # We need to sync the just copied local weights to all the
            # remote workers as well.
            # trainer.workers.sync_weights(policies=[new_pol_id])

    def _determine_opponents(self):
        shuffle(self.opponent_policies)
        self.chosen_ops = self.opponent_policies[:self.num_ops_per_iteration]

    def _get_use_local_critic(self):
        raise NotImplementedError()  # Not needed

    def _get_policies_to_train(self):
        raise NotImplementedError()  # Not needed
