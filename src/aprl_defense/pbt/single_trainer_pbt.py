import random
from copy import deepcopy, copy
from pathlib import Path
from random import shuffle
from typing import List

import numpy as np
import ray
import wandb
from ray import cloudpickle
from ray.rllib.agents import DefaultCallbacks
from ray.tune.logger import pretty_print
from tqdm import tqdm

import aprl_defense.configs.train
from aprl_defense.common.io import get_checkpoint_file
from aprl_defense.common.train import run_trainer, load_saved_weights
from aprl_defense.common.utils import create_trainer
from aprl_defense.pbt.fixed_iter_pbt import _get_agent_weights
from aprl_defense.pbt.utils import create_pbt_eval_func, custom_eval_log, create_pbt_eval_func_separate
from aprl_defense.pbt_scheduler import random_policy_scheduler
from aprl_defense.training_manager import TrainingManager, create_ma_trainer


# This callback does the same as the code surrounding the trainer. It could be used if there would be a need to transfer this to ray.tune()

# def make_pbt_callback(main_agent_id: int, main_policy: str, op_policies: List[str], op_iteration_factor: int, op_fraction: float):
#     class PBTCallback(DefaultCallbacks):
#         def __init__(self):  # , main_policy, op_policies):
#             super().__init__()
#
#             # Fill fields from captured vars
#             self.main_agent_id = main_agent_id
#             self.main_policy = main_policy
#             self.op_policies = op_policies
#             self.op_iteration_factor = op_iteration_factor
#
#             self.num_ops_per_iteration = int(len(self.op_policies) * op_fraction)
#             if self.num_ops_per_iteration <= 0:  # Always train at least one op
#                 self.num_ops_per_iteration = 1
#
#             self._determine_opponents()
#
#             # This field is used to ensure alternating between training main and ops.
#             # We initialize it to true such that training starts out with ops
#             self.opponent_iterations_left = self.op_iteration_factor
#
#         def on_train_result(self, *, trainer, result, **kwargs):
#             if self.opponent_iterations_left > 0:
#                 if self.opponent_iterations_left == self.op_iteration_factor:
#                     # All iterations still left -> previous iteration was train main and now is first op train since then -> determine new ops
#                     self._determine_opponents()
#                 # Previously trained main => next iteration opponents will be trained
#                 policies_to_train = self.op_policies
#                 self.opponent_iterations_left -= 1
#             else:
#                 policies_to_train = [self.main_policy]
#                 self.opponent_iterations_left = self.op_iteration_factor
#
#             def policy_mapping_fn(agent_id, episode, **kwargs):
#                 """Policy mapping function that deterministically chooses based on worker id and episode id"""
#                 if agent_id == self.main_agent_id:
#                     return self.main_policy
#                 else:  # The agent that is controlled by one of the opponent agents
#                     op_idx = random.randint(0, self.num_ops_per_iteration - 1)
#                     return self.chosen_ops[op_idx]
#
#             def _set(worker):  # Function for syncing new settings
#                 worker.set_policy_mapping_fn(policy_mapping_fn)
#                 worker.set_policies_to_train(policies_to_train)
#
#             trainer.workers.foreach_worker(_set)
#
#         def _determine_opponents(self):
#             shuffle(self.op_policies)
#             self.chosen_ops = self.op_policies[:self.num_ops_per_iteration]
#
#     return PBTCallback


class SingleTrainerPBTManager(TrainingManager):
    def __init__(self, env,
                 log_dir: Path,
                 alg: str,
                 max_timesteps: int,
                 checkpoint_freq: int,
                 override: str,
                 main_id: int,
                 scheduler_func,
                 main_steps: int = 25,
                 num_ops: int = 10,
                 op_experience_factor: int = 1,
                 baseline_checkpoint=None,
                 specific_folder=False,
                 baseline_policy_name='policy_1',
                 num_workers=None,
                 episodes_per_worker=10):
        super().__init__(env, log_dir, alg, max_timesteps, checkpoint_freq, override)

        self.main_id = main_id
        self.opponent_id = 1 - main_id

        self.num_ops = num_ops
        self.main_iter_steps = main_steps

        self.baseline_checkpoint = baseline_checkpoint
        self.specific_folder = specific_folder

        # Calculate what is the factor of iterations that have to be performed more for opponents compared to the main agent
        self.op_iteration_factor = op_experience_factor

        self.eval_agents = {
            'gen': []
        }

        # Update config with settings that are specific to PBT
        self.config.update(aprl_defense.configs.train.pbt)

        # Override PBT settings if given
        if num_workers is not None:
            self.config['num_workers'] = num_workers

        self.config['train_batch_size'] = self.config['num_workers'] * 25 * episodes_per_worker

        # Config for catching up does not need eval
        self.catch_up_config = deepcopy(self.config)

        self.config.update(aprl_defense.configs.eval.online_eval)

        # Apply conditional eval settings
        self.config.update({
            "custom_eval_function": create_pbt_eval_func_separate(self.eval_agents, self.main_id),

            # Enable evaluation
            "evaluation_interval": 16,  # self.checkpoint_freq // 25,
        })
        self.main_policy = 'main'
        self.opponent_policies = [f'op_{i}' for i in range(self.num_ops)]

        op_fraction = 1
        self.num_ops_per_iteration = int(len(self.opponent_policies) * op_fraction)
        if self.num_ops_per_iteration <= 0:  # Always train at least one op
            self.num_ops_per_iteration = 1

        self._determine_opponents()

        self.baseline_policy_name = baseline_policy_name

        self.agent_infos = {'num_agents': self.num_ops,
                            'opponent_timesteps': [0] * self.num_ops,
                            'deactivated': [False] * self.num_ops,
                            'opponent_policies': self.opponent_policies}

        # self.config['callbacks'] = make_pbt_callback(self.main_id, self.main_policy, self.opponent_policies, self.op_iteration_factor, 1)

    def train(self):
        print(f"Saving output to {self.log_dir}")

        use_local_critic = False  # Only applies to MADDPG agent.

        # Add baseline policy to eval agents
        if self.baseline_checkpoint is not None:
            file = get_checkpoint_file(self.baseline_checkpoint, specific_folder=self.specific_folder)

            baseline_weights = load_saved_weights(Path(file), self.scenario_name, self.baseline_policy_name, self.trainer_cls)
            self.eval_agents['baseline'] = baseline_weights

        policies = {}

        for policy in [self.main_policy] + self.opponent_policies + ['eval_op']:
            agent_id = self.main_id if policy == self.main_policy else self.opponent_id
            obs_space = self.env.observation_space_dict[agent_id]
            act_space = self.env.action_space_dict[agent_id]
            policies[policy] = (
                None,
                obs_space,
                act_space,
                {
                    'agent_id': agent_id,
                    'use_local_critic': use_local_critic
                }
            )
        # Apparently fields don't work but variables do in this closure. Values are constant anyway, so it doesn't matter
        main_id = self.main_id
        main_policy = self.main_policy

        def policy_mapping_fn_eval(agent_id, episode, **kwargs):
            if agent_id == main_id:
                return main_policy
            else:  # The agent that is controlled by one of the opponent agents
                return 'eval_op'

        multiagent = {'policies': policies,
                      'policy_mapping_fn': None,
                      # I'm not 100% positive but it seems like only policies which were declared trainable in this config settings at time of initializing the
                      # trainer can actually be trained. Even if we change this setting later online during training, only a subset of these policies provided
                      # here will be updated. As such it is important that this setting is set correctly and it can't be corrected by chaning this setting
                      # online.
                      'policies_to_train': [main_policy] + self.opponent_policies

                      }

        self.config['env_config']['scenario_name'] = self.scenario_name
        self.config['multiagent'] = multiagent

        # Update eval config
        self.config['evaluation_config'] = {}
        self.config["evaluation_config"]['multiagent'] = deepcopy(multiagent)
        self.config["evaluation_config"]['multiagent']['policy_mapping_fn'] = policy_mapping_fn_eval

        conf_copy = deepcopy(self.config)
        main_trainer = create_trainer(self.trainer_cls, conf_copy)

        print('Config:')
        print(pretty_print(main_trainer.config))

        # Start the actual training
        self._train_loop(main_trainer)

    def _train_loop(self, trainer):
        print(f'Running PBT')

        checkpoint_dir = self.log_dir / 'checkpoints'
        checkpoint_dir.mkdir(exist_ok=True, parents=True)

        # Save params file
        with (checkpoint_dir / 'params.pkl').open(mode='wb') as file:
            cloudpickle.dump(self.config, file=file)

        # Convert constant fields into values, otherwise the closures won't work as policy mapping functions
        main_id = self.main_id
        main_policy = self.main_policy
        opponent_policies = copy(self.opponent_policies)  # List of strings -> no deepcopy necessary

        timesteps_main = 0
        iterations = 0
        num_checkpoints_collected = 0
        pbar = tqdm(total=self.max_timesteps)
        while timesteps_main < self.max_timesteps:

            # ===== OPPONENT TRAINING =====

            trainer.workers.sync_weights()
            # test_weights = deepcopy(trainer.get_weights('main')['main'])
            # Iterate over all opponents and train each separately
            for op in self.opponent_policies:
                trainer.workers.foreach_worker(lambda worker: worker.set_policies_to_train([op]))

                # This policy mapping function deterministically always lets the main agent play against the current specific op
                def policy_mapping_train_op(agent_id, episode, **kwargs):
                    """Policy mapping function that deterministically chooses based on worker id and episode id"""
                    if agent_id == main_id:
                        return main_policy
                    else:  # The agent that is controlled by one of the opponent agents
                        return op

                trainer.workers.foreach_worker(lambda worker: worker.set_policy_mapping_fn(policy_mapping_train_op))

                for _ in range(self.op_iteration_factor):  # This factor will cause opponents to be trained multiple times
                    results = trainer.train()

            # ===== MAIN TRAINING =====
            # This policy mapping function chooses random opponents
            def policy_mapping_train_main(agent_id, episode, **kwargs):
                """Policy mapping function that deterministically chooses based on worker id and episode id"""
                if agent_id == main_id:
                    return main_policy
                else:  # The agent that is controlled by one of the opponent agents
                    op_policy = random.choice(opponent_policies)
                    return op_policy

            # Enable training of main agent
            trainer.workers.foreach_worker(lambda worker: worker.set_policies_to_train([main_policy]))
            # Set the policy mapping function for training the main agent
            trainer.workers.foreach_worker(lambda worker: worker.set_policy_mapping_fn(policy_mapping_train_main))

            trainer.workers.sync_weights()

            # assert np.array_equal(test_weights, trainer.get_weights('main')['main'])

            # Train main agent for only one iteration
            results = trainer.train()
            timesteps_total = results['timesteps_total']
            timesteps_main = int(timesteps_total / (self.op_iteration_factor + 1))

            if self.main_policy not in results['policy_reward_mean']:  # Sanity check
                raise ValueError(f"No result values collected. Is 'horizon' set?")
            else:
                # Log results for iteration
                for policy, value in results['policy_reward_mean'].items():
                    wandb.log({policy: value,
                               'timestep_main': timesteps_main,
                               'timestep': timesteps_total})  # Logging multiple x-axes

                custom_eval_log(results, timesteps_total, timesteps_main)

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * self.checkpoint_freq
            if timesteps_total > next_checkpoint_at:
                trainer.save(checkpoint_dir=checkpoint_dir)
                num_checkpoints_collected += 1

                next_gen = []
                # Update stored old opponents
                for op_policy in self.opponent_policies:
                    weights = trainer.get_weights(op_policy)
                    weights_copy = deepcopy(weights)
                    next_gen.append(weights_copy)
                # Append the list of the next generation of opponents
                self.eval_agents['gen'].append(next_gen)

            iterations += 1
            pbar.update(timesteps_main - pbar.n)
        pbar.close()

    def _determine_opponents(self):
        shuffle(self.opponent_policies)
        self.chosen_ops = self.opponent_policies[:self.num_ops_per_iteration]

    def _get_use_local_critic(self):
        raise NotImplementedError()  # Not needed

    def _get_policies_to_train(self):
        raise NotImplementedError()  # Not needed
