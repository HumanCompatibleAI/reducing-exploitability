import math

import ray
import wandb
from ray.rllib.agents import Trainer
from ray.rllib.evaluation import collect_metrics
from ray.rllib.evaluation.metrics import collect_episodes, summarize_episodes
from ray.rllib.evaluation.worker_set import WorkerSet

def create_pbt_eval_func_separate(eval_agents: dict, main_id):
    main_name = f'main'
    secondary_name = f'eval_op'

    # Define the function
    def pbt_eval_function(trainer: Trainer, eval_workers: WorkerSet):
        """Custom evaluation function for PBT.
        Args:
            trainer (Trainer): trainer class to evaluate.
            eval_workers (WorkerSet): evaluation workers.
        Returns:
            metrics (dict): evaluation metrics dict.
        """

        remote_workers = eval_workers.remote_workers()
        # if any([agent is None for agent in eval_agents['current']]):
        #     # As long as not all agents have been trained against at least once, we don't start evaluation yet
        #     metrics = collect_metrics(eval_workers.local_worker(),
        #                               remote_workers)
        #     return metrics  # We return an essentially empty metrics dict (contains a bunch of nans)

        my_eval_line_plot = {}
        metrics = {}

        for key, val in eval_agents.items():
            # Val is a list where each index corresponds to an op id
            # For every op id we either have the weights, or a list of historical weights
            if isinstance(val, list):
                if len(val) > 0:
                    # If we have a list of historical generations we iterate through these
                    for i, generation in enumerate(val):
                        num_opponents = len(generation)
                        metrics = _evaluate_metrics_helper_separate(num_opponents, remote_workers, trainer, generation)

                        my_eval_line_plot[f'eval_{key}_{i}'] = metrics['policy_reward_mean'][main_name]
            else:
                num_opponents = len(val)
                metrics = _evaluate_metrics_helper_separate(num_opponents, remote_workers, trainer, [val])

                my_eval_line_plot[f'eval_{key}'] = metrics['policy_reward_mean'][main_name]

        # TODO: Should we also log separate metrics for each opponent_id?

        metrics['my_eval_line_plot'] = my_eval_line_plot
        return metrics

    def _evaluate_metrics_helper_separate(num_opponents, remote_workers, trainer, weights_list: list):
        """Helper function that runs eval workers for all agents that can be received from the function.
        weight_func is lambda that receives the opponent index and returns the opponent weights that are supposed to be evaluated."""
        num_workers = len(remote_workers)

        # If num_workers < num_agents we need to do multiple passes of eval
        num_eval_passes = math.ceil(num_opponents / num_workers)

        all_eps = []
        for pass_i in range(num_eval_passes):
            opponent_i = pass_i * num_workers
            # Number of workers to use this pass, either num_workers or the number of agents that are left
            num_workers_this_pass = num_workers if opponent_i + num_workers <= num_opponents else num_opponents - opponent_i

            worker_slice = remote_workers[:num_workers_this_pass]  # Iterate over first n_w_t_p workers only

            # Set the weights for the workers
            for worker in worker_slice:
                if opponent_i < num_opponents:
                    weights = list(weights_list[opponent_i].values())[0]
                    worker.set_weights.remote(({secondary_name: weights}))
                    opponent_i += 1

            # Run the eval episodes
            for _ in range(trainer.config['evaluation_num_episodes']):
                # Calling .sample() runs exactly one episode per worker due to how the
                # eval workers are configured.
                ray.get([w.sample.remote() for w in worker_slice])

            # Collect the accumulated episodes on the workers
            episodes, _ = collect_episodes(remote_workers=worker_slice, timeout_seconds=99999)  # TODO timeout? what's up with that

            all_eps += episodes
        # All eps collected for this set of agents

        # Summarize the episode stats into a metrics dict
        # You can compute metrics from the episodes manually, or use the
        # convenient `summarize_episodes()` utility:
        metrics = summarize_episodes(all_eps)
        # Note that the above two statements are the equivalent of:
        # metrics = collect_metrics(None, worker_slice)

        # Some sanity checks
        assert metrics['episodes_this_iter'] == trainer.config['evaluation_num_episodes'] * num_opponents
        return metrics

    # Return it
    return pbt_eval_function



def create_pbt_eval_func(stored_agents: dict, main_id):
    sec_id = 1 - main_id
    main_name = f'policy_{main_id}'
    secondary_name = f'policy_{sec_id}'

    # Define the function
    def pbt_eval_function(trainer: Trainer, eval_workers: WorkerSet):
        """Custom evaluation function for PBT.
        Args:
            trainer (Trainer): trainer class to evaluate.
            eval_workers (WorkerSet): evaluation workers.
        Returns:
            metrics (dict): evaluation metrics dict.
        """

        remote_workers = eval_workers.remote_workers()
        if any([agent is None for agent in stored_agents['current']]):
            # As long as not all agents have been trained against at least once, we don't start evaluation yet
            metrics = collect_metrics(eval_workers.local_worker(),
                                      remote_workers)
            return metrics  # We return an essentially empty metrics dict (contains a bunch of nans)

        my_eval_line_plot = {}
        metrics = {}

        for key, val in stored_agents.items():
            num_opponents = len(val)
            # Val is a list where each index corresponds to an op id
            # For every op id we either have the weights, or a list of historical weights
            if isinstance(val[0], list):
                # If we have a list of historical weights we iterate through these
                for old_i in range(len(stored_agents[key][0])):
                    # This lambda returns the weights of opponent with given index for the old_ith generation of saved weights
                    old_weights = lambda op_i: stored_agents[key][op_i][old_i]

                    metrics = _evaluate_metrics_helper(num_opponents, remote_workers, trainer, old_weights)

                    my_eval_line_plot[f'eval_{key}_{old_i}'] = metrics['policy_reward_mean'][main_name]
            else:
                # This means we simply have a list with weights for every op

                # Simply return the agent with the given opponent id
                current_weights = lambda op_i: stored_agents[key][op_i]

                metrics = _evaluate_metrics_helper(num_opponents, remote_workers, trainer, current_weights)

                my_eval_line_plot[f'eval_{key}'] = metrics['policy_reward_mean'][main_name]

        # TODO: Should we also log separate metrics for each opponent_id?

        metrics['my_eval_line_plot'] = my_eval_line_plot
        return metrics

    def _evaluate_metrics_helper(num_opponents, remote_workers, trainer, weight_func):
        """Helper function that runs eval workers for all agents that can be received from the function.
        weight_func is lambda that receives the opponent index and returns the opponent weights that are supposed to be evaluated."""
        num_workers = len(remote_workers)

        # If num_workers < num_agents we need to do multiple passes of eval
        num_eval_passes = math.ceil(num_opponents / num_workers)

        all_eps = []
        for pass_i in range(num_eval_passes):
            opponent_i = pass_i * num_workers
            # Number of workers to use this pass, either num_workers or the number of agents that are left
            num_workers_this_pass = num_workers if opponent_i + num_workers <= num_opponents else num_opponents - opponent_i

            worker_slice = remote_workers[:num_workers_this_pass]  # Iterate over first n_w_t_p workers only

            # Set the weights for the workers
            for worker in worker_slice:
                weights = weight_func(opponent_i)
                # weights = stored_agents['old'][agent_i][old_i]
                worker.set_weights.remote(({secondary_name: weights}))
                opponent_i += 1

            # Run the eval episodes
            for _ in range(trainer.config['evaluation_num_episodes']):
                # Calling .sample() runs exactly one episode per worker due to how the
                # eval workers are configured.
                ray.get([w.sample.remote() for w in worker_slice])

            # Collect the accumulated episodes on the workers
            episodes, _ = collect_episodes(remote_workers=worker_slice, timeout_seconds=99999)  # TODO timeout? what's up with that

            all_eps += episodes
        # All eps collected for this set of agents

        # Summarize the episode stats into a metrics dict
        # You can compute metrics from the episodes manually, or use the
        # convenient `summarize_episodes()` utility:
        metrics = summarize_episodes(all_eps)
        # Note that the above two statements are the equivalent of:
        # metrics = collect_metrics(None, worker_slice)

        # Some sanity checks
        assert metrics['episodes_this_iter'] == trainer.config['evaluation_num_episodes'] * num_opponents
        return metrics

    # Return it
    return pbt_eval_function


def custom_eval_log(results, timesteps_total):
    # Log evaluation results if applicable
    if 'evaluation' in results and len(results['evaluation']['policy_reward_mean']) > 0:
        for key, val in results['evaluation']['my_eval_line_plot'].items():
            wandb.log({key: val,
                       'timestep': timesteps_total})
