import math
import random
from copy import deepcopy

import ray
import wandb
from ray.rllib.evaluation.metrics import collect_episodes, summarize_episodes
from tqdm import tqdm
from ray.rllib.agents import Trainer
from ray.rllib.evaluation import collect_metrics
from ray.rllib.evaluation.worker_set import WorkerSet

import aprl_defense.configs.train
import aprl_defense.configs.eval
from ray import cloudpickle as cloudpickle
from aprl_defense.common.train import run_trainer
from aprl_defense.training_manager import TrainingManager, create_trainer
from aprl_defense.pbt.fixed_iter_pbt import create_pbt_eval_func

from ray.tune.logger import pretty_print

from pbt_scheduler import random_choice_scheduler, random_policy_scheduler


class FixedIterPBTManager2(TrainingManager):
    def __init__(self, log_dir: str, alg: str, max_timesteps: int, checkpoint_freq: int, override: str, main_id: int, scheduler_func, iter_steps=25000,
                 num_agents=10, evaluation_num_workers=10):
        super().__init__(log_dir, alg, max_timesteps, checkpoint_freq, override)

        self.iter_steps = 25000 if iter_steps is None else iter_steps
        self.main_id = main_id
        self.opponent_id = 1 - self.main_id
        self.scheduler_func = scheduler_func
        self.num_agents = num_agents
        self.iter_steps = iter_steps

        self.stored_agents = {
            'current': [],
            'old': []
        }

        # Update config with settings that are specific to PBT
        self.config.update(aprl_defense.configs.train.pbt)

        self.config.update(aprl_defense.configs.eval.online_eval)

        # Apply conditional eval settings
        self.config.update({
            "custom_eval_function": create_pbt_eval_func(self.stored_agents, self.main_id),

            # Enable evaluation with given frequency
            "evaluation_interval": self.checkpoint_freq // 25,
        })

        self.main_policy = 'main'
        self.opponent_policies = [f'op_{i}' for i in range(self.num_agents)]

        self.agent_infos = {'num_agents': self.num_agents,
                            'opponent_policies': self.opponent_policies}

    def train(self):
        print(f"Saving output to {self.log_dir}")

        use_local_critic = False  # Only applies to MADDPG agent.

        policies_to_train = None  # Trains all

        policies = {}

        for policy in [self.main_policy] + self.opponent_policies:
            agent_id = self.main_id if policy == self.main_policy else self.opponent_id
            obs_space = self.env.observation_space_dict[agent_id]
            act_space = self.env.action_space_dict[agent_id]
            policies[policy] = (
                None,
                obs_space,
                act_space,
                {
                    'agent_id': agent_id,
                    'use_local_critic': use_local_critic
                }
            )
        # The scheduler decides which opponent policy is taken when a new policy is chosen for opponent agent
        scheduler = random_policy_scheduler

        # Using a closure directly doesn't work, but this weird construction does
        def make(main_id, main_policy, agent_infos):
            def policy_mapping_function(agent_id):
                if agent_id == main_id:
                    return main_policy
                else:  # The agent that is controlled by one of the opponent agents
                    # return random_policy_scheduler(self.agent_infos)
                    return scheduler(agent_infos)

            return policy_mapping_function

        # Convert to tune func. TODO: is this necessary?
        ray_map_func = ray.tune.function(make(self.main_id, self.main_policy, self.agent_infos))

        multiagent = {'policies': policies,
                      'policy_mapping_fn': ray_map_func,
                      'policies_to_train': policies_to_train}

        self.config['env_config']['scenario_name'] = self.scenario_name
        self.config['multiagent'] = multiagent
        trainer = self.trainer_cls(env='mpe-push', config=deepcopy(self.config))

        print('Config:')
        print(pretty_print(trainer.config))

        # Start the actual training
        self._train_loop(self.checkpoint_freq, self.max_timesteps, trainer, self.config)

    def _train_loop(self, checkpoint_freq, max_steps, trainer, config):
        print(f'Running PBT')

        checkpoint_dir = self.log_dir / 'checkpoints'
        checkpoint_dir.mkdir(exist_ok=True, parents=True)
        # Save config for the checkpoint
        with (checkpoint_dir / 'params.pkl').open(mode='wb') as file:
            cloudpickle.dump(config, file=file)

        # Initialize PBT training
        # Table that contains infos for each agent, used to determine next opponent.
        # Currently temporary simply represented by a dict
        for i in range(self.num_agents):
            self.stored_agents['current'].append(None)
            self.stored_agents['old'].append([])

        num_checkpoints_collected = 0
        timesteps_total = 0
        pbar = tqdm(total=max_steps)
        while timesteps_total < max_steps:
            # Determine id of secondary agent for this training iterations
            # sec_id = self.scheduler_func(agent_infos)

            # Get stored weights, initialize new ones if necessary
            # agent_weights = _get_agent_weights(sec_id, self.stored_agents, config, secondary_name, self.trainer_cls)
            # Use these weights in the trainer (for secondary agent)
            # trainer.set_weights({secondary_name: agent_weights})

            # TODO: Log which opponent is being played against
            # wandb.log({f'opponent_id': sec_id,
            #            'timestep': timesteps_total + 1})  # The + 1 ensures this timestep does not have duplicate values for op_id

            results = trainer.train()

            timesteps_total = results['timesteps_total']

            # Sanity check
            if self.main_policy not in results['policy_reward_mean']:
                # raise ValueError(f"No result values collected. Is 'horizon' set?")
                pass
            else:
                # Log results for iteration
                for policy, value in results['policy_reward_mean'].items():
                    wandb.log({policy: value,
                               'timestep': timesteps_total})

                # Log evaluation results if applicable
                if 'evaluation' in results and len(results['evaluation']['policy_reward_mean']) > 0:
                    for key, val in results['evaluation']['my_eval_vals'].items():
                        wandb.log({key: val,
                                   'timestep': timesteps_total})

            # Log which opponent was played against
            # wandb.log({f'opponent_id': sec_id,
            #            'timestep': timesteps_total})

            # actual_iter_length = timesteps_total - timesteps_before  # In steps
            # iter_lengths.append(actual_iter_length)
            #
            # running_avg_iter_length = sum(iter_lengths) / len(iter_lengths)
            # wandb.log({f'avg_iter_length': running_avg_iter_length,
            #            'timestep': timesteps_total})

            # Store updated weights
            # new_weights = deepcopy(trainer.get_weights()[secondary_name])
            # self.stored_agents['current'][sec_id] = new_weights

            # Save new checkpoint if applicable
            next_checkpoint_at = (num_checkpoints_collected + 1) * checkpoint_freq
            if timesteps_total > next_checkpoint_at:
                trainer.save(checkpoint_dir=checkpoint_dir)
                num_checkpoints_collected += 1

                # Update old opponents
                for i, policy in enumerate(self.opponent_policies):
                    weights = trainer.get_weights(policy)[policy]
                    weights_copy = deepcopy(weights)
                    self.stored_agents['old'][i].append(weights_copy)

                    # This is temporary for now the current agent is the newest of the old agents.
                    # TODO: fix this so the current agent is actually the current agent
                    self.stored_agents['current'][i] = weights_copy

            pbar.update(timesteps_total - pbar.n)
        pbar.close()

    def _get_use_local_critic(self):
        return [False, False]

    def _get_policies_to_train(self):
        return None  # Trains all


def _get_agent_weights(id, stored_agents, config, sec_name, trainer_cls):
    agent = stored_agents['current'][id]
    if agent is None:
        agent = _init_new_weights(trainer_cls, config)[sec_name]
    return agent


def _init_new_weights(trainer_cls, config):
    # Create a new maddpg trainer which will be used to train the adversarial policy
    trainer = trainer_cls(env='mpe-push', config=config)
    new_weights = deepcopy(trainer.get_weights())
    return new_weights
