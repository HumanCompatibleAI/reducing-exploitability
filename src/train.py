import argparse
import time
from pathlib import Path

import ray

from aprl_defense.common.utils import trainer_map
from aprl_defense.pbt.fixed_iter_pbt import FixedIterPBTManager
from aprl_defense.pbt.fair_pbt import FairPBTManager
from aprl_defense.pbt_scheduler import random_scheduler, random_choice_scheduler
from aprl_defense.training_manager import FinetuneManager, NormalTrainingManager
import wandb

from aprl_defense.pbt.fixed_iter_pbt_v2 import FixedIterPBTManager2
from aprl_defense.pbt.separate_pbt import SeparatePBTManager


def main():
    args = parse_args()
    out_path = args.out_path
    alg = args.alg
    timesteps = args.max_timesteps
    checkpoint_freq = args.checkpoint_freq
    mode = args.mode

    if args.name is None:
        run_name = f'{mode}_{alg}'
    else:
        run_name = args.name

    wandb_mode = 'online'

    if args.disable_log:
        wandb_mode = 'disabled'

    wandb.init(project=args.wandb_project,
               name=run_name,
               config=args,
               mode=wandb_mode,
               group=mode,  # Group by mode
               dir=Path(out_path).resolve(),  # Change wandb dir
               )

    train(mode,
          out_path,
          alg,
          timesteps,
          checkpoint_freq,
          victim_path=args.victim_path,
          cycle_ops=args.cycle_ops,
          num_ops=args.num_ops,
          harden_id=args.harden_id,
          adversary_id=args.adv_id,
          iter_steps=args.iter_steps,
          op_experience_fraction=args.op_experience_fraction,
          override=args.override,
          local_mode=args.ray_local,
          specific_folder=args.specific_folder
          )


def train(mode,
          out_path,
          alg,
          timesteps,
          checkpoint_freq,
          local_mode=False,
          victim_path="",
          cycle_ops=False,
          num_ops=10,
          harden_id=0,
          adversary_id=1,
          iter_steps=None,
          op_experience_fraction=1.,
          override=None,
          specific_folder=False):
    ray.init(local_mode=local_mode)
    if mode == 'normal':
        training_manager = NormalTrainingManager(log_dir=out_path,
                                                 alg=alg,
                                                 max_timesteps=timesteps,
                                                 checkpoint_freq=checkpoint_freq,
                                                 override=override)
    elif mode == 'finetune':
        training_manager = FinetuneManager(log_dir=out_path,
                                           alg=alg,
                                           max_timesteps=timesteps,
                                           checkpoint_freq=checkpoint_freq,
                                           override=override,
                                           victim_path=victim_path,
                                           adversary_id=adversary_id,
                                           specific_folder=specific_folder)
    elif mode == 'pbt':
        training_manager = FixedIterPBTManager(log_dir=out_path,
                                               alg=alg,
                                               max_timesteps=timesteps,
                                               checkpoint_freq=checkpoint_freq,
                                               override=override,
                                               main_id=harden_id,
                                               scheduler_func=random_scheduler,
                                               iter_steps=iter_steps,
                                               num_agents=num_ops)
    elif mode == 'separate-pbt':
        training_manager = SeparatePBTManager(log_dir=out_path,
                                              alg=alg,
                                              max_timesteps=timesteps,
                                              checkpoint_freq=checkpoint_freq,
                                              override=override,
                                              main_id=harden_id,
                                              scheduler_func=random_scheduler,
                                              main_steps=iter_steps,
                                              num_ops=num_ops,
                                              op_experience_fraction=op_experience_fraction)
    elif mode == 'pbt-v2':
        training_manager = FixedIterPBTManager2(log_dir=out_path,
                                               alg=alg,
                                               max_timesteps=timesteps,
                                               checkpoint_freq=checkpoint_freq,
                                               override=override,
                                               main_id=harden_id,
                                               scheduler_func=random_scheduler,
                                               iter_steps=iter_steps,
                                               num_agents=num_ops)
    elif mode == 'improved-pbt':
        training_manager = FairPBTManager(log_dir=out_path,
                                          alg=alg,
                                          max_timesteps=timesteps,
                                          checkpoint_freq=checkpoint_freq,
                                          override=override,
                                          main_id=harden_id,
                                          scheduler_func=random_choice_scheduler,
                                          cycle_agents=cycle_ops,
                                          num_ops=num_ops,
                                          iter_steps=iter_steps)

    else:  # Anything else is unsupported
        raise ValueError(f'Illegal argument for mode: {mode}')

    print(f'Training with mode {mode}')
    time_start = time.time()
    training_manager.train()
    print(f"Training of {alg} took {(time.time() - time_start) / 60} minutes")


def parse_args():
    parser = argparse.ArgumentParser("Adversarial Policy on MADDPG")
    parser.add_argument('--wandb-project', type=str, default="ap-defense-2")
    parser.add_argument('--victim-path', type=str)
    parser.add_argument('--mode', choices=['finetune', 'pbt', 'pbt-v2', 'separate-pbt', 'normal', 'eval', 'improved-pbt'],
                        default='normal')
    parser.add_argument('--out-path', type=str, default='~/ray_results')
    parser.add_argument('--alg', choices=trainer_map.keys(), default='ppo')
    parser.add_argument('--max-timesteps', type=int, default=1_500_000)
    parser.add_argument('--checkpoint-freq', type=int, default=100000)
    parser.add_argument('--disable-log', action='store_true')
    parser.add_argument('--cycle-ops', action='store_true')
    parser.add_argument('--num-ops', type=int, default=10)
    parser.add_argument('--name', type=str, default=None)
    parser.add_argument('--harden-id', type=int, default=0)
    parser.add_argument('--adv-id', type=int, default=1)
    parser.add_argument('--iter-steps', type=int, default=25)
    parser.add_argument('--op-experience-fraction', type=float, default=1.)
    parser.add_argument('--override', type=str, default=None)
    parser.add_argument('--specific-folder', action='store_true')
    parser.add_argument('--ray-local', action='store_true')

    return parser.parse_args()


if __name__ == '__main__':
    main()
